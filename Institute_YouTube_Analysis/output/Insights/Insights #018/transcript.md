hello everyone and welcome back to active inference insights as always I'm your host Daris p uh py Wayne can't even say my own name anymore and today I'm joined by Dr Sam sanjie namjoshi sanjie is an expert in data science computer vision machine learning and bioinformatics with strong teaching experience in bioinformatics Biochemistry and computational Neuroscience he is also what I would say is a World expert in Act of inference uh in fact he's currently putting together a textbook which explains the fundamentals of the theory which will act as a wonderful addition to the education resources we as a community already have sorry for butchering that intro but sanjie welcome to the show uh we've got past my we've got past my least favorite part of the show which is the introduction so we're here thank you again for for joining me yeah thank you so much for inviting me really appreciate it no no it's it's it's really is a pleasure because I have been you know it's the least I can do I've been benefiting a lot from your textbook um which is not out so people if they're if they're sort of scrambling on on the internet to try and find your textbook they may only find the parpet Zulo and friston one um so you may want to start just by saying sort of well I think today's episode is going to be slightly different um normally we sort of do some deep dives into people's papers but I think today we're going to start at least by talking about the kind of education side of Act of inference um so nothing too technical necessarily but just s of thinking about how these ideas are permeating academic culture intellectual culture and then sort of broader culture but maybe where we could start I sort of gave a very brief introduction there is with you just kind of outlining how you found active inference uh because people might not be aware of your work sure yeah so the my path to active inference um was really unusual I think what was really unique about it was I had being at a point in my career where I was starting to figure out where all my interests were but I didn't know that active inference actually existed so that was the journey toward that was that moment of finding out that all of my research interests were colliding in one place and if we go back really far to just the beginning um when I was doing my undergraduate research I was working in neurobiology I also worked in genetics um I did a lot of work in Biochemistry and a lot of my my PhD work was actually molecular Neuroscience but I had wanted to move into more technical Direction so I did do uh my informatics work in my PhD uh which introduced me more to some of the techniques and skills that I would need um in computational Neuroscience which is when my postto first post was in in that area and it was at the end of my postto when I was getting close to leaving Academia that I discovered active inference while just browsing papers writing a grant um I was the work I was doing was uh modeling human behavior with reinforcement learning so there was definitely an overlap in relationship ship active inference but um my adviser has said yeah yeah we we know little about active inference we don't really use it in the lab but um it's something we're interested in and so that kind of just led me in that direction and reading some of Carl's original papers uh that just got me off on that path and I became really excited about learning more about it and that's kind of where I stumbled into the original problems of there's just no resources out there this was around 2018 yeah 2018 there were definitely no resources out there uh we're doing a bit better now we have the Institute we have the textbook by par psul and Frist which people can buy and then hopefully your one which is coming out um so want to go back it's it's interesting you said sort of the skills that you learned in doing bioinformatics were you know really contributed to your capacity to sort of pick up active inference quite intuitively what skills are they and what sort do you feel has benefited you most yeah so the the skills that I'm kind of talking about here um are very Broad in general so when you the strategy that I've taken with with the textbook is really to break it down into the language that everyone would understand if they're in any technical field like um you know engineering or computer science or statistics and that language is probability Theory you if you strip away the neuroscience and a lot of the other themes and ideas in there you're left with just a mechanism of basian inference um and a lot of uh the bionformatics methodology that I was using in my PhD when you look at it they boil down to some probabilistic formation so I had some of that background I had enough of the background in the neurosciences and enough in the probabilistic reasoning side to be able to translate that into the active inference domain there's all these other layers on top of it of course but at the core that's kind of what I see as the Baseline and that's common to all types of statistical modeling methods can motivate it in a probabilistic formulation sure and when people hear the word probabilistic maybe they their mind does jump to Baye but maybe it doesn't um so active inference obviously leans very heavily on basian inference variational basian inference you know to your eyes would that be the only path that it needed to pursue could there be any other mathematical formulations that might flesh out the claims of active inference and self-organization that isn't necessarily basian I definitely think there there could be multiple ways we can frame the active inference problem um because we're you know ultimately computational neuroscientists or modelers they're trying to find the most convenient way to model a system um not just for the purpose of making predictions but also something that's intuitive and easy to communicate and that makes sense to people um when we try to formulate these more broader claims and bigger hypotheses about the brain um I think it's just the convenience of the beian formulation that things neatly fit into this picture um but there are of course instances where sometimes you have to kind of force it into the picture sometimes that's you know if we try to modeling something um there may be other ways to do it and um the you know their probabilistic reasoning I think is just a really convenient way to being able to model dependencies among variables and when you pair that with this Dynamic formulation you talk about dynamical systems in a probabilistic setting um all the tools that you get out of it I think are really important for being able to model the brain but it's probably not the only way we could we could conceive of yeah I feel like I mean again I'm coming into this very new because you know I've discovered this during my masters which was you know a year ago so I'm pre-doc I'm I'm very sort of babyish into this field but when you read the kind of history of I guess computational Neuroscience or computational psychology it feels like there was this transition between dynamical system into active inference so we haven't really touched actually on dynamical systems theory on this podcast in in part because it's an active inference podcast but in part because it's it's Technical and I don't really have the sort of chops to get into that but I've been reading a little bit of Scott kelo's work and getting into sort of meta stability maybe you could just very briefly it doesn't need to be super deep but talk about how we get you know what does that transition path look like from dynamical systems what the in what are the key insights from dynamical systems because Carl was working in you know in dynamical systems in the late 90s and the early 2000s and what does that add or what does that what the convergence points between that and an active inference so the biggest uh thing about it is that when you when you look at the history of um active inference and you go back to the 90s a lot of the work that uh was inter interacting intersecting with Carl's work was the work that was going on at the Gatsby computation unit at UCL um and so a lot of the uh what you would call the uh figures the grand figures in unsupervised learning techniques and beijan networks we're being developed uh people like Hinton harani and um David McKai and there are many others as well we're developing those techniques in the 1990s um and because of uh you know frisen working at UCL as well there was a lot of overlap in in knowledge of course he was attending a lot of the same meetings and conferences and things like that and adapted that into SPM um if I'm not mistaken I think SPM 2 which was uh the 2002 version I believe switched into the beijan world from the more frequentist version of SPM in earlier editions and so you have this unsupervised learning idea which I think is really deeply powerful because um a classic problem in in Dynamic systems is you know the idea of filtering you have um some observed data that links to some unobserved process and you have to figure out what can we learn about about this process that generated that data from just these noisy samples alone and there is a really great paper um garmani and I think I might be pronouncing this wrong but rois it's R Weis I believe um I think it's 1999 paper they reformulate all the unsupervised learning problems and these ideas of um the expectation maximization algorithm variational Bays they kind of put it all together and show you how you can model State space equations in a Invasion setting and I think that's kind of the start of this move in that direction um and there's a great thesis be 2003 um uh PhD thesis which then kind of puts all those ideas together and I think uh the basic way of looking at it is that you're doing beian inference but you can look at it as beian inference over time and with all of the work in the 90s with Dynamic models of the brain like you you mentioned Kelo and there others as well um looking at the brain is a dynamic system which uh changes in a way that settles toward some kind of equilibrium um those ideas I think when you combine with the basion inference and all of these unsupervised learning ideas coming together under Dynamic systems umbrella uh it's a natural fit to then say okay how can we model the brain with certain equations uh where these unknown states trying to be estimated and the equilibrium solution is the most likely estimate of those States given the data cool yeah I want to get into expectations maximization cuz I was just very excited when I feel like I learned what it was although I'm probably missing something but I thought it was incredible um but just to sort of finish up on the dynamical systems theory and and the sort of chronology I guess there's two questions one is a very broad question which is in some ways is active inference a kind of apotheosis or the end point of dynamical systems fa can it be couched within that overarching frame and then second point which is a little bit more specific is in terms of meta stability so this idea that the let's say the neuronal Dynamics are jumping from as you say like an attracting set or what people talk about Wells but it never really settles in one place that will sound very similar you know very familiar to people who are familiar of active inference uh so I guess is my question is what's interesting there I guess is the kind of as you say over ESS the DI chronicity of meta stability where does it diverge from active inference what is it kind of convergence point and then again yeah how does that contribute to the difference between dynamical systems theory and active inference yeah so that's of course there a huge I think so much could be written and talked about on this so I'll kind of give like the sure the high level answer my interpretation of that which is that um active inference I think is what's really interesting about it um if you look at the equations um of active inference this paper is um it's hierarchical models in the brain Carl's 2008 I want to say paper um and it's a great paper because he spends a lot of time digging back through that old literature from the 1990s and then saying look if we add all these other extra elements onto it which is what some of the um those papers were doing for in your Imaging setting um we get all of these different techniques that fall out of it so it's kind of like uh active in even if you just take out the active part just the perception alone perception of both hidden States uh learning model parameters and also learning the Precision of uh sensory data or your priors all of that information um in one algorithm if you start taking all these special cases you can get all the neural network and supervised learning literature you can get um all these unsupervised learning problems filtering problems uh whether it's dynamic or static so I I see that as kind of like active inference um is sort of a global overarching method uh you could you could apply to any kind of um stochastic time series type of analysis um if you take out the action part then you just have purely a perception part looking for patterns in data but if you had action in there then you kind of recover this idea of control systems and the ability to control um stochastic dynamical systems um in a particular setting so that's the way I view it is kind of like you get all these other things that kind of fall out of it um so as far as I I can see it is it's kind of the broadest generalization one could imagine of these kinds of basan motivated statistical systems sure I'm curious again before we get into the sort of nitty-gritty I mean I'm I'm not here to sell active inference but I'm always curious you know about it scope about what the next 10 years is going to the next 20 years as you say in the 90s I I had Chris fron and he was sort of telling me just how exciting it was in the 90s uh because you had the Gatsby you had all the psychological work the kind of pure psychological work that he was doing and then the kind of Middle Ground that Carl was occupying which was a sort of foot in both U you know lands and as you say they kind of coalesced in the late 2000s and in the 2010s I'm wondering whether you can see any other sort of not a rival Theory but something that could you know if we're talking about dialectics and we end up with some symphysis what are the kind of dialectical uh you know counterparts to act of inference in the 2020s and the 2030s is there anything that it's going to synthesize with to give us something even bigger and broader and more overarching or is this you know I'm I don't know why I've gone very hegelian but is this the kind of is this the kind of end of history in some ways uh yeah that's that's a really good question and I I think it's you know it's really hard to to know I think at the moment right now if you think about just um if you're talking in purly computational terms um you know everyone is talking about deep learning um and that's what of course is an industry all over what you know research programs are focusing so much on deep learning systems and that's kind of the same thing you know could is claims have been made about that as well right the Deep learning is the end game and so everything will fall you know it'll kind of become the the main methodology um I mean there other there are not like a lot of other things that I think have the same broad scope trying to Encompass so many different areas I think that's kind of the the biggest advantage of active inference is the unification um but there's something to be said about the fact that you know active inference uses one objective function to do everything um you know how far can we get with that you know is it better to have tailor made systems where for different problems you have really specific types of objective functions um you know there to me I would say like the biggest Contender or biggest rival to active inference isn't one specific field but more an approach of just saying um how can we engineer problems in a really specific way um with very heavily tailored objective functions that satisfy a particular problem domain or something like that because I'm wondering if you run into a problem with this sort of generalist specialist issue where active inference is kind of great at everything but maybe not uh maybe can be outperformed by a specialist system that's really tailor designed for that kind of data right I think that's exactly right I mean the university that I work at for my day job which is not Super Active inferenced inferencing a Royal Holloway there's a lot of RL modelers um and when I speak to them that is the kind of I mean they're very obviously very respectful and very interested in active inference modeling but I think that is in many ways the critique which is that sure the free energy principle might be applicable to everything but as you say in the very sort of particular domain you've got to do the model comparison and if you're RL like if if your RL uh model works better than so be it I think the act of inference was response to that would always be well you can just Encompass RL with active inference so whatever claims you're making about RL will be ultimately you know will ultimately be yeah you know taken up by active inference I guess the one difference and you you have to tell me about this because you know you're saying you're starting in RL and then found active inference is that active inference is very strong on the as if as if theology as if it's doing variational basian inference is RL does RL have an equivalent so does RL say um that you know individuals look as if they're maximizing a reward function or is there a kind of more ontological claim that no they actually are doing that and you can see that in neurons and then maybe in that case you know just in terms of the philosophical uh strength of these positions if RL is making more solid ontological claims then maybe your RL researchers are going to get excited over that so there's two different angles that I would kind of up take this question from and it's you know when we talk about active inference or reinforcement learning are we talking about it from the perspective of what can we do in build of these systems or are we talking about you know the as if nature as in like we're trying to model specifically the brain and we want to go for biological plausibility because you know ultimately if we're just trying to build you know an AI like you know when I say AI I mean uh an intelligence that mimics animal and human behavior maybe we don't need to model it perfectly after the brain we can get the as if the as if part can get us you know it may not be exactly the same as right as biological intelligence yeah but it's it's a version of intelligence that does the same things and as far as anyone looking from the outside it effectively functions in the same way so that's kind of like that's sort of like I think with with both reinforcement learning and active inference um you kind of see the split like reinforcement learning started as you know looking at models of human behavior and um and human animal behavior and then got taken off by into the into the computer science realm where it then all these new techniques that have been developed after it may not necessarily resemble the brain anymore so it's when we talk about RL it's it's hard to you know what what does that mean because it's so many different things now and the same thing is kind of happening with active inference right where we started in that realm and now we're in a place where we're applying it to robots and other things and ways that may not be necessarily biologically plausible um so I guess the direct answer to your question is um I think it I think we need to Define what RL and active inference means specifically to be able to to kind of characterize the asness of it um and I think the other issue is that it's really hard to test these kinds of things like how do you test that neurons are representing things in a certain way you're always kind of locked into some kind of philosophical um backbone whenever you do these experiments too um you know and so I to some degree I feel like as if is maybe the best you can do and it's all about predictability like are we predicting the activity of neurons and what they're doing and that's kind of the best thing you can get to I think that the on ontological part of it that's hard to close that Gap and I don't know if we'll get there at least in the in you know the next decade or so maybe a lot longer to get to that point yeah I think as a naive psychology slash philosophy student I sort of came in and thought well we'll just get to the construct but you realize that in well in Psychology especially you know if I'm studying if I've got an attention Paradigm I just take for granted that I know what attention is but like frankly there's well was this good paper actually I think it was homel uh the title of it was no one knows what attention is and I kind of think you can have that for every construct I mean there anyone really and and in some ways that's a shame but as you say it's predictability I think the difference where well my stance is that I'm not an out andout modler so my interest is in saying okay well let's take a cognitive function like attention how close can we get to well let's just say this a general agreement we have over what attention is and fundamentally that's a phenomenological thing because we only really know what attention is by paying attention to our own attention in some strange way and I guess this brings me to ask do you think in some ways like you know you saying we got active inference in robotics and you got someone like me who's trying to do phenomen ology and active inference then you've got people applying it to atoms and you know the the very fundamental physics and so on do you think active inference is risking becoming too diffuse that it's just this kind of on size fits all and let's just take any phenomenon phenomenon and see what we can say about it in terms of variational basing inference and if so is that a kind of big problem what's the kind of narrowing down or refinement of the theory and the community that that might need to happen so this is this speaks to a problem that I've already run into when I'm uh people are reading drafts of my chapters I'm already seeing that everyone kind of has their own vision and understanding of what active inference is there's sort of this local knowledge problem right everyone comes in it from a different angle and the way that they've interacted with it means that they see it uh they see active inference in in one light or another um and that is a big challenge because there is this prediction ER of people reading the book they're like well I never thought of active inference that way here's what it means to me they're expecting it to be a certain way um and I think this becomes especially true the further out you go from the actual mathematics yeah because the actual mathematical models they of course there's always interpretation of what what the math means but they're a lot easier to say like hey I made the simulation here's what it's doing uh you know okay it converge it didn't converge we have these tools and metrics and things that we can say what's happening when we we run a model but when you start getting to the outer edges of that where there's a lot of the like the philosophy you know there's active inference in and ecosystems and weather and active inference um you know talking about like plants and how plants can be predicting um and we start I mean that's kind of the the real the draw for beian mechanics is that it's well it should apply to any dynamical system that are trying to predict one another within certain set of constraints which should cover a very broad spectrum of types of systems um I definitely think that's both hurt and helped active inference I think there's a tendency um in any field when you know the classic um when all you have is a hammer everything looks like a nail um if your proposal is vague enough you can probably massage it into the active inference framework but then when you start going down further and further the actual math of it okay so what is what is the math of ecosystems and communities and economics like huge all these agents that are all their own generative models all interacting in a big you know group what does that look like mathematically does it really scale to that level um or is there some other generalization we're missing um I I think it's great to speculate personally as someone I just love the idea of taking into these new areas I think that's what Spurs creativity and allows people to then explore new ideas but I think we should distinguish carefully between active inference itself and applications of active inference in a theoretical sense philosophical applications of it yeah yeah yeah yeah I mean this has been my personal experience which actually is that my life becomes a lot easier when I'm writing papers to stick to the maths as in not that I'm writing mathematical papers but that when I have the very just very simply if I have the variational free energy formula and the expected free energy formula in front of me I actually can't go too wrong in some ways um not that I run the derivations and obviously there is an appeal to Authority that those derivations hold but you know I remember my introduction you know when I started thinking about active inference and reading the kind of seminal philosop philosophical Pieces by Andy Clark and Jacob Hobby and these are you know Classics and very important but you then go well am I learning about predictive coding okay what are the hell all of these equations doing the variational mess and then that's really a mess for a lot of people and so I think for a lot I don't know my experience has been when people ask me okay what what is act of inference I literally just go well if you have just the variational free energy formula like let's just start there um a question that comes to mind actually when it comes to sort of talking to people about active inference is that people will say okay well what every Theory's got its Miracles every theory is got it axioms what are the axioms of Act of inference what is it presuppositions and I say well I kind of think there are two but I think you'll be able to tell me there are more one I say you have to kind of accept that there's thingness that there are individuated things and I think that's an actually really interesting development that could go which is like what is an individuated thing like how do we even get there how do we start there but that's a kind of axum and then the other one I think is time that you have that you have this kind of movement back to the attracting set over time is there anything more that we should add to that kind kind of set of presuppositions to active influence rest on I know there's going to be stuff like ergodicity which is a little bit more technical but there anything more that I should be telling people about when they ask me a question like that so I think the the biggest motivating motivating factors here uh what you said I think is those are definitely um really important assumptions and I think there are others that can be paired with it or um maybe are broader categories and one is that when you talk about thingness and existence right like we're defining what existence means like existence is is a philosophical concept that has been explored for you know thousands of years so there's many different ways of looking at existence I think this is more of a pragmatically motivated way of saying that you know when we when something exists it exists meaning that we can measure its properties over an appreciable amount of time so thingness of course there is that problem like how do you define thingness how do you define what how one or two things are identical to one another um you know there're all these kind of philosophical questions and you get into like mology about how parts and holes come together and what is a part and you know there there's all kinds of topics one could go into here um sure that have you know Rich literature right but um that's one part of it is that we have to talk about what is it we're actually talking about I think that's one assumption that you that you uh mentioned and along with the time assumption it's the idea that we're taking a physics perspective which physics has a very has a particular reductionist way of looking at the world and here we're saying that you know everything we're modeling can come down to stochastic differential equations um because everything in active inference ultimately is a form formulation of a stochastic differential equation where you have two different systems that are themselves modeled that way and you look at their interaction so I mean there are some other things too like you know you talk about like sparse coupling in Markov blankets there's there's other like little bits bits and pieces that are in here um that are more the philosophical foundations but then when you go into the mathematics there are a lot of little simplifying assumptions here and there that you could toss in um one example is like the llas approximation which is used in active inference it makes it simpler um there is some biological reasons to believe that that that assumption is true same with the mean field approximation there are reasons to believe that the brain has these sort of independent separations um but it becomes especially true when you're getting to the actual mathematics they're all these little pieces gaussian assumptions for example um that you start to add on there yeah excellent okay so yeah let's Okay so yeah so the one thing that I've been thinking about is what AC should I tell people then the other thing that people ask me is well what is it in the first place and your explanation at least in the first chapter I think of the textbook which is the kind of um this notion that the brain is sort of Trapped In This Cranium and has no access to the world and receives it through this uh you know these sensory data channels was incredibly I mean one hears about these in you know these examples in philosophy but I think that was really on the money so maybe you could do a better job than me just saying well what is the fun like if we're just talking about human well cognitive creatures creatures that are trying to exert some control some active sensing movement well or even perception you know within its environment and trying to survive let's say without invoking any strong tileology what is the problem that they face yeah yeah the biggest problem is you know absolutely that the physical systems that make up the world um are hugely complex so they by complex I mean there are many many variables that change um you know obviously there there's a lot of structure in the world that we can leverage but moment to moment um things can move and be very volatile um and so that's I think this is getting a bit into other topics but that's the reason why we formulate um you know things like culture and society and structure into our world we make the world more predictable by by having these elements in there that just make there's less overhead computational overhead and I think that's why you know groups of uh we look at the evolutionary history of humans um social groups uh small social groups and um the ability to see others as part of your whole like you're you're part of other you're part of a community of people where you all collectively care about each other I think these are all survival techniques that come into play um and a necessity for making the world more certain because things can change at any moment you know even just you know you drive to work one day um and the road's closed right like their decisions even in a regular World there are always things that come up all the time in your personal lives you get sick um you know things come up meetings get canceled and you're you have to go you know change your schedules things like that these are all part of our everyday lives and ultimately I think the idea is that when we think about active inference it's the idea of if we're trying to deal as these biologically from the biological perspective we're looking at humans trying to make sense of their world and quickly identify what's going on and quickly make a decision for the purpose of survival which is the whole point it's not to represent the world perfectly and all these other things we might want to do it's literally to be able to survive and pass on our genes to the Next Generation then what are the minimum requirements to be efficient to do that and also make those decisions in a really quick time knowing that the computational complexity of the world is enormous and we're dealing with like you know fractal levels of systems of nested uh hierarchies of things that are changing dynamically how can we leverage the data that we have to quickly make decisions and I think if we're talking the most broad way spectrum of thinking about active inference it's the looking at how the brain has solved this problem and attempting to write that down mathematically or at least something that behaves like how the brain how we perceive the brain to be behaving cool yeah so the way that I kind of see it it's very core is that you have as you say a coupled system the envir the agent in the arena the environment and the organism and the organism hasn't got direct access to the environment but it has direct access to sensory observations and so it develops a So-Cal generative model which is trying to in some ways mimic or replicate the so-called generative process generative process is the fact that there are so-called hidden causes or latent causes out there in the world which actually generate those sensory data and we're trying to have a model of of model of the world or be a model of the world whatever you want uh which maps on to that allbe it not perfectly and I think this is a really important point because well then we would just be the world and we don't want to be the world we want to be us and so we have our preferences and whatnot now let's I I'm I want to go to sort of expect maximization because I think it's a really interesting way into this problem so if we were let's let's not even think about genetic priors or phenotypic priors because in some ways that hacks its way to an answer if I was John locken I thought the brain was really a tabularasa like I literally give it nothing but the capacity to take in sensory observations what's my way of building a generative model I have my my sensory observations and that's about it and I want to build some hidden I want to build a model with hidden causes and parameters that relate the hidden causes to those sensory observations what's my what what is the kind of technical way of doing that so you know so if we're talking in kind of like you said like a tabularasa kind of thing where it's I's thought that you can think of like maybe like a uniform prior was the way that Invasion inference You' think about like you don't necessarily have any particular belief about what the state of the world is like uh you know denovo and you would accumulate over time um so I think like that this is when when we do active inference simulations we're essentially mimicking that because you can you know you might have like some uh you know very basic simulations you can you can put a uniform distribution over your model and you're kind of just saying all states are kind of flat there's no I don't have any one belief that one state is more likely than any other uh in my environment and I have to just learn what is out there um with just this really simple model um I think it's really hard to not have some assumptions because even in a model like that you still have a gaussian assumption about likelihood and prior um and you also have some kind of generating equation where you have some mapping that's there about okay how do what is the relationship between environment States and the sensations that I as this agent experience those are kind of built into the model so I think there's always I think it'd be very hard to start from nothing um other than doing kind of like a um sort of like an evolutionary kind of algorithm where you just have random connections forming over time and the ones that survive are the ones that make good connections and that's probably how you get off the ground in the first place is like in a limited environment there's just random selection going on and you just have you could have just a big set of random Connections in in a likelihood um and even maybe just choose from a random set of families of distributions from the exponential family and then just kind of see run it and see what you get out of that which ones survive and the one that survives you propagate it to the next generation and you continue to run more experiments in a dynamic environment and see which one comes out of that and what you'll probably be left with at a certain point is something like those kind of priors we talked about like where you were saying not to get too much into like the genetic priors and stuff like that they probably come from that kind of a process is that you have supervised learning that's kind of what evolution is it's like many many years of this sort of training with the environment and the ones that succeed are the ones that pass to the Next Generation yeah so I think this is a really interesting question actually because I've never really thought about it but I don't want to get too philosophical because it's it's an unanswerable question but as you say if there was no so we might have some prior we can have like a gussian assumption but let's say we just actually had no prior expectations over latent causes mhm it seems to me that the the the space of potential latent causes is near infinite at least in its gradations right I mean I could go from you know every single decimal point all the way down which makes me think that at the very Inception of life let's say how do we even get off the ground at all given like that every single potential hidden causes as likely as another and once I have one or two then sure I can bootstrap off those and and get to some uh descriptive viability in terms of my model but if we were if you literally had something that had like none Z maybe some assumptions like a gan assumption but in terms of actually like expected States none how do you even get off the ground on r on Randomness because can't you just go in infinite directions so um are you familiar uh with Terence Deacon's uh Terence Deens a philosopher he has something called the autogen and his 2011 book I'm faintly familiar with him but not enough to really say much about him okay so I I'm going to appeal to this because I think it's a really interesting way of kind of motivating this question and this is getting to more in the broader topics about like autop is self-organizing and self-maintaining systems um I would say that his example of the autogen um is is an example of um and I'll explain this in a second of of what you know how could something get off the ground from basically nothing and so when he proposes the autogen um the very brief idea is that he says suppose you just have some molecules that are just in a liquid some kind of um solvent and they're just these molecules floating around um and when they're in close enough proximity to each other uh they're able to form a little catalytic reaction so imagine four molecules one makes the other in a chain um such that a makes b b makes c c makes D but D makes a so now you have this little self-sustaining propagating chain just because of pure chemical Dynamics um but if there's not enough raw materials to make those those proteins it'll eventually fall apart yeah but suppose that a an enclosed membrane depends on some of those proteins so then you get this positive feedback loop where um the existence of those proteins encloses it in a small space and so it perpetuates that reaction and also the raw materials are outside of that membrane so that whenever the membrane deteriorates because there's lack of components the raw materials just float back in and they res sustain the system again which then rebuilds that cell wall or you can think of it as just it could be just molecular components that are just housing that reaction in a small space so in that small space you effectively limited the space bace to very small number of things and the breakdown of that system is a negative that negative feedback cycle ends up causing the system to then re again right so you could motivate that probabilistically like um you can make a simulation where that you know those are all in probability distributions and we're not assigning agency to that it's not like it knows what it's doing but one can see how you could scale that up into more and more complex systems and arguably that's exactly what's happening in the brain if you think about it in terms of self-organization you're just looking at another set of positive negative feedback loops on a huge scale that we describe in the language of probability Theory um so that's another way in I think it well it's very obviously it's very reminiscent of Auto Prius as first introduced by verel and Matana and then sort of taken up in the 90s by people like Evan Thompson but I don't I I'm sort of stretching into my past reading here and I'm not 100% sure that something like a classic autopoetic system needs to break down its uh you know needs a breakdown of its membrane in order to build up its membrane I think you can invoke something like a partially permeable membrane where you just have like the constant flooding of U raw materials which allow you know allow the metabolism to happen that still like recreates that because obviously the partial peral membrane itself is going to you know it's going to dissolve or break down because of entropy and so you you allow it to build back up but it's it's more or less the same I guess to be a philosopher an annoying philosopher you continue to ask well how do we get the kind of what would be operating organizational closure whereby uh you have a you know a instigating B which instigates C which instigates D but then that also instigates a like how does that Loop get off the ground in the first place because I feel like autop pesis is a very good description of things as they are and the classic example is the cell you know the cell which creates itself from within but I guess the question is you know how does that circularity how does that circular causality as they say get going in the first place and and I think that's probably at this moment in time an an unanswerable question I definitely agree so first of all just to say that like yeah this is not the only kind of you know system we could imagine this is just one example that I think just captures the basic ideas and there may be many other ways of doing this um this is getting a little bit out of my this is definitely out of my field of expertise but just from uh when I was in in graduate school there were Labs um friends of mine that worked in labs in synthetic biology um and they were building that you were using DNA and other types of other molecules like proteins and they were doing experiments exactly like these where um autocatalytic reactions would just form uh given certain set of conditions being met um and so they would just you know put particular compounds in because you don't always know which ones will work but you can toss them in and under specific conditions of heat and pressure and whatever uh salts and whatever compounds are needed um Metals you end up getting autocatalytic Loops forming very simple ones that of course break down over time um I don't think anything like the autogen as far as I I know has been replicated in a lab um but at least like the getting off the ground part I think just from um you're probably familiar with Stuart Kaufman's term like order for free um you you do have uh like especially when your state space is constrained when there's only a certain set of possibilities you have less possibilities and then you kind of get this idea where where one system starts forming you constrain the state space further and you get this kind of building Domino reaction where eventually you end up getting something that seems convergent because of small little chains of random decisions that were kind of made along the way um so that's kind of one way I look at it I I don't know too much about synthetic biology so that's not my my area but yeah well I had Mike levanon and I should have asked him uh my um so let's then build in something like so evolutionist bestowed on us these genetic priors um which I don't think we need to necessarily get into what is the essence of them um because the nativist position or what really is a kind of Prior or uh prior knowledge is still quite fuzzy I feel like whether it's a mechanism or an actual piece of knowledge and this goes all the way back to Chomsky but what's the kind of so I want to I I want to get to expect expectation maximization um whether that's deriving a prior from parameters and an observ ation or whether that's deriving par um parameters from a prior observation what's this kind of it's just very there's a very beautiful again it's kind of like a circular causality whereby you can derive one from and it's a kind of work in progress Tri and eror process but you'll be far better at explaining it to me so for the audience who don't know what expectation maximization is what is it and what does it give us what how can we bootstrap our way to intelligence with it so expectation maximization uh is algorithm that was it was developed in uh 1978 paper I think it's Dempster is the paper the author first author on that paper um and what was essentially happened in the 90s was uh that algorithm was kind of put into the unsupervised learning uh foundations that were going on in that time uh specifically what's called linear gaussian systems that's where um it was being uh understood in that particular context um and you can see that variational inference is kind of like a more broader perspective on expectation maximization so from the perspective you're asking about the EM expectation maximization algorithm um is a way of essentially there's kind of a bootstrap going on here where so on the one hand you have some kind of a model and I'm I'm speaking more from the the uh predictive coding type of perspective on or unsupervised learning perspective I should say is a bit more accurate On Em there's many different ways you can interpret it so this is this is particular way of looking at it um in the in the systems where you have latent variables so you have something unknown and all you have is sensory data of some kind um and so the idea is well the problem is that you're trying to figure this out that what you're trying to do is you're saying okay we know that there are some states out there in the world and they're generating data and there's some function that relates that has parameters that relates the that relationship so for example in a linear setting um the relationship between hidden States and sensory data will just form a a line just a linear relationship maybe it's just uh you know just as an example it could just be 3x plus one so you take any hidden State value multiply by three you add one and that's what your data is well there are there could be parameters that we might add in there so other little other things that could change the nature of that relationship the problem is you don't know that so you don't know what those parameters actually are but you also don't know the hidden state so you basically have two things that you're missing uh parameters in this sense are not random variables they're treated in the EM algorithm they're just uh Point estimates they're single values um but then you have this probabilistic system on the other side here generating that information your hidden states have noise on them so now you have the situation all you have here is just some data so can you take that data and from that data learn about the other parts of the system that you don't know and the reason this works is because the data is conditioned on those things it's affected by it it's not like in isolation in some sense you can think of it like they they lie behind the data itself and they're part of that process in some way the problem is it's all mixed together so this is sometimes motivated in like a signal um blind Source separation problem or signal unmixing in um uh in Signal processing literature um and so the EM algorithm is a a very clever way of getting off the ground so to speak um you describe it kind of like the circular causality or something like that it's kind of a bootstrapping perspective where you say well I don't know what that hidden state is out there but I can start from a random guess and give my expectation of what I think it is what's the most likely value it probably is given what I currently believe and then from that hidden State you do maximum likelihood estimation to say well here's what from if this state is this way here's what I think the parameter is most likely to be um given that I've just estimated what the state is and then you you go back to the next round you say Okay usually you can motivate this in terms of a like a gradient descent or there's many ways to to look at this problem but basically uh usually use the log likelihood as your as your objective function or negative log likelihood and you say okay well I'm going to make another adjustment down the log likelihood um for a new guess at what the par what the hidden states are what my expectation is now that I have those new parameters I just I just estimated and then you go back another round again and you keep going in this circle until you hit convergence which I will say that em natively um often has trouble converging so you have to you may have to add other constraints to make that system work but ultimately you get off the ground with just sensory data alone you are able to estimate other things that you don't know um that's the essence of also variational Bays is very similar to that as well that was what I thought the answer to my tabaza question was but we got to autens which was which was cool um yeah there's I guess yeah I guess uh you mentioned I was going to ask um you mentioned maximum likelihood estimation so I think it's wor people might not know what that means um what what is Mac maximum likelihood estimation and how does it feed into this picture so maximum likelihood estimation is a really well-known statistical technique um that has its origin in many different it's used in a lot of frequent statistics as well um as Bean there are many ways to kind of formulate it but the basic idea that the way that I like to think about it in terms of um this sort of predictive coding or active inference-based um perspective um just because that's kind of the Lang common language we're using right now um is to say um we have some um unknown variable of some kind so that you have uh something that's latent in this case um can we estimate the most likely value of um that particular unknown variable instead of the whole distribution so you could get back the whole distribution and do basian inference or you could turn it more into this restricted problem where you get an estimator which tells you the mode or mean of that distribution instead y um and you need some kind of a likelihood model to do that and you make some assumption about okay I have P of y given X that's your likelihood model you have some guess about how uh observations and states are related and you invert that relationship by just finding the mode only so the most likely value instead of the whole distribution and it can be an easier less computationally expensive problem uh that you can get like a simple update equation for given what distribution you're using um but the problem is that there there may it may be a biased estimator it may not be exactly right given the other assumptions that are in your model um so it's just a convenient way to estimate what parameters are that's kind of the most common way of using it is to say I have some you can use it in linear regression for example what are the most likely parameters that are give rise to this particular model that I haveh interesting you know it's funny there well there a couple of things that come to mind one is that people will be you know people who did what you would call High School maths what I would call Senior School maths uh would be very familiar with the terms right mean mode me whatever and yet it sounds really complicated so I'm kind of bringing it back to this education point is it really complic I had I had a question R down which I haven't asked but you know people think that the m is really complicated is it really complicated or does it seem or does it just seem really complicated I have to admit that um the perspective that I'm going to give you now might have been different than my perspective 5 years ago so it's hard to tell sometimes because you're so deep in something right is it now appearing easy because you know you've learned all the material because I remember how I felt when I first encountered all this stuff so I'll say that um beian mechanics I think that side of things is a lot harder to explain and a lot more technical um you can motivate everything in there as well probably within probability Theory I would say and that's kind of the approach I'm going to take um in the second book is going to be in that direction but if we're just restricting to active inference um I would personally say that it's not as complicated as it looks um it all can be motivated in terms of simple distributions um gines are easy to deal with means and modes I think what gets complicated is when people start dealing with linear algebra where you just have multiple dimensions in your system then the notation gets kind of very abstract and compact but in the univariate case um everything has a really strict and very simple interpretation that you can you can think about in terms of like equations how you're deriving things um personally I think it's almost there there are times when I'm writing and I look back at the equations and I'm like wow there there's so few things in here really um when you think about what things you need to tune um and obviously when you're building like huge you know like you're looking for replicating all aspects of human and animal behavior yes it's going to get complicated but the core actual methodology I would argue is something you could teach to high schoolers um you may not use all the technical terms but you could get them to run a simulation and see what's going on and play around with variables and they'll have a pretty intuitive understanding of what's happening for example right and so how would you recommend so people you know people might not have access to your textbook or they won't and you know if they in the public and the par petul and friston textbook although it's very good does I think for a lot of people feel like they've been thrown in the deepend that you know certain chapters I think are are relatively taxing what do you recommend how do you know how do you recommend people go about this if they're not you know if they're like me for example you I'm trying to pick it up and think I'm doing okay and and learning step by step what would you recommend people do is that are there certain prerequisites you know should you do a statistics course should you do a linear algebra course should you do a calculus course what are the key uh fundamentals that people need to do and and and do you have any recommendations I mean this is also asking for myself about how to really what bootstrap your own learning experience well uh of course I would I you know that's my aim for the book is to provide this perspective but um in the you know in terms of prerequisites you know when I think about um what active inference at its real core having a solid understanding of introductory probability Theory so we're talking about um something you would prob you could learn it some people learn it in high school um this kind of introductory probability conditionals joint distributions um having a good intuitive sense of that but it would probably be for most people it' be some undergraduate course um the second would be calculus um if you want to just learn the concepts um single variable calculus would be enough so if you know how to take a derivative and you know what an integral is that's sufficient for most things um and then the third when you start getting into linear algebra that's useful when you start talking about more useful systems that are multi-dimensional but if you're just staying in univariate everything you can graph you can visualize all the equations are just numbers there's no matrices or vectors that core idea uh which you can get you know the scaling of the percentages of active inference from zero to 100% understanding um is definitely not linear I think it gets you know the distance between 95 and 96 is so much bigger than like 0 to 50 right so there's levels of understanding but I think you could get with that alone you could get 75% of the way there of understanding what's going on now there isn't a lot of resources on that I think ultimately what you need to do is have some very basic Python Programming background um and being able to uh make a very very simple simulation with the most minimum materials this is what I was going to ask a lot of advice that I've got is well you know I'm going to be doing a PhD they just say well you'll do it during your PhD the more you do it the more the easier it gets and I get I actually think that's the case of a lot of things not only I mean there are certain things where it's not the case actually which I think is quite interesting so for example philosophy I would argue that in certain things actually sometimes your earlier intuitions are a little bit better because sometimes you end up down such a rabbit hole that you are a bit blinkered and you know I'm writing stuff like self models well there's just so many recursive elements to it and and so you get a little bit stuck but I found that for example uh coding so like R I use R because in my daily job I do statistics you really just learn by doing it and then it becomes like a language in many ways so would that be yeah I want to hear your sort of thoughts on that learning by doing it's not necessarily what people have been trained to do because we're used to going to school and reading textbooks but it sounds sounds like that's a really strong recommendation on your part yes and actually for me I love reading textbooks that for me is how I've always learned I've always been in situations where I didn't have a teacher and that's same with active inference I didn't do research in it first I taught myself active inference and also I taught it while I was writing it I mean there there isn't any resource out there there are a lot of you know little tricks that are in people's heads so even when I have all the model in front of me just tuning it with like learning rates and stuff there's all kinds of behavior I have no idea what's going on just comes from trial and error um so I think yeah I have the perspective of yes I I love learning from textbooks but I learned the most teaching was one way teaching it to people but also through just messing around with the models and the crazy thing about the active inference models that's really really nice about them the simple ones is that um if you're just trying to just do a very basic simulation everything is analytic update rules so if you can just take a derivative and you can do a bit of addition and subtraction um knowing how all the pieces fit together is a challenge but the actual code for a really simple active inference model is just a bit of for Loops addition and subtraction and some like updating of of some data structures just so you can keep track of the history of how the system evolves over time um the actual core of it is really simple to implement and um that's that's my experience is the what I try to do in the book um uh especially in the revisions I'm doing right now going throughing doing revisions at the moment um having pseudo code that is nearly exact to what you would put into python or R so you start out you initialize these things then you make a loop and then you update these things in a loop and then you interact with the environment get a new data point or you act on it depending on what your perception or action and you just continue that Loop and all the equations are right there so there may be a couple different ways you could you know you could put it into code But ultimately the sequence is really clear and I think that that part of it with a bit of guidance um someone who doesn't have a lot of experience in probability could get a feel for what's going on without even knowing all of the requisite background um and all the details that go into there and that's the fun part I really want that's kind of something for the future is to have like sliders and stuff like on a website where you can just play around with sliders and Learn by just interacting with a graph that has a pre-made model um you can get so much intuition from just doing that alone yeah I think that would be great I think that would be great I also wonder whether a thing that's been really useful for me and and and really this is just because of the generosity of people who you know are being far too kind someone like Carl yeah walking me through a soft Max function um just because he's nice and generous I wonder how how much did you you know you're saying you're teaching yourself active inference how much did you get out of corresponding with other people and is there anyone that sort of comes to mind that that sort of has you know formalized intuitions or has been a good sort of uh discussion partner so possibly to my own detriment um I seem to have a habit of having to do everything myself um which on the one hand is really good because I I I to learn things really deeply and I I have to go into it and and really understand it but on the other it's been a big Challenge and the honest answer to that question is um there are very few people that I've actually interacted with uh personally now that I i' I've been working at versus now since um December and there I definitely have now started to um for a lot of the technical questions um um Magnus kudal is one person who's been very very helpful on the factor graph side lance deosta as well uh recently we we spoke and he was very helpful helping me out with u generalized coordinates of motion which are still very very tricky for me to understand um they're pretty big challenge in in um in active inference models um and I would say that for me it's been more Brute Force so there are two or three really good reviews um Buckley at 2017 it's very dense um also Rafael um boax I hope I'm not pronouncing that yeah correctly that's another review and there are a couple others too like the Ryan Smith's paper with with Carl um and Christopher white um that has the discreet time tutorial essentially that's what I did was I took those papers and then I scoured the internet for code examples and little Snippets and put all that together um and that took me five years to get to that point um but that's what I did is just that and then constantly reading papers and being confused and um so maybe it could have been easier but there was no active inference Institute I didn't know people um that was the problem I was working in isolation which is a challenge always is yeah well good I mean it's amazing I mean it's it's super cool um yeah I point I I point people in the direction of Smith and colleagues but there are other tutorials uh CU I've spoken to Ryan as well and he's great and um I think that's the easiest one although to be honest I always I always go into it and then like get halfway through and I'm like I've got some philosophy to do and then I it's a long 100 pages so really long and I think um yeah I think you're right you need to have the coding up with you while you're doing it and and you're sort of figuring things out on the go and that's for my PhD so I'm not going to do that yet it's an interesting point about coding because I feel like um my intuition here is that it's all been happening on mat lab a very Tech this is a very nerdy technical point but people might find it interesting because they might as you say go now inspired by this and try and build their own active inference simulations so you know Carl's famous mat lab that you can download uh where you know you said there I think there is a python um there is a python you know whatever you would call it um package thing that you can package that you can install thank you um what you know is is a would you say one is superior than the other I mean most psychologists are used to R um what you know what's the kind of good starting point you mentioned python is that would you recommend python for a starting point so I I so the book is I'm using python as the main language in the book um and uh I so I I I've used R from many as the first real language I used in in grad school was R for most of my PhD work so I'm familiar with r as well lab I only have a passing familiarity with I use it a little bit in my one of my post STS but um like anything like any tool you know it depends what you're trying to do with it uh personally I think that you can make some really quick like data processing in R especially with like howly becomes tidy verse and stuff like you can do some quick like processing and stuff you can do a lot of quick stuff in R that's my life yeah the tiny I love tiny vers yeah yeah it's it's great and it's you know it made a little language it's not even some people might even argue it's not even R anymore it's a new thing on top of it or a framework but you know there's there are ways that you can um you know you can make a quick and dirty script in r that does active inference super easily you can do the same in mat lab um I find mat lab to be really confusing personally um and python I'm only picking because it's kind of the de facto language for machine learning because um if you're trying to build big systems with it you might get into trouble if you don't know what you're doing because things get very complicated um and but if you're just doing Simple scripts there isn't a lot of stuff that stands in the way so I like python because it's kind of clean um R is fairly clean I think people I think people are unfair to R I think it actually is a great language um and I would say that um there's nothing wrong with using any of the three languages it's just whatever you're comfortable with um and I would say that the minimum hello world of active inference would probably look pretty similar in all three languages um it really just comes down to something like how do you you know slicing notation and really basic things might differ but the actual like code there isn't anything super specialized if you're just doing like a really simple discret or continuous time active inference model I'm not sure if that answered your question yeah no no no that my question that answer my question and yeah I think people as you say hope hopefully over time we'll find more and more resources that kind of guide them through the process we'll get there um I want to get to versus at some point next point but we had a good back and forth about priors and I I feel like it would be remiss of me not to mention priors um I also had this chat uh with Ronnie sladkey who had written this paper the amigdala complex um from an active inference perspective which I think I mean I read a lot of active inference papers as you can imagine not only for this but for my own work and that was really I think the first place where I really felt that the notion of Prior had been nicely disambiguated because they are so it's so loosely used yes so people will talk about and I think I think until you're familiar with something like the pomdp scheme it's it's very confusing because at least in the pomdp schemes for example you have D Matrix you know your initial State prior you have your C Matrix the preference prize and that begins to you know separate things in a way that's possible um but let's I'm you have a really nice diagram which I wish I could show but I'm not going to share my screen so it won't work um but you know we have different priors in active inference although we make we lump them under this kind of general term prior so we talk about preference priors we talk about and that kind of builds in stuff like phenotypic priors or genetic priors and then we also have state priors and I think the main difference that is you know intuitive to me and hopefully intuitive to other people is you know in in all aspects of life both are going to be at play because you need to work out where you are more or less or what's going on and so basically what the state you're in but then you're also always going to have a preferred version of that I mean it's a terrible way to put it but you're going to have a preferred outcome you're going to have a preferred state that you want to be in and there's might be a Divergence there um is there any other kind of type of Prior that you might want to build into that picture and how problematic just more generally is this idea that we just have priors post Thea people don't even talk about likelihood distributions right they just talk about prediction error or sensory data which is confusing once you start looking at like an a matrix so yeah let's just kind of get into the the nitty-gritty how many different types of priers are there will they end up all being the same what's the kind of situation there I think the distinction comes down to how we're talking about priors um like you said right they're used in so many different ways so I would say like in the most General non-technical sense when people use the word prior they typically mean assumptions that are held by the agent in some way some kind of assumption and assumption can even just mean like um you know my actions have consequences you know I take an action I get a sensory out you know it could even be something like um uh just built in assumptions about the structure of the world itself so if you're talking whatever right so if you're talking very broadly yeah a likelihood you can call it a prior if you're saying this really really vague sense of just anything that's your assumption about whatever is going on about how you think the world operates or how your own actions operate right now that's of course um I think where the confusion comes from is that prior just kind of means like presuppositions or assumptions that are sort of just there in the model um when I look at when we think about prior in the beian sense then it has to take on a really specific meaning so I think that's where the modeling when you talk about an actual pom DP or you're looking at aian Network um that to me is where you can be a lot more um particular about what kind of Prior you're you're dealing with so just to be clear here a prior in that sense means um you have a variable whose value is uncertain so you can have a distribution over the values that that variable is most most likely to take you don't know which one is the one it's going to actually take but you have learned from some interaction through data um or possibly through Evolution or through teaching you know people learn about how the world Works uh by parents teach us certain priors about how using prior in a in a bit of kind of both senses of like um you could motivate it in an evasion way but it' be like prior about here is how the world works and those could be part of your values right you can you can think of it in such so many broad ways so that's thing is the problem is that we take these models that are really simple and we're trying to describe them attribute them to really hugely complicated things like culture and things like that and it's a lot harder than what is a prior inas Network look like when you're talking about a culture I mean you could think of it in 10 different ways I mean I'm sure you could make a model for what those priors are like and how they you know work into a model of human behavior for example but there is no right way to do that I think um it depends how you're kind of structuring the research problem so getting more to your question you asking like what other kind of priors are there I mean I think there are like like you said there are priors about U the agent's expectations about how the environment behaves uh it's preferred States about what states it would like to be in in the environment but also um all the sort of priors that come down for how its Body Works how movement Works um the assumption that you you just have about the nature of the world and the body the nature of your own model um that come down to personality preferences about how you view the world there's so many ways to look at it so maybe a better answer would if if we narrow the scope um unless that answered your question not sure no no no that that that that's useful I mean I think something you said that was really interesting there is that the prior is kind of the the the variable encoded in the prior is random variable um such that it's kind of the function of other possible outcomes it's a probability distribution but I guess people don't think about them like that right like intuitively you don't think about that you go well I want to be well let's say like a body prior I want to be 37.8 degrees celsus that's always useful one but it's not like that right it's it's it's over a probability distribution um and maybe that's worth unpicking uh Maybe not maybe it actually just confounds or muddies the water somewhat but how do people how should people sort of tackle that intuition no I just have like so-called preferred States and they're one thing they don't have there's no scope for them there's no distribution of them I just want to be here how do you get around that problem building in this notion of like a probability distribution or tending towards a probability distribution yeah so the answer to that question I think comes down to Precision so the each of the probability distributions that we can we can you know we talk about a prior over a variable we're saying that that variable I think because the non-intuitive part is that everything is couched in probabilistic language so one thing that I like to kind of say is like desire is expectation so metaphorically when we we think about desire it's not we want it's what you re-encode that is what do you expect and that's a really hard thing to do because that's not how we think about things we don't think about like I want to eat means I expect to have food in my body that sounds really weird right like it's just no it's naturally I want to eat is just a feeling that I have that I know that if I have food it'll be satisfied and that's kind of how we think about it so when I mean Precision I mean that when you for example you talked about body temperature so yes I prefer my body temperature to be at this particular um 98 Degrees let's say Fahrenheit um but actually the way to say it is I expect it to be that way because that's what your brain is predicting and with a prior really the truth is that there's actually a range right because um more than likely you can you can push that to about 104 degrees with a fever you don't want to do that very often but you could and you could push it the other way so now you have a distribution that's centered on 98 Degrees now it could be very precise when I said Precision it means how precise are we talking is it just a really tall point right around 98 Degrees um and for other priors it may be more spread out like you can tolerate a range so the priors are really useful because then you can encode this notion of ranges of acceptable values with a preferred value in the middle and if it's something where you're so certain about it like you have this preference this is the exact State I want to be in nothing else then the probabilistic notion still works you have a random you increase the Precision till it hits a point mass and then it becomes a single value and you only entertain that value for that prior so I I presume that mathematically that that that sort of uh that limit is basically infinite right in in the in the actual maths and you never actually reach it in terms of biotic limited creatures but that aside it's kind of that's kind of yeah I mean in mathematically like if you look at like Factor graphs and stuff we use Delta functions all the time in there and um but they're kind of yeah like from the mathematical perspective I think mathematicians would probably get upset with the way they're used it's more of a practical way of just saying like Hey we're turning a probable distribution into a single value so it just becomes a number not a scaler rather than yeah this is the So-Cal directa Delta function that again I got very excited over I get very excited to though for very odd things but I guess the philosophers would also get annoyed about the infinity thing as well it wouldn't just be the mathematicians um yeah this is all really useful for me and really fascinating so I guess well I guess people might think okay so I have these probability distributions that kind of makes sense for me in terms of like my C Matrix my preference priers but then how am I going to have probability distributions over something like a state prior um what what I guess the question here is what triggers that probability distribution because it sounds like you know sure we have uh observations and we have a likelihood distribution and that allows us to say okay I can do state transitions because I have this constant interaction of observations and States but just from my initial State what's the kind is is I mean I guess the question here is is there such a thing as an initial State um because I think it's always a mistake to take the generative model and slice it up in terms of really you know discrete time obviously you have to for modeling purposes um but yeah I mean I'm going to going around in circles here but my question is yeah is does it make sense to really think of a completely isolated initial state of D Matrix really um and if so how how do you get what's the context that drives that initial probability because it sounds like without any context every state is equally as likely as each other so um if I understand your question correctly like you know you take a the the D Matrix at the beginning of a of the discreet time pomdp active inference problem right you're saying that say that there's only four possible states in the environment that it could be and here is my my belief about the probability or expectation about which one is most likely right so you're asking like but you know any agent that exists I mean you know you can say like I don't know if it's birth or when you're a clump of cells at a certain point whatever you know what is the starting point like is that was that agent really starting with just like okay here's my initial State now I'm in my simulation begins right that's how we do it we model right but is that what you're kind of asking like what about there it's unbounded right because there's infinite number of states effectively yes I I guess the issue that I think people have and maybe I had until I sort of saw this written out is that when you have like a tensor or a matrix there's not one thing in that right and I think that's so you have the big letter this is something that actually recently I've sort of properly got my head around you have the big letter which is the tensor yeah and then you have the small letter um which is the is that the beliefs encoded in the tensor am I getting this right those are the parameters of the tensor right yeah which are the same in this case it's the same thing because we represent the tensors themselves are the parameters that's how we do them but the question is where do those values come from right so lowercase D would be you have these states with those probabilities how do you learn what those probabilities are in the first place assuming there's only four for example right we're already restricting it that's you can call that a prior right we only the agent beli there's only four states in the environment um but then the question is how do we learn what those particular probabilities are that's what the lower case D comes I'm just sort of curious yeah so yeah I guess I'm just curious about what what does that even look like when I say I have four potential states that might be encoded in my deter so I get you know when I feel like when uh when someone like me who's interested in computational phenomenology but learning it and is coming from the philosophy and the phenomenology one understands what a d tensor is and a b tensor and an a tensor because like it's like oh it's that but then the fact what what does it mean for something to be built into that tensor such that let's say you have multiple States in a d tensor with multiple different distributions like what what does that actually look like is that just is that purely a mathematical modeling thing or is there a way that we can intuitively think about that so uh if I think am I'm understanding your question correctly I think um the issue you're describing here is something that I think is what I've wondered like this is a model ultimately right like it's a model of a very simple scenarios because that's originally where active inference was came from was can we model simple human animal behaviors in a very restricted environment so if you restrict your environment to a tze where you know m is at one end of a teamas and it can only move in four places it can only exist in those States yeah okay then you can you know in that it's an abstraction of a real physical real world scenario um in reality um you know these are it's actually all continuous when you get down to it right like I think the discret state space formulation is kind of an abstraction to think about highlevel categories uh which themselves are encoded neuronal Dynamics the question is like you know let's say you really made a real agent that you were going to put into the world say it's a robot um and it's going interact with anything right well what does its D Matrix look like is it an infinite Vector right um I think that's kind of the open problem is like how do you actually do that for a real agent I mean there are states of the world that I don't even know exist right so yeah if I have a d Vector right like it's not there yet and so what I think would probably need to happen is you have a d Vector that has the ability to grow is one way you could treat the problem so if it encounters a new kind of state that's that's just something it has never explored before you add in a new slot to the D vector and have to recalculate recalibrate all your probabilities relative to that um something like that I think is that's getting up to what you're asking yeah yeah I mean my my intuition here is that it's really important to always also C curtail the state space or the belief space in these tensors so I've just written this paper just weing the preprint it's a more sort of cognitive sciency philosophy paper it's basically about how underactive inference for my eyes something like attention is always constrained by preferences so it doesn't really make sense to talk about like I I'm trying to collapse this classic endogenous exogenous distinction that exists in the intentional literature because under Act of inference I'm saying well think about what you're paying attention to it's always that which is most Salient to you and that's always governed by your prior preferences but then what I kind what kind of came to mind is well it's not every single prior preference is exerting pressure on your attentional schema at any given time right I am not simultaneously paying attention to how hungry I am how tired I am and so on there's got to be some context driving mechanism that means that when I get hungry I start paying you know I start paying attention to food when I recognize my child is in the Middle Road I start paying more attention to the cars and the future you know the prospect of them getting hit and and so on and so forth which again implies some kind of paradoxical circular causality because I need to be paying attention to that in the first place to drive my con constrain State space such that those preferences continue to drive the attentional specification and that's kind of similar to what I'm talking about with this D Matrix right I can't entertain every single possible probable State it's just computationally intractable but I also but for me to know which states are going to at least be feasible I need toay I need to be observing the world right like I don't know if I'm in Iceland or if I'm in the Sahara and so on so this is kind of what I meant by is there such a thing as the detener because an unbounded detent so-called prior to Observation to my more philosophically leaning mind sounds like it just could consider absolutely everything which it clearly doesn't so it makes me think oh is the dent are just the kind of be tents are in you know in Disguise um sorry I rambled I rambled no but I mean to answer your question that is exactly what the D tensor is because if you slice a benser with an action you get a vector out of it that right because that's the whole idea is that it's like assuming whatever ended up happening this is now the state that you're in you can kind of think of it that way because um whenever you you slice the B Matrix you you get back that slice only and whatever slice happened to be your starting slice it's kind of a way there's no action that preceded it because it's your starting at the beginning of the simulation but that's why it's a vector is because it's actually that's what ends up happening when you take an action is you do slice the benser into a vector and say like okay I'm transitioning and now that becomes my my prior um that we use for the next round of inference um so uh that's that's just just wanted to comment on that uh your question that you're asking is a really deep question that I don't I don't know if there's a good answer to that yet like what does it look like when you you know is there are deters floating around and being computed in the brain somewhere like what do they look like is that what you're kind of asking like they're just this unbounded entities like I thinking is that there's got to be some topown constraints which is context related because yeah you know we can't just at a very basic level we can't consider let's say in terms of action we can't consider every possible action right because there's infinite things in principle every infinite gradations of things at least that I can do in any given context and so this is I I'm not the first to think this right there's um Carl's sophisticated active inference paper with Lance and others where they talk about you probably do need a higher higher order action policy which constrains the possible actions that you might take in given context and and when I was thinking about this attention paper I was thinking well so people talk about for example hunger has really high Precision I'm like okay and then people say well Precision is attention and I'm like well no because I'm not always thinking about how hungry I am and so what I do is I try to disambiguate precision and precision waiting and attention and all of these different categories but what I'm saying here is there has to be a there has to be a sing that says say Well when I'm hungry because I need I need to eat when I'm starving over everything right I need to stop buying my essay I need to stop speaking to people I need to eat there needs to be some prioritization mechanism that says when your glucose drops below this point everything gets focused on food now on one hand yeah that makes sense because presumably my set point my homeostatic set point is very deep in my generative model so it trickles down all of the predictions at the same time it's still notle unless you have that higher order action policy why you would even prioritize hunger over playing a video game so I I think I wonder whether Act of influence needs to go in this direction and say well what is constraining these spaces in the first place all well and good having them but that's really my fundamental question here is do we need to invoke something like a high order action policy just to constrain the possible beliefs that we can have in a generative model I guess the answer is yes yes yeah definitely and I think um this area is something that I've actually been I've been reading um just just last couple days for one of the chapters I'm writing um and uh I I still don't have a great grasp with the literature but here's just kind of a sketch of my understanding uh to answer or just at least address my speculation on what you're asking um so there is you know the idea of attention in the in the act of inference uh formulation is attention is also active right like you you can choose what you're attending to at one time so attention is usually considered like the the newsworthiness of something so how Salient or how in interesting or informative is something at one point in time or another uh which in the active inference model like you say there's a Precision waiting on the signals the prediction error signals um for either Sensations or state Transitions and things like that um so but it's context dependent there's a paper recently about social cues in context so just as an example of a word I'll i s that paper later I think you might find it interesting but it's like even the example of a word right a word in isolation has no particular it may have many many different meanings so now there's uncertainty you have a signal someone says a word right that's not enough because there's uncertainty in the actual content of that signal it could mean so many different things but right the context might now that scope and tell you what it actually has what the actual meaning of that word is in the sentence that you're using it right so the context would be the sentence in this case um I was very inspired yeah I was very inspired by b b mit's work um he's got all of 2016 2017 especially 2019 paper with Carl on selective attention which I think you if you haven't read it you would really really like okay shows that like context so for example if I'm doing a Yus task which is a classic attentional Paradigm where uh someone says you know you show a picture of a family let's say and they say well how much do you think this family is worth and then all they do is look at the furniture and their clothes and then they say uh I don't know uh do these F I don't know actually what the second condition is it's like do you think this family get along and all they look at the eyes and they look at how they're looking at one another what that shows is in like what's Salient I actually prefer the term relevant but that's for technical attentional reasons but doesn't really matter um what's relevant is always relative to context right it complete it's so it's always contingent on the goals of the agent at the time but then you get I think the issue that I kind of skirted and made need to think about is you you you potentially end up in an infinite regress because you say okay I have a higher order context or a higher order action policy that is saying for example when I'm hungry prioritize hunger over the fact I want to finish this video game level okay M but then what then you but then someone might come along and say well why is that higher order action policy relevant right maybe that's just a hidden state for another higher order action policy and so you may end up with this infinite regress of saying well where does the relevance kind of stop and maybe you just get something like embedded priors genetically embedded prior that get you out of that hole um because I can't really see any other way out of it but all of that to say that I think I think context is really important in terms of guiding the Precision waiting of beliefs I don't like the term I don't like the although okay I don't like the alision of precision and precision waiting because Precision for me is a very mathematical term and precision waiting is a much richer term MH but you're absolutely right attention is not only pulled towards something it's also d uh you know often times seemingly purposefully directed like the point I was trying to make in this paper is that it's fundamentally gold directed even at the very lowest level right like even in terms of like color discrimination you could you know you can describe it as if your visual neurons have these prior preferences and so on anyway so actually it would be worth saying about this like you know I'm trying to I'm trying to work through your question like probabilistically so we don't get into the like you know philosophically it's very hard to talk about these things cuz like but I'm trying to look at like actual model right um so let's just say that you know hunger is a random variable right like your state of being hungry which is dependent on uh blood glucose levels right so your body is actively sensing your your your you know the um your your bloodstream hormones are sent out you know those levels come back a sensory information and that's a sensory signal you get from your body that your brain will then say okay blood glucose is a thing I'm measuring here it's gone down to a certain range and that there's a prior literally in an invasion sense there's a prior on um blood glucose that has you know distribution of acceptable range and so now that the state is changed conditioning on that distribution makes hunger now more likely or more expected in the model yeah because blood glucose has now fallen um so I'm I'm thinking of it through conditioning right these effects are all conditioned variables where the state of hunger becomes more likely because it's conditioned on some other thing that's being measured so it's kind of I don't know if it's like an operating system right where there's like tasks and you know you're sending something up to the top is like process this first you know but it feels like this queing like kind of happens through the distributions is kind of what I'm saying but that's the question right is now that the body recognizes it's hungry why does it say okay I'm not I'm going to stop petting my dog or playing on my phone and I'm going to go get food and let's say it's starving and that's all it can think about it's all it's paying attention to what is governing that okay well you go there's a higher order policy because it has to be which says okay given this hidden State now now so the the the hidden state of hunger has become an observation for a higher order action policy which says given this this observation go find food my point with the infinite regress is is there an action policy I've got to try and make sure I guess right I didn't write this in the paper so this is actually very influenced by John V's work on relevance realization so if anyone's curious about this argument he puts it in his it's in his relevance realization paper so I'm butchering it sorry John John might be watching sorry John um but the idea would be why what you know is there an action policy guiding that action policy like where does the limit where does it stop because you might say well why would I when I'm hungry go and find food and that's because you have a higher order action policy which says that given your observation that there is this action policy and so on and so forth so I think there's an important question here which might be remaining in the active in literatures which is where does the cap of context stop like is there a point at which it just stops and where does that come from and again I think the answer probably has to be that it's evolutionary um but that's boring B well it is I don't know if you would call it a cop out but that's kind of what you end up having to do at some point right like when you have this regress if you were saying like the boring answer is well you know we have these DNA is compressed source code of all these other things that you know we've experienced and it overrides other things right I guess um yeah I don't know if I have a good answer to your question other than just saying like if you're talking in a purely beian sense like if you just had uh Bas theorem um and you increase the because the Precision right the Precision is also learned from the data that's also part of right like you learn how much to modulate that Precision um you know if you have just even a three-way Network where you have your data you have hunger and you have play games um and those two things are being integrated together to have this distribution over like what do I do um like what's the next you know do I eat or do I play games what what's you're inferring from the data well if you're learning from the data the Precision is much higher on the distribution for Hunger then its waiting is much more higher Invasion inference than the gaming playing games and so your posterior will be weighted much more highly toward the state would be more likely is getting food yeah so I don't know if you even need to evoke that infinite regress other than I'm not sure I'm thinking about this through this I'm not maybe we're not understanding I'm not understanding your question but no no I think you are I think you when I've spoken to John about it I have said I think you get out of the infinite regress through Evolution um or you get out of it through well in yeah you could say learning right whether it's um kind of this supervised learning over e EPO or whether it's in real time I think the problem with in real time learning is like why if it's it's not evolutionary and it is in real time why would the system prioritize those you know the Precision waiting of those signals over others and then maybe you end up in the infinite regress I do think at a certain point you have to bottom out somewhere you're right because I mean why is it in the first place that food is you know why is distribution constru where did that distribution come from in the first place like why is it constructed that way exactly yeah that's exactly that's the point right like yeah and I'm just really curious about what these what's the drive because I I I invoke this kind of notion again of like a circular causality between attention and preferences whereby for me to prioritize a preference in my Precision waiting right for me to go okay uh the prior for Hunger is now more you know heavy in Precision waiting than the prior to get you know play video games well I need to pay atten as I said before I need to pay attention to the observations that I'm making so as to drive that which in turn drives my attention towards that and you get these kinds of positive feedback loops um which I think actually also end up explaining something like addiction and maybe in the count you know I know John would argue that the opposite of that would be something like AAP agapic love where you're one with the universe but anyway we don't have to go there um if I can I will find what John says about relevance realization because I think it's a um it's his it's his kind of it's his response in some ways to the frame problem so the frame problem being well how do you know like if you're an artificial intelligent being how do you know which information to zero in on um okay because you have a pol well how do you end up with a policy to zero in on that information where you have a higher order policy what how do you have that and so so that's the infinite regress is the infinite regress of the frame problem but I guess when you bottom up we're just going around Circle that but when you bottom out with Evolution you seemingly resolve that problem I think so and that's just as an example and I think I I'm not sure this is true but I believe it's true that I read somewhere that um like infants will um you know I'm talking like very small infants few months old they will um I don't know what the exact age is so I can't give a great um answer on this but they will prefer they will you know be selective about the kind of liquids they will ingest so like sweet things is one example but like bitter or you know salty there could be those could be poisons so you and there are many other things too like suckling Behavior other instinctual things that infants within first few weeks have the awareness of or ability to to do these sorts of very very basic behaviors where does that come from right because if you're you know there aren't any where are those priors coming from I think you have to bottom out on some level of evolution in some way eventually um but along that way in the development right I think there's so many other things that happen in there and a lot of it is also cultural things you know you were taught um even in the early um reinforcement learning or active inference paper 2009 with Carl's paper with a few other authors on there they actually use supervised learning to teach the model it's like priors first to solve the mountain car problem right so like I think and they argue like well you know people you know you learn from other people like your caretaker caregivers uh people around you teach you some things so I think that's the other option it's either evolutionary or it's learned very very early on as something that you learn during development that becomes encoded as this as this highly precise thing um but that's just speculation I'm not quite sure no it's fascinating I mean it goes back it's really it's really amazing I mean I think active inference in giving us these kinds of phenotypic priors answer so many questions that were long-standing in cognitive science I mean like this argument of kind of foundational Concepts goes to back to Someone Like Jerry Fodor and Fodor saying well we kind of need these innate Concepts you know um because how would I ever be able you know it's almost platonic it's it is a PL argument you know how could I ever bracket something into the concept of red rather than blue without already having the concept of red and Plato argues that you have the form of red and Carl will say well no you have a sort Proto foundational prior that allows you to do this assortment so thank God for Mr friston um let's um let's finish up just by talking about verses so people are probably aware of what versus is but if they're not um maybe you can give just like sort of a brief overview of what versus is what the ambition is um and also what you do there because I'm sure people are very curious sure um so I've only started versus in um December so uh versus in any ways is still very new to me so this is um just to give an overview uh for verses um versus in um about maybe I think it's almost two years now um Carl and um many of the researchers that were working with him joined the uh versus R&amp;D group so uh versus AI company they're working on um active inference models um and U applying them um and scaling them and learning how to use active inference models in more realistic uh settings so kind of the scope of when we talk about versus um we look at how we have you know natural Intelligence on one hand and artificial intelligence so that's kind of the way we're reframing it we're thinking about how do biological systems intelligently do things um bi biology biologically inspired intelligence and how can we engineer those things using active inference so in one sense like the R&amp;D group is continuing the work that Carl was doing with many of his colleagues but now we're trying to apply them in uh specific settings so I can't talk a lot about the actual research itself of course I'm not you know but so I'm giving more of a very very high level explanation um but the major uh contribution is you know we have definitely Open Source papers that have you know been published a couple like on Federated Act of inference as one example the inductive inference paper uh there's a couple others on structure learning that came out recently um so there is some insight into some of uh the the work that's being done at versus there through those papers um but more generally versus is focused on the in vision of the spatial web um which uh there's a book uh the CEO of of versus Gabriel as well as Dan Mapes they wrot co-wrote a book together the spatial web which is uh looking at the future of what the internet could be so looking at the isolated technologies that are that make up web 3 um when you look at things like um uh blockchain and arvr Technologies internet of things and smart cities and things like that uh digital twins they all have their own little research area they're all kind of being different companies in isolation are studying these things so the idea is to put them together into one umbrella um where we have the new version of the internet where you don't really have web pages anymore but you have people places and things so there's so much to unpack in that umbrella um that we could talk about that the book goes into and I'm still myself learning and getting my hands on all the uh details of that but it is um the new web standards that would come out of this are we registered with itle e so you can attend uh look at the documents and things like that that are part of the development of the spatial web and active inference is playing the core AI component of what the spatial web will entail so it's kind of functioning as you know the yeah the broader scale of it how can we take it out of the neurosciences and scale it up into something that can be used these kinds of Technologies really cool really really cool and a lovely explanation um I was curious just when I heard you you know you one does hear the versus people talk about mimicking or or replicating biotic intelligence in artificial uh entities how much do you guys again this is a very high level question because I don't want Maxwell to shout at me but how much do you guys run into like the as if that we've already discussed namely maybe in terms of like the ground truth of active inference is that things look as if they are conducting variational basing inference over their external States but how they're doing that whether they're actually doing that right these are still kind of live debates I'm wondering whether when you translate that over to an artificial intelligence agent it leaves you a little bit well up in the air about okay exactly we we can say okay they have to be abiding by by the free energy principle they have to look as if they're minimizing variational free energy but is there a problem where you go well how on Earth are we how what's the process Theory how are they actually going to be looking as if they're doing that so what's the kind of yeah I'm curious about how you how you skirt that problem so I think these are I mean these are the kinds of things that will probably look different these discussions look different to different people depending on your your philosophical presuppositions so different researchers at versus and outside of versus all have different takes on this um so my personal take is that the as if problem you know is not as important I don't think in an engineering setting because ultimately it's sort of the scientific process about okay I'm making this model that I want to do something in this world and I and it needs to predict in a certain way it needs to do these certain things is it doing that it's not then what do we what is it missing that we need to add to make it make it and if end of the day I mean I don't think it really matters if something is totally mimicking human like intelligence in a way that for all intense and purposes maybe like kind of the the Grand Touring test of uh you know in general of any kind of skill or thing that it can reason about if it's doing that effectively then I feel like the as if question doesn't really matter for this kind of a circumstance and yeah you start think about Consciousness and the effects of conscious Ai and whether something like embodied cognition active inference yields Consciousness again these are deep philosophical questions yeah I agree yeah I just wonder whether when when I think about active inference and I the more time I spend with it the less Concrete in a Socratic way about what I can say exactly it's claiming right um because sure you have the mathematical derivations but you know going all the way back to just basic analytical philosophy all the maths just turns in on itself right it's an analy it's an analytic truth it's kind of tautological it doesn't necessarily tell you anything about the world apart from the fact that things have to look as if they're abiding by this principle like things look like they're abiding by hamiltonian law of least principle of least action so I'm just wondering whether in an engineering sense right so in a very pragmatic sense what and again not revealing any secrets because ndas and Maxwell scary um joking um how can you even like how do you get from okay you have like the vfe equation or the EF equation and then you have these kinds of offshoots like predictive coding or Pond DP schema or variational message passing and so on how do you kind of choose in a way um which one's the best option and also then we have to build in this issue of okay we got discret State space we've got continuous State space in reality everything's hybrid so I'm just wondering whether how you can get from like a super fundamental mathematical Axiom like the variational free energy formula to something that can actually be programmed and is applicable and I guess this comes to a question of how does someone decide what kind of simulation to run in the first place yeah I mean this is a this is a big problem that will come from scaling any technology but active inference you know if you even just look at the robotics papers that already it kind of come out the last 5 years or so there's been a lot more exploration in that area um a lot of them have to make practical considerations and tricks to make it work in the robotic setting so whether that's you know further approximations of certain signals that are just taken for granted and the equations you know it's like well there's latency issues there's all these other things that you know you have to deal with and these robotics papers often start with the core of active inference and then throw a bunch of other things on top of it so the most common trick is the like amortised inference um which would be like you know you you know it's taken for granted if you have a small tiny little you know toy simulation oh my generative model is this linear equation or this quadratic or whatever it is um how do you actually you know you already have learned those variables okay well how do you do that in a setting where it's this huge multi-dimensional thing well you can use deep learning use some Universal function approximator that learns the parameters of all these distributions so the you know the broad answer to your question is is like you come up you hit a wall with a lot of these sorts of things when you look at like the robotics literature um and you're trying to engineer these sorts of systems where you have to start including tricks that were not necessarily in the original formulations and you think okay well you know I have to prune my my policy tree there's so many different options here how do I figure out what's the best action to take because I have to do an engineering setting um and so there's a paper recently on reactive um uh decision trees uh reactive uh behavioral trees in uh in robotics that came out a couple years ago another perspective looking at branching time active inference for example so these are all like little modifications on the original methodology the core is still the same but you can add these extra layers on there so you make better predictions and you can actually function and build systems in the real world and I think it's it's sort of like a guess in check like you you build a simulation or you build a robot and you realize okay there's a major limitation here what do we need to add onto the system mathematically for it to actually deal with that problem and I see this as an iterative process like you don't know starting out like you start out with just the basic model and then you realize wait that doesn't work very well and then you keep adding components on there till you get a more holistic model that can make better predictions you guys are very pragmatic it's very good it's very good it's very yeah I like it all engineering is right yeah yeah yeah yeah no it's um yes uh yeah yeah well let me know let me know how you guys get about with this pruning via context problem cuz I think that's a problem and I'm curious about how that's getting done probably again with these uh phenotypic priors um I did actually get up this vvi thing so I'll mention it because you might be interested because it actually comes from artificial intelligence so this is actually from a really nice paper which was in phenomenology and the cognitive Sciences by Brett Anderson Mark Miller and John vvi called predictive processing relevance realization exploring convergent solutions to the frame problem and therein they say and I will get it up um in the middle of the 20th century artificial intelligence researchers discovered what dennet 1987 called a new deep epistemological problem that came to be known as the frame problem in its original form the problem consisted of how to program an artificially intelligent agent to intelligently take into account the side effects of its actions for example if a robot needs to retrieve a battery but the battery is on a wagon that also has a on it the robot must be able to realize that retrieving the battery will bring the bomb along with it thus resulting in its own destruction this may seem like a relatively simple issue but it turned out to be incredibly complicated the problem is that there are a near infinite number of potential side effects to any particular action how can we program the robot to take into account the relevant side effects while intelligently ignoring the irrelevant one we cannot program the robot to systematically assess all of the possibilities because there are far too many um and so I guess that idea is uh okay you you give it a prior which says you know don't pick up wagons with bombs on them but then there's a potential side effect that lams if there's a child underneath it and you want to pick up the wagon with the bomb on it and you want to resolve the child and what you end up here right you're overfitting on that scenario right yeah you get massive overfitting yeah and so what I think active influence brings him really nicely to this again this comes out in my paper I guess is the notion of a hierarchy and I guess that feeds in right like if there's a child and you know if there's a child at risk you do it and I guess the question of the frame problem is well what's at the very top of your hierarchy of values um and maybe those are are evolutionary priers that's a really good question I don't you know of course I don't think anyone knows the answer to that question but I will point out when I'm thinking about this what you you know you brought that up that you know um you know children to me children are like that robot like really like you know toddlers they don't know the context differences between things right like but you're taught it over time if you put them in the world yeah they're probably going to take that bomb and they won't know the difference but we tell them by saying no don't do that don't do that they learn over time uh to make those those contexts and I I don't know exactly like maybe some of it eventually bottoms out at Evolution but I think that's what the learning stage is for you know young children they have to go through this sort of you know trial and error where they're doing all kinds of things that could kill them and they don't know until a parent tells them and explains you know like this is dangerous this is why you know um and there's still in that in that question is like how do you then you know how are all those things ranked where like you said like what if there's a child like it doesn't completely solve the problem but I'm just pointing out that like perhaps a robot needs to go through like extensive training um where you know like if there was a way to encode those priors maybe we could build them in but it's probably so complex that the only way is to make like an infant robot and have it go up through stages of development where we start teaching it things and we you know instruct it and it starts to build in those those pieces in there that's just speculation no no no I think that's ABS I think that's absolutely right um again the philosopher in me is saying well why why and this is the infinite frame problem why would the child listen to the parent and try and survive in the first place and then maybe you do get grounded out an evolution um there's definitely yeah the instinct to you know your caregivers are the you know why is there an instinct to cry and why you know there all those sorts of things like their animals that don't cry out in pain um are often ones that are not they're ones that don't have a mother like there's no there's no one going to help you right like what you you know crying out in pain is a social reaction because you know that others will take care of you so like yeah is going to be eventually it's going to be built into social Customs which again then bottom out into evolution is my guess right like eventually all of this is some kind of foundationalism and active inference gives us that foundationalism so we it took me it took me an hour to get there but we got there so I'm very sorry for everyone who had and you I mean these problems are like that an hour is good to me like I knew this  answer already I already spoke to John about it for some reason it didn't come to me but anyway we got there it's all Evolution uh well it's not all Evolution it's a hierarchy um final question yeah uh there's a lot of hype about deep learning and large language models and neural networks and we haven't even touched upon this podcast's most frequently touched upon topic which is consciousness and this is going to be a very flippant factious question and I want it to be answered in a flippant fa man factious manner uh are any large language models that exist right now conscious my instinct tells me no um but I will say that large language models continuously surprise me with the kinds of things they can do um the you know there's all this research coming out right now about like well what appears to be imersion is really you know not now that we kind of like kind of look at it a bit more closely um I think that it's worth considering um not that they're conscious but there may be some level of I'm trying to think of the right word here cuz like you know self-awareness all those things are not quite there but there's always this argument like oh well something is mimicking it it's not actually the same thing as that like a parot right kind of thing like he doesn't understand the meaning of the words where does that boundary get crossed because there's eventually a point right where the mimicry is the same as the actual thing right and I don't know very deep it's a very deep philosophical question um which could only be answered in factious ways because it's it's hundreds of years of thinking yeah well SS Chinese rumors well you just classic you don't yeah the idea you just don't right like you do the mimickry and actually well you know there might be some people who say that Chinese room is conscious um that's by yeah you're Guided by intuition if you don't have a definition of what conscious is which we don't it's really hard I think we have only you can do is you can kind of poke holes in things and be like well we know we're conscious at least we think we are and here's what we do and this thing is not doing that thing like you know yeah the whole whole the Chinese room thing is a whole lookup table right like well yeah yeah yeah we're not doing a lookup table but there's some compression going on like we have a compressed representation of something is that so is is compression sufficient and maybe you know how compressed information or relevant yes I'm think I'm thinking about these ideas I'm not going to SP I'm not going to give any teasers but I'm thinking about these ideas um okay and final question is will large language models get get us there and by get us there I mean um well actually you can pick you can either have Consciousness or you can have what people throw around as you know with the term General artificial intelligence or sorry artificial general intelligence AGI uh God knows what that means but you may have your own definition so will it get us either to Consciousness or AGI take your pick please be factious personally I find llms really boring so I mean I just don't find them very interesting and there's been a lot of you know hype around them and what they can do um you know I'm the kind of person I don't have a lot of very strong opinions about these kinds of things because I feel like we just we just don't know enough but I can have I have hunches right I have a hunch that it's not going to get us far enough um and I don't think if you throw enough compute power at an llm eventually you'll have something out of it and I think the big reason is because there's so much when you look at like active inference models they're not nearly on the scale of you know they're not llms right but there's causality there's sense of causality and structure in the data generation process that I think you're not completely recapitulating within llm this is more of an intuitive feeling I don't have data to back that up um but when I look at an llm I don't see it as I see it as clever memorization with some level of compression um in there and you if if it's possible to do you know you take maybe LMS is too too much to look at but like a simpler deep learning model can you take that deep learning model and do it better with active inference with less parameters less training time less data can you do a lot more with a lot less better generalization if you can then my hunch would be that the active inference model which is obviously where my position tends toward is compressing or capturing something in the data much better than a deep Learning System is now I don't know what that difference is in what it's doing but if I had to make a bet I would make it on that active inference is doing it better and more compressed in a way that's probably more scalable and more likely to bring about so-called AGI which for me means mimicking human and animal behavior on reasoning tasks is a very vague like what we see in each other as being intelligent Behavior we recognize it in the systems that we build and doing it as well as human performance that's kind of what I would consider as a vague definition of what a AGI might be that's good that's a that's a nice uh it's a weak version of AGI it's good it's good it's good we're not going to get anyone too angry with that one I don't want to commit too hard to it so no no no well you never know who listens you never know who listens I might get you know an email from Jeffrey Hinton or someone you never know I'm not I I'm just going to watch um but if verses need philosophers then well actually I know there are philosophers of verses and it sounds like yeah xell for example but yeah there's others too yeah and so on well Sanji I think all of that means that you're absolutely on The Cutting Edge of well of the future of intelligence and that's exciting um so I think well from my side you're you're a real inspiration and I say that a lot to people and that's because I mean it because I'm really really very inspired and touched by all the people who have helped me but you have helped me not only with this textbook but also you know privately in messages and you've put up with some terrible questions probably but they're all questions and question thank you uh I was waiting for that um but yeah I think I this hopefully I mean this podcast I've really really loved because I think it shows people that something that can seem incredibly complicated well in some sense is but can be you know it's it's tractable to use a sort of basian term just given a lot of hard work determination and and kind of motivation so you're definitely a kind of Shining Light although people may have not you know people may not know you because you're not sort of one of these kind of you know you have got millions of papers out there but nonetheless it's uh it's a great pleasure to speak to you and um thank you for the work you do I always ask people before they finish just to let people let the public and the listening public know where they can be found so you know if anyone does have any curious questions about the textbook um and maybe you want to speak about the textbook you can give yourself just a kind of 30- second plug but yeah you know you know email Twitter whatever whatever's best yeah email is the best way to reach me um It's sanjie s n j v. namjoshi n mji gmail.com um you can also find me on the active inference Institute Discord Channel if you want to DM me over there um my website is uh snam ji. github.io I noticed it hasn't been picked up by search engines yet so uh I put it in the video descript but yeah it's hard to find but I just have a little bit more about my papers and research and things um yeah I'm open for people taking a look at the textbook basically if you send me an email just saying you want to look at it I can give you a link to um the uh Koda page that's hosted by the active inference Institute or their chapter drafts and uh just uh video lectures that I've given still a work in progress I'm in second um revision right now and the first there's two books the first volume is due in on June 1st um and wow coming up been pretty pretty slanted and then the next one is Ban mechanics it'll be much later but um if you're willing to look through any drafts and you're very curious about anything else um you're welcome to send me an email and I can give you access and also that's just yeah feel free to send me an email and I'm happy to talk awesome sanjie this was awesome really really fun really informative um thank you so much great thank you very much I really appreciate it 
