**Refined Text:**

Hello and welcome. It is September 9th, 2024, and this is Active Inference Art Stream number 3.1 with Diana Omegi. Today, we will discuss curious sounds, prediction, and the human drive to engage with music. The presentation will be followed by a reading of comments and questions from the live chat. Thank you, Diana, for joining us, and we look forward to your presentation. 

Thank you, Daniel, for the invitation, and to everyone who has joined today. I am a Senior Lecturer and researcher at Goldsmiths, University of London. In today's talk, titled "Curious Sounds, Prediction, and the Human Drive to Engage with Music," I would like to discuss a statistic I recently encountered. The International Federation of the Phonographic Industry publishes an annual report indicating that the average person listens to approximately 20.7 hours of music per week. Assuming that individuals sleep about eight hours each night, this means that, on average, one-fifth of our waking hours is spent listening to music. This raises an intriguing question: what drives this substantial engagement with music?

In today's presentation, I will outline findings from neuroscience regarding the neural substrates of music perception and pleasure, which may shed light on why we choose to listen to music. I will then explore how the predictive processing framework can help us understand fluctuations in our attention toward stimuli over time. Finally, I will present a model of the dynamics of extended engagement with music based on insights from neuroscience and the predictive processing framework. 

Music perception involves many unconscious processes that are surprisingly powerful. For instance, we can identify incorrect notes in music we hear for the first time, differentiate between two melodies, and recognize familiar pieces even when they are performed with different lyrics or instruments. Furthermore, we can predict how a piece of music we have never heard before may unfold, and we may even be able to sing along to it upon first hearing it. Various theories have been proposed to explain these capabilities. For example, a model published in 2003 posits that acoustic input undergoes analysis, separating it into pitch and temporal components for further processing. These components interface with our stored musical phrases and patterns from previous experiences. 

However, this model oversimplifies the complexity of the brain's network of connections, which includes subcortical structures and the auditory cortex, as well as frontal regions involved in complex human functions. Notably, electrophysiological responses to pitch—measured intracranially—align with predictive coding principles. Early tonotopic structures provide frequency information to later areas where an abstract model of pitch is formed, facilitating predictive signaling.

Most individuals can recognize and discriminate between musical pieces, though not everyone possesses these abilities. For nearly 200 years, literature has documented congenital amusia, a condition characterized by difficulties in music perception, including melody discrimination and recognition. Early research suggested that individuals with amusia struggled with fine-grain pitch discrimination. However, later studies indicated that these individuals may also experience difficulties with pitch memory, prompting questions about a more generalized mechanism underlying this condition. 

During my PhD research in the late 2000s, I collaborated with Marcus Pearce, who developed a statistical learning model of musical expectations. This model learns from the regularities in a corpus of music, predicting the information content and surprise of musical events. Our studies showed that individuals with amusia exhibited diminished predictive error signaling in response to musical notes. Subsequent research corroborated these findings, revealing reduced connectivity between frontal and temporal areas in individuals with amusia. Additionally, structural connectivity analysis demonstrated compromised integrity of the arcuate fasciculus, suggesting underconnectivity between frontal and temporal areas.

Interestingly, research has expanded beyond the traditional auditory areas to include regions such as the insula and gyrus, which also track the information content of musical events. This adds nuance to our understanding of how surprising musical events can elicit physiological arousal and engagement. We found that the size of the prediction error response correlates with physiological measures such as pupil dilation, modulated by the entropy of the musical context. 

Listeners bring diverse backgrounds to music perception, with some possessing musical training that enhances their engagement with music. Studies indicate that musicians exhibit heightened activity in motor-related networks when listening to pieces they have learned compared to unfamiliar compositions. This suggests that music perception involves not only acoustic processing but also areas related to movement and motor planning.

We have proposed that music perception operates through two primary pathways, akin to the "what" and "where" pathways in visual perception. The ventral pathway involves core auditory areas communicating with inferior frontal areas, facilitating the registration of expectation violations, while the dorsal pathway engages parietal and premotor cortices, allowing for temporal processing and motor manipulation of stored information.

Regarding musical pleasure, significant contributions have emerged from research at McGill University, where it was shown that pleasurable experiences during music listening correlate with activation in the ventral striatum and dorsal striatum. This suggests a nuanced relationship between the anticipation and experience of pleasure in music, with implications for understanding the connectivity between auditory and reward systems.

In cases of musical anhedonia, individuals demonstrate normal responses to secondary rewards but fail to experience pleasure in response to music. Studies have shown reduced connectivity between the superior temporal gyrus and the ventral striatum in these individuals, further supporting the role of connectivity in musical reward processing.

To summarize, my aim today has been to explore the underlying motivations for human engagement with music. We discussed how music perception and pleasure are informed by neural processes and predictive coding principles, while also considering the broader implications of curiosity and attention in music listening. Ultimately, our engagement with music is driven by various factors, including pleasure, curiosity, and the desire to learn about the world around us.

Thank you for your attention, and I look forward to your thoughts and discussions on this topic.

---

**List of Changes Made:**

1. Improved sentence structure for clarity and coherence.
2. Corrected grammatical errors and punctuation.
3. Used formal and academic language throughout the text.
4. Removed redundant phrases and avoided repetition.
5. Organized ideas logically to enhance the flow of the presentation.
6. Clarified technical terms and concepts related to neuroscience and music perception.
7. Ensured consistency in terminology and formatting (e.g., names of studies and researchers).
8. Maintained the original meaning and intent of the user's text while enhancing clarity and readability.
