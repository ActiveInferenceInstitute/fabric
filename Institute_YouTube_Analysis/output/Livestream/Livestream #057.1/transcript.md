hello and welcome it's May 28th 2024 we're in active live stream 57.1 welcome to the active inference Institute we're a participatory online Institute that is communicating learning and practicing applied active inference this is a recorded and archived live stream please provide feedback so we can improve our work all backgrounds and perspectives are welcome and we'll follow video etiquette for live streams check out active inference dorg to learn more about live streams and other projects okay today in 57.1 we're continuing the discussion of the paper on active data selection and information seeking with K friston and Thomas par joining so thank you both we did 57.0 covering some of the back round and to begin the 57.1 whoever would like to go first how about Thomas lead us into the active data selection setting SL Challenge and introduce yourself however you would like thank you the and go for it okay so to introduce myself I'm I'm Thomas I've been working active infs for a few years um and I split my time between research and clinical practice which I think is part of the motivation for some of the later parts of of this paper uh relate to some of the clinical type problems that might face with us um I guess the background of this particular paper is that a couple of years ago now I I was in Australia to present at the australasian Basin Network modeling Society if I hope I've got that right um who ran a really fascinating conference using Basin networks to model everything from volcanoes to uh to Coastal erosion and um and various other things um and there are quite a few clinicians involved in that conference as well in interested things like um respiratory diseases and Diagnostics um and one of the organizers of that conference happened to be running a special issue on basa networks and kind of invited us to to take part in it and so this was our contribution to that and the idea is to take some of the same principles that we use in active inference and some of the longstanding principles from from people like Stu studs like Lindley um to apply to the problem of experimental design and to think about how active INF line principles can be applied outside of biology sorry I'm talking too fast for you time thank you um okay Carl do you want to add anything to that or as we kind of begin no no no I I I repeat I'm an admiring spectator this okay okay Joy Thomas's Lucid account of this very pertinent application of some basic principles of optimal basian design epistemic foraging okay then no hackling okay let's go to some of Chris's questions and also as people ask questions in the live chat we'll go in there um okay so let's just start with time sampling how did you or in what equations or in what conceptual ways did you bring in time and the kind of time sampling that was explored in this paper so time is is a really interesting um issue in terms of how you model a lot of these sorts of processes um partly because a lot of interesting processes evolve in time but it also brings in some really interesting issues in terms of how you sample uh and how you how you gather information about uh and the most obvious problem there is that some things change and some things don't um so that brings you into the world of of a lot of what psychologists often deal with things like inhibition of return the idea that when you when you look at something you learn something about it and uh the time it takes for it to be worth looking back there will depend upon how unexpectedly things might change in that location um so that's a really important part of a lot of the sorts of generative models that that we deal with both in biology and out of out out of that area because you need to be able to deal with with worlds that change um and that that s calls into question how do you actually approach time in generative models there are a whole range of different approaches that one can take that so I think a lot of a lot of early work in active inference was formulated in terms of continuous time models working with differential equations um generalized coordinates of motion which is a where representing trajectories in terms of positions velocities accelerations as if you're doing a TA expansion and taking the coefficients of that and that would be one one very good way of representing a temporal progression a lot of a lot of the more recent work we do in actually been is based upon discrete time models where you see things as being a sequence of events and you just represent what happens at a sequence of different time points um in this particular paper we ended up using a slightly different approach and that was to to um to essentially use a basis set or a set of basis functions to um to to allow there to be some smooth progressions in time um rather than dealing with specific time steps themselves so it sort of blurs the gap between um between using um discrete time steps where there's a clear boundary on the side of it and continuous progressions where where there's no boundaries um by using a series of basis functions placed at different points in time you end up getting this sort of smooth progression from one uh sort of blurry time step to the next um so I I I think time is sort of incredibly important in how we how we think about these sorts of things again in terms of thinking about how things change um but also in terms of thinking what's the right way of representing time in a generative model awesome welcome Chris as well this definitely is reminiscent of figure 4.3 from the textbook which I'll bring up how would you contrast it or which figure should we look at to explore which variables are continuous or discreet this figure is the one you so this paper won't have anything specific but you're right that that probably is one from the textbook that deals with with exactly that this distinction um so here I think one of the later figures does use a sort of um basis set type representation to look at tempal progressions I can't remember which figure that is I'm afraid okay did you did you want to talk through this figure that's up maybe summarize this figure and so this figure was trying to trying to demonstrate in a sort of relatively simple um analytic case what it might mean and and sort of taking time out of it for the moment what it might mean to select those data points that are going to be most informative about um some underlying model uh and the way it's been set up here is to start with um uh so the sort of upper left plot here we're just showing a function um with a whole series of different possible parameters that might exist so you can imagine these Theta parameters that are sort of shown as one being bigger than the next um each represent which function might be used here um and uh and you can see that in some of the there's a lot of variation in certain parts of the function so for the green one you can see there's a sort of very big peak around zero but slightly smaller Peak for the purple one slightly smaller for the yellow um but actually there is also certain points where where it's not so obvious and you wouldn't really be able to distinguish it so they all more or less cross the x-axis at the same point and and on both sides of zero so if you were to choose CH somewhere on this plot to work out which function you were sampling from you're much better off choosing from somewhere around zero um then you are choosing at the point where where it's actually crossing the x-axis where the where all look very very similar and so this is just to get across the intuition behind this process of of selecting your data efficiently to be able to resolve between alternative hypotheses the best you can or even to estimate along some continuous parameter value as it would be here um so following the the sort of sequence round this um this graphic um the idea is that we can represent these things into several different kinds of generative model so here we're representing either a basing Network where you say my explanatory variable is this Theta that's generating some data y um and then we' have this Pi variable which like a lot of active infation repres our policy or our choice of where we want to sample so here this is choosing somewhere along the x-axis that we might want to um work out what the corresponding why might to be um the the factor graph representation deals with exactly the same model but but now expressed by putting the factors in explicitly and by factors I mean that our probability distribution which is made up of our PRI beliefs about Theta and a likelihood which maps from Theta to our data y um are now sort of explicitly put in as squares that represent the different bits of of that generative model the next step is to to use a normal or a forny factor graph which essentially just emits the circles so now the variables are represented by the edges the points that connect the different factors so you can imagine if we take our probability distributions so probability of latent states of parameters uh and the probability data given those and we separate those out into factors and we draw a square for each of the factors of that probability distribution and connect those up whenever they share a variable so the edges or the connections between the factors are the random variables might want to estimate and then I'm trying to see behind the um cameras across so the um the next step is is to use the forny factor graph because it's it's often quite a nice notation to use when trying to understand um message passing and the different things that contribute to um to the way we estimate these these variables so here for each different Square we can see there being messages being passed between the neighboring edges and when we combine those edges we can we can arrive at a posterior estimate of the marginal of the variable we're dealing with so this would be how we might go about estimating the theeta there but the key twist here is that we're now not just interested in message passing for the process of inference but we can also now start to interpret those messages um in terms of how they contribute to various different entries that together make our information G so here we have um both a conditional entropy sometimes refer to as an ambiguity and that conditional entropy essentially says um if I were to sample over here in a more biological setting if I were to look over here um how confident am I in in terms of what I would see or what how confident am I that these parameters will map to something meaningful and informative um the predictive entropy is is another form of of entropy or uncertainty that essentially says before I've looked at anywhere regardless of the parameters even if I've marginalized them out how uncertain am I about the data I'm going to get so the conditional entropy is how precise is the mapping or how imprecise is the mapping between parameters and data predictive entropy is just my uncertainty a prior a priority about what I'd see if I if I looked over that those things are quite interpretable when you start thinking in terms of experimental designs because a um predictive entropy I I if I'm already very certain about what I'd see or what what would happen if I performed a particular experiment there's almost no no point in doing that experiment there's only a point uh acting to find something out if I don't already know what I'm going to observe so this predictive entropy um is a positive thing that if if I'm uncertain about what I'd see then I should go out and seek that out I should get those data points because there's something potentially to learn however there are are certain circumstances where where all you get is noise and you may be completely uncertain about what you're going to get you know you if you just keep running a random number generator over and over again you're never going to become much more certain about what you're going to see next uh and so that's not a terribly useful thing to seek out and that's where this conditional entropy or ambiguity comes in in that it penalizes those things that are not going to resolve any uncertainty it's the way often thing about it is if you take the predictive entropy you have the total amount of uncertainty about what I'm going to observe if you then subtract the conditional entropy from that then you you you're left with the amount of uncertainty that you might be able to resolve and I think this comes back in in a bit more detail and we can discuss that a little bit more in some of the later figures if we get to those um but essentially the difference between these two things is our expected Information Gain uh which is also a mutual information can often be expressed as the difference between those two entropies how much am I going to update my beliefs how much am I going to change what I I believe as a consequence of making a particular observation thank you Thomas that was a great summary I think we'll return to this I wanted to bring up figure 4.3 from the textbook ah yes yeah so this is and yes anything on this and to kind of juxtapose the conditional entropy with the a matrix and the sense making and the mapping and then the predictive entropy to the B Matrix and the transition among latent States yes that's right so um for those who haven't haven't seen these before these are again of factor graph type representations where we're now dealing with um temporal progressions um and so the the graph at the top represents a series of States as they transition from one to the next over time there may be more or less uncertainty in those transitions so I might be very confident as to how a sequence progresses if there's very low volatility Dynamics are very precise um and then and so those the the factors label three and the factors label two represent the way in which my data regenerated given some states of the world so I might not be able ble to directly observe the s's but they will give rise to the O's or the observable outcomes um and exactly as D says there's a a link between the um the two different terms in um in this expected information game so the conditional entry will depend almost entirely upon this mapping from states to outcomes upon this um conditional entropy um will depend upon this light C distribution and so it will say that if I'm in this state then there's a very precise mapping to the data whereas in in alternative states there might not be and so that allows me to then select between between these things based upon policy Pi I choose um the the sorry the predictive entry will actually be a function of both forms of uncertainty so uncertainty in the transitions so if things change since I last looked there and my predictive entropy might be very high and I might want to look back there sooner again in psychological terms that's a sorry the lights have gone off I'll see if I can turn those back on um the um so the the um predictive enty deals with that sort of inhibition of return um if it's due to the transition uncertainty however if it's due to the uncertainty in the um in the likelihood then that will remain relatively fixed over time and I won't be able to resolve that sorry let me see if I can sort out those it's better so the figure on the the sort of lower part of the screen so the the below graph is actually a similar representation but now in continuous time so here we're now dealing with the the states X might be continuous variables the X Prime is the um rate of change of my um my state X the X double Prime is the acceleration and if you know all of these things and further coordinates of motion you can start to construct a trajectory or or or predict anticipate both the future or the near future based upon these variables and the recent past as well um and exactly as D's written there it's it's effective Taylor series expansion where these are the coefficients of that AA Series so these would be sort of the two ends of the spectrum dealing with sort of a purely discrete time process and a continuous time process there are various ways of either blurring those boundaries or even um even of combining the two so maybe now H having brought this out could you return to the basis sets and comment on how it's similar or different to the time handling in the discrete time or the continuous time setting yeah so um so the use of Base assets will often imply that you're dealing with A continuous function but rather than have variables that represent either positions and velocities and those things I mean what one could argue that actually a generalized coord the motion system is a form of basis set in itself it's just a polinomial basis set So you you're just waiting sums of of different um orders in a polinomial series um but there are other basis sets that that one could use and those include things like um fora basis sets or cosine basis sets where it would be difficult to sort of localize things in a specific point in time but is much more frequency based um or as as happens to be done in particular paper um it's a series of gausian basis functions and those are centered on different points in time but because they're Goin shaped the the boundaries between them a little bit blurred so the value at any particular Point how much you weight that particular function localized around a specific time point will also have an effect on the neighboring time points and that's effectively a way of putting in a belief that that there although there's a temporal progression and perhaps a series of discrete steps there's actually a smoothness to that as well there are no discrete jumps in this that actually are just adding subtracting functions that vary smoothly to construct that overall picture and that gives you quite a flexible way of representing a lot of a lot of often quite nonlinear functions I think from memory that um har actually used sort of basic set type function um in some of the covid modeling as well in some of the later papers to model things like seasonal fluctuations changes in um transmissibility over time so they have quite a there's quite a broad um way of using them the the downside of using a lot of those functions that they're often less mechanistically interpretable so there something that that are often quite convenient in terms of getting a a flexible function representation wouldn't necessarily speak to a specific mechanism thank you Thomas yeah also basis sets are brought up in SPM and I think one advantage that's raised there is how it allows the prespecification of all of the oscillatory patterns of a given frequency range and then filtering the data with that pre-specified seeve in a way um interesting and and compressing information about the hypothesis of what structure the data could have like nearby things being more related events in time being more related if they're close so on yes and compression is a good point there that actually as soon as you start using a basis set you've now got the option to to stop after a certain number of functions um so in generalized coordinates of motion that there's a limit to how many um additional coordinates is normally useful to put in because the the degree of accumulation of the uncertainty means that once you be on a certain point it becomes so uncertain that it doesn't necessarily contribute to the earlier terms sequence um and that sort of arises as a function of of Auto correlations over time um and and you're right to say that using um using things like frequency based forer type basis sets also gives you that opportunity for compression um and I think a lot of a lot of imag image processing does use things like the discrete cosine um transform which is based upon series of cosine basis sets um where you can throw away certain um orders of of that or certain frequency bands um without losing too much information awesome okay let's go to a second question by Chris but Carl or or Chris just raise your hand or totally go for it um let's go to the second one or let's ask this question by upcycle Club do you think that there might be any algorithmic modifications metah aristic or hybrid approaches that could potentially lead to even better data selection optimization [Music] probably um I I suppose it's a matter of thinking about what so hybrid approaches um it's a it's a question of what what it's being hybrid with um and I I imagine we may come on to to some of those questions um later on as well um so metah heuristics well I suppose a heuristic um from a basin perspective is is just a prior and so I think it's exactly right to say that with the right priors in place for the right sorts of models and the right sorts of settings um this process can be made much more efficient um so if you if you know a reasonable amount about the structure of your problem um then you don't necessarily need to rely on on um basis sets and that sort of thing that actually often you can write down something with a good deal of information in it already and then your model doesn't necessarily need to learn all of that it just it's able to then use the resulting predictive entropies and conditional entropies to to pin down more specific things that that you wanted to know find out um so yes I'm sure that there it it's probably not going to be a one siiz fits all in many ways that the prior are going to be those that are most appropriate to the particular problem you're dealing with particular agent you have but ultimately they all I think lead come under the same underlying problem and the same underlying principle which is that um that if you select things in order to optimize your model of the world then it's about finding the best fit with your model of the world uh and that fit would would normally be Quantified using something like a free energy or a marginal that beard so I think it all comes from the same principle but if you're a different sort of creature or a different sort of agent than the implicit model you and the way in which that directs your exploration to find out more to to improve your model fit uh can be very different Chris or Carl want to ask anything or bring up anything uh not quite yet I'm kind of thinking through um you know talking about the time series and that sort of thing so I think we can kind of keep going on Yeah Carl [Music] um sorry I was just thinking about heuristics and um what it means to people in uh a certain kind of Economics something think about gigar renza and the like which I just read as as as prior the right kind of Prior um that have a minimal complexity um but I was just talking had what is a metah heuristic it's a heuristic next door to a heuristic So Daniel or Thomas can somebody Define for me Hyper prior perhaps yes that's what it must be it must be a hyper PRI that's what I'd assume okay thank you although you know what even is a hyper prior it's effectively just a prior isn't it yeah I suppose it's a prior in the context of a you a certain kind of hierarchal generative model absolutely yeah also there's a very practical element I think about chapter 6 of the textbook and the recipe for modeling so maybe to kind of bring in this practical element in the modeling of these adaptive data sampling what is the recipe for active inference generative modeling maybe even as you've learned or updated in the 2 years since chapter 6 I I mean I think the basic recipe is still the same um so uh so the often I think what we do is is think about what sort of um what sort of model that we're interested in and what sort of question that's supposed to address um and ultimately when you're using modeling in the context of of the data sorts of questions you might want to ask are things like um which of several alternative models is the best explanation um so doing a sort of discrete hypothesis test uh or um trying to actually recover particular parameter estimates so so coming up with posterior distributions over in an active inference context that will be over particular kinds of beliefs um that then determine Behavior um and that's particularly important in the context of things like um um computational phenotyping in clinical populations so trying to understand which beliefs um might might be and by beliefs I mean here sort of implicit Basin beliefs not necessarily something um explicit that you might be able to articulate um but what sort of beliefs might vary between different populations that then lead to particular kinds of um what we might think of as pathological behavior um and so you can start then ask questions in terms of if you can phenotype different people you can start to use those beliefs almost as if they were data that you can then ask other questions so how how does this vary with a particular diagnosis therapeutic intervention um so there was a figure in the book which I can't remember the sequence of Now setting out sort of sequence that you might follow if you were you were trying to develop a model and um and then use that to actually answer a question based upon some empirical data but I think the the key idea is um you're going to bring that up um I think that the key idea was if you have your model can predict some behavior from it you can then use use that model of um that that is effectively someone's brains model of the world but then you're modeling someone else modeling world and here we are yeah uh so I think in the um yeah this this one 9.1 so so so so the figure you've sort of copied into the lower left there is um is quite a nice way of thinking about um this process and active inference of modeling other PE other people or things doing modeling um so someone's brain um that you're trying to explain is itself self modeling the world so from that brain's perspective the data that need to be explained are sensory data so visual data SM sensory data auditory data whatever um and then as a scientist you're trying to explain the way in which that brain is behaving and generating behavioral outputs or electrophysiological signals or or whatever you might be measuring as a neuroscientist um and so my model of the brain's model ends up becoming the key thing and that's sort of sketched out in this particular figure um and the other figure that you had up um for a moment sort of talking through the sequence that one might adopt um in terms of how you might construct this uh and it's not necessarily linear I think as the arrows show um but essentially you need to need to think about what sort of task you want to uh form what experiments you want to perform um you need to think about what model that's optimizing um and here it's showing the partially observed Mark of decision process model which might in this case be the internal model of as rattin a maze from that you then need to construct a likelihood function which says given that the rat had this sort of model and these parameters in it uh what's the likelihood that it would um behave in a particular way so that that um P of you is the probability that they would have behaved in a particular way given that model and then we might equip that likelihood of some prior beliefs about parameters in that rats model um and combining the prior and the lik we can then invert that model in much the same way we do any other sort of Bas in inflence um and and there are further analyses we could then do on that so so this is what I was sort of sketching out um earlier on that if you have ramet estimates and you know the posterior for a number of different RS in a maze or or patient populations or um or even artificial systems of some sort you can then perform other analyses on that almost as if those posteriors were data points themselves and you can start to ask how do these beliefs vary as a function of diagnosis or experimental condition or whatever else thank you okay let's in the in these beginning Parts focus on what the model is and then as we kind of move towards the end in the second time talking we'll look more at the clinical trial and maybe the questions with um that are more open for the future how about connection with machine learning so Chris wrote um your examples primarily focus on statistical modeling how do you see active data selection methods integrating with other machine learning models especially deep learning architectures that typically require large data sets are there specific challenges or considerations in this context I mean I I would argue that almost in a in a sense the the answers in in the question that that these large data sets that they they often seem to acquire um is exactly the opposite sort of Principle as to to um what we'd be working with in this sort of Pap so the idea is how do I work with more efficient data sets how do I design not just models but also the selection of data to make sure that even with a smaller data stat it can be more informative um so there's I think there's a bit of a difference in the underline philosophy but I would also say that that most machine learning models are forms of statistical modeling in some sense whether or not they explicitly represent the uncertainty that um that you get out of them um I mean I I think it it is also what you know the comment about um hybrid approaches that was was discussed on one of the previous questions as well one way of of thinking about that might be in terms of using things like neural net work architectures and function approximators as part of the uh as part of a a basinal statistical model um and and that's sometimes referred to as a process of amotization you can fix certain parts of your function approximation and in a sense using the basis sets is also a form of function approximation that then simplifies um simplifies or in some ways makes it cheaper to do the process next time you need to do that inference you can sort of save a function that says given this input this is the outcome I should I should end up with or this is the inference I should arrive at um one of the challenges there is being able to go in the other direction and to be able to say if I'm going to actively select data then I need to be in a position where I can predict or generate the outcomes I might to get um and that might be something that that is um possible to do in the context of of generative approaches in in in machine learning um and I suppose all of the approaches we've been discussing really are generative approaches but but because I can now generate not just the outcome itself but I can generate a distribution over the outcome conditioned upon what I might do that's one of the key things in in being able to um being able to select those data appropriately and if you're using a model where it's very difficult to project that into that space and to ask um what are these conditional entropies then it does become very difficult to select data appropriately so using something that's sort of very highly nonlinear and and difficult to be able to to work with those things it does become more challenging to do that sometimes people might sort of res might sort of approach it using um um sort of sampling based methods to come up with an empirical distribution but by the time you start doing that you end up with something that is much more costly so if you're if the problem you're working with is one where you want to try and bring it down to uh to use fewer data points and to select your data more intelligently uh then often that does require particular kinds of model that let you do that in in a very efficient way there it's an interesting area and it be be interesting to see how how how you might interact these sorts of approaches um one thing you might imagine is that that actually if you've got a very clear unambiguous mapping that you can learn with a deep learning type model it might be that you could actually just treat the outcome or the labels that that are produced by that deep learning model as data from the perspective of another model might go about selecting the right sorts of of labels that are going to be helpful in terms of learning about things so there may be levels of a model that that can be when they're sufficiently ambiguous sufficiently precise mapping where you might not need to go detail and and um and select things in that way but it may be that the outputs of that are things then need to be combined um to form interesting mechanistic hypotheses about what happens next that's very interesting like thinking about different kinds of Communications and reliabilities of different kinds of analyses or computer systems and then it's it's actually the active data sampling conditional entropy in practice which is that reliable sources are pursued as part of the information foraging adaptive playout that every agent has to be involved in like reliability is its own suitability for an information resource and then the other side of it is actually resolving the the variability of the latent States however those may have extremely high fundamental variability so often pursuing the the conditional entropy the agent has more control over because they can resolve something more at least they can resolve that yes it's knowing what what is and isn't resolvable I think that that's key because if you were to just seek out things that were very uncertain you might be sat there pressing the random number generator over and over again without being able to learn anything new from it and attractor state that you end up being stuck in unless you can contextualize that with how resolvable it is I think one of the simulations we we put together in this particular paper um explicitly varied the um uncertainty as a function of where you are along particular number line and and that was just to illustrate the point that actually there is a so before this section I think yes it would have been this one right um so here you can see that we we're dealing with a function where the variance grows as you move away from from about X is zero and if you know that then you know that that actually the the informativeness of the data at the peripheries is much less because you're not going to be able to resolve a lot of that uncertainty no matter how much you look there uh and so if you see in the the choices plot in the lower right here um the solution it arrives at looks like a very sensible deliberate solution which is to start in the Middle where things are most precise then fan outwards moving left and right to resolve as much and certain as it can uh before it's then resolved enough the cost of sampling further data is is more the amount of uncertainty that might be resolvable where is it the ambiguity of the conditional enty were consistent across the entire line then you might expect a much more even pattern sampling and something that that in practice might not very different from random samping except for the sort of sequential effects i' I'd love to hear a first pass on how this figure addresses some of the similarities and differences between active data sampling and maximum entropy sampling yes so um I don't know whether you have one of the graphics of one of the previous figures where it doesn't have this imity to have um but so perfect yeah um so so this figure shows an example where the ambiguity is actually the same everywhere so the way in which the data are generated and you can see that because the you can see that that in the sample data plot there is a sort of well- defined function but the amount of variance around that function is more or less constant throughout that entire line throughout that function um so in this circumstance the conditional enty or ambiguity is the same everywhere that means that the only thing that really differs when you're deciding where to look next and where to sample next is actually the predictive entropy um and so here it will just choose locations about which it's most uncertain and that's quite a sensible thing to do because there's Noh that's limited any more than anywhere else in terms of the amount of resolvable uncertainty um and so you can see that the the prediction it arrives at after making a series of choices and and you can also see the series of choices it makes a fairly well distributed um uh that they're quite quite evenly spaced and cover a lot of that ground um and so the the prediction it comes to actually matches that uncertainty quite well um in a maximum ENT so this is this is completely identical in this particular setting the maximum entropy approach of selecting those locations that have the maximum predictive entropy that's very different to what we were seeing on the previous figure that the Daniel was showing the one where where it starts in the center and fats outwards um because if you were to adopt a maximum elentary approach here then then you can see that you'd spend much more time sampling in the far peripheries which is actually not a terribly useful thing to do here because um because you're going to just get a lot of Uncertain data you're not going to be able to resolve it very effectively and so here the big difference is that ambiguity AV verion forces you to start in the least ambiguous places and only once you've got the most information you can out that you start Fanning outwards and going for the the more um uncertain regions either side where there's actually less information overall that you can gain um because most of the uncertainty is now unresolved the other thing that we we we've done with with this plot and I think the other one as well is um specify cost to sampling cost to choosing the next data point um and and this is very much like we do in active inference simulations where we might um trade off uh exploration and exploitation so trade off having preferences for one thing over over another um and this is just to get across that idea that that there often are going to be energetic or computational or even Financial costs um to going out and getting more data um and those costs are specified just as prior beliefs about what I will tend to do in the absence of any potential information that I might gain so being a prior belief that can just be added to the log uh probability of me stopping sampling can just be added to the um the the the mutual information or expected Information Gain which then gives you an automatic tradeoff between um exploitation and exploration and effectly means that if I keep exploring one here we are exactly so this C here so once the expected information g once the amount of uncertainty I can resolve drops below um the probability that I I would um stop sampling then I will effectively just stop sampling so you get an automatic cut off as to when you want to stop um doing your experiments that's a very interesting piece this was this was kind of a cool aspect of it and thank you for that connection to the maximum entropy Chris want to ask anything or we can ask a question from the live chat or one of your prior questions um not sure it's necessarily a question but more I appreciate your answer for that um that last question that we were just discussing because it it makes a lot of sense and it almost seems like at some point in more complex models that you can't really necessarily know exactly what you're feeding um some sort of like machine learning or neuronet you might want to take a hybrid approach where you have something that you select a small amount select down your sample U your data down to a certain size and then you can kind of tweak it can optimize your model a little bit more by throwing a couple of random things in there but at least you have a smaller set that you're starting from um kind of get I mean is that something that you can kind of see doing is like instead of trying to sample down to something that's most informative sample out what is the least informative yes and I suppose that that's the same sort of approach isn't it as as to this starting in the the least ambiguous place with the low hanging fruit and then moving out from there until you feel you're not really gaining anything further and I and I think you're absolutely right because is as long as you're able to specify the cost of carrying on sampling and seeking out new data that gives you quite a principled way of working out when you stop doing that um because a pure Maxim momentary approach will carry on going um because there's lots more entry there's lots more uncertainty and so you might want to resolve that unless there is um cost to doing so any perfectly reasonable thing to do it there is no cost to doing so you can just keep looking um but as soon as you're dealing in a realistic setting where actually by doing this you're not doing something else or you're investing some resource be that energetic computational Financial um then then taking account of that can be quite a useful way of of doing that um I mean it's an interesting point about in more complex and large models and I suppose one of the key points there is that the best models are always going to be the simple models you can get away with um so if you have very large models with lots of parameters the advantage is they're very expressive can fit lots and lots of um functions or or different patterns of data um the downside is first of all that they become less interpretable but second of all that you expose yourself to problems of over fitting and one one of the key things in the active inference approach is that by minimizing free energy uh marginal likelihoods you're implicitly minimizing complexity so the best models always be the simplest you can get away with and by get away with I mean as simple as you can get without compromising the accuracy right and that Mak lot sense but but that's the key point isn't it so sometimes actually there is quite a complex process generating data and you do need a relatively complex model to be able to capture that without compromising the accuracy um so in in that setting it you know a really good question as to how would you go about actually selecting data um in a model that that is sufficiently expressive that it's fit for purpose in a particular domain um and I think a large part of that is being able to to actually quantify the um the likelihoods and the uncertainties in terms of how that model works and that is a challenge in in a lot of deep learning based models of neural network papers models but not all of them some of them will explicitly account for these sorts of things as well Coral yeah anything go for it yeah no that was just um very nicely expressed you a key issue here I was um I was just T taking my mind back to um when I first learned about Active Learning um as you of the kind described by people like David McKay um where you know the objective was to try and find the right data features that will resolve the uncertainty in exactly the way that we've been talking about because we've been talking about it largely in the context of inference um your inferring latent states of Affairs out there given some data um and clearly the original notion of active learning um pertains to learning the parameters of a generative model we can also apply the same notion to model selection or structure learning um and that resonates very much with you know the conversation um with Chris Chris tier um you know one can either look at um seeking out those informative data the data features that are good and good simply in the sense that they inform um your generative model in terms of ultimately maximizing the evidence or the marginal likelihood of your generative model or you can look at it as triaging bad data that's uninformative um and you can imagine that process a little bit like Maxwell's demon so the data that you don't go and actively seek out or if you're in a big data setting and you're trying to ingest or simulate large data what this notion of um Active Learning and active selection brings to the table is that you don't just blindly ingest all the data as current um deep learning does you carefully assess should I ingest this data point or not um and you know implicit in what to was Thomas was saying possibly explicit but I'll have to ask him um well have think what what what's that kind of objective function what is the if you like um another way of talking about The Information Gain that I would get if I accepted as Maxwell's demon this next bit of data and that uh Information Gain is just a mutual information so what you're saying is that you want to only take those data that are literally good because they are informative that will technically or operationally increase the mutual information of inherent in your model between cause and consequence so I think that you there's something really quite fundamental about this sort of whole notion of being active in the way that you deal with data and gather information that you know that underwrites active inference as you know um with its special focus on active sensing and Active Vision and active inference and active learning that you can actually go right through to active natural selection you know and one has to wonder now whether there's a homologue in in evolutionary thinking of this kind of active learning thanks for those points th those are those are great yeah Thomas go for it I don't think I had anything really to answer that I it's exactly the because it's it's being able to decide also upon your model space isn't it think it's it's part of the point that Carl's making there that actually it's the selecting your model but is also knowing which models even to select between is another is another key part of it that's certainly been a interesting area in terms of structure learning sted absolutely and I I really like the a call to evolutionary biology I think that there may be an analog there you can kind of wrap your mind around some of the things that you're doing if you frame it kind of from an evolutionary standpoint too it's alternative way of looking at it but I think it's quite valid yeah also um speak that those two points there they speak to um a key issue in structure learning read as Bas in model selection do you do you take a top down approach or do you take a bottom up do you grow your models carefully by accepting this mutation or this extra state in a factor or indeed an extra level in a deep generative model in the face of this data this evidence of how good this uh model is you know almost in the sense of adaptive Fitness in an evolutionary context or do you use good old Basin model reduction um and start with something that can do everything and then prune away in the bits that you don't want um I just thinking it from a biometic point of view or an evolutionary slash developmental at least neurod developmental perspective sort of Evo Devo it looks as though um biology does both doesn't it so we start off with a brain which is over param I over wired overc connected um and then carefully prune away using basic model reduction what we don't want um and yet of course at the level of um exploring that model or hypothesis space I would imagine we rely a lot on carefully crafted um mutations split and merge like operators that you know gently feel out into into different you know and from that perspective each of us is a data point point for evolution that's quite chilling isn't it lot to say on this um one really practical note with the cost of sampling and the opportunity to stop is to have different information foraging locations that have different costs like different apis you could call or different costs associated with taking on or holding a record so it's really true a multiple of you alluded to like it does reframe the question into creating this semantic kernel of understanding for example the clinical trial setting something that can be shown up on a graphical representation and then being able to use that purely in the generative forward direction to generate synthetic data with whatever other evaluative criteria apply to that or just to learn active inference and then also to use that generativity not just to make more data which is often of limited utility for retraining but actually it includes that adjacency of being able to ask how much another piece of information or evaluating the last incoming piece of information or rank the informativeness however in a way it moves all the challenge into how you define the gener model surprise surprise because you could have a model of information in the world that would lead to very various uh attractor States repetitive looking fixed looking and so on so it's it's pretty interesting how I guess that's the high road and the low road kind of coming to meet each other in the information foraging setting because the fundamental constraints and tradeoffs associated with sampling they do come into play and it does not solve the question it just sets up some of the objective functions that can be used and have interpretability for the these uncertainty [Music] situations okay I'll ask another question from the live chat all right Demetrios asks what is the nature of the Precision waiting mechanism in this process is it possible that high Precision of the prior blocks the prediction error to convey the newsworthy [Music] information so um precision and the balance between prior likelihoods and what determines which precis we might use and I know I suppose relating that explicitly back to this this question of information seeking so um so Precision can be applied to any probability distribution essentially so so you could have a precise prior you could have a precise likelihood or impr precise versions of either of those and this gets particularly interesting and and sometimes quite confusing in the context of hierarchical models where the prior from one level is effectively the likelihood or conditional distribution From perspective the level above um and so so you end up questions am I increasing prior or am I decreasing with the effect of the lik or and it it often depends upon exactly where you are in these s models but taken to its simplest level you imagine you have a prior and likelihood some the data that you're evaluating likelihood in relation to um the relative precision and it is the relative Precision that's important between the prior and likelihood will determine which one of those has more than final beliefs on your posterior we have a very precise prior it says I'm so confident about this that the matter what I see uh I'm not going to shift my beliefs and relating this back to the concept of of information seeking that would be the situation where um where it we a dynamic s um I think there's been so little change from the last time I saw something that even if I saw something new it would make any difference um the likelihood if that is very very precise um leads to very low ambiguity so if I believe there are very precise uh regions of space from from the perspective of my likelihood or regions of whatever um whatever sort of process I'm selecting between the different policies I could I could adopt um then I'm more likely to select those locations compared to others um and this is often in in context of neurobiology and psychology these are often how people talk about processes like attention so being able to say that my likelihood is very precise in context of spatial attention associated with these particular spatial channels or this area of space then that means I'm going to update my beliefs much more based upon this thing that I'm paying attention to um the link back to information seeking is that that because I might update my beliefs much more based upon this location in space I might act in such a way that I get more information from it I might move my eyes to look over there um or or take whichever action is relevant to whichever data set is I'm dealing with um and we can think of that as is essentially saying this region of space this region of of policy space is is more Salient than others because when I choose to do something that leads to that outcome I'm going to update my beliefs much more as a consequence um so I think that that sort of gives you a brief summary of of how priers and likelihoods interact with one another the the notion of prediction era came up as well which I think is is you know an important one to think about particularly when you're dealing with um with continuous States based models and perhaps most simply when it's a a simple linear gausian model because the degree of update um that you might make is effectively going to be um a linear error function where that error is the difference between what I predicted and what what actually happened and that waiting is going to be weighted by the Precision of the likelihood so the degree of update I'm going to make to my beliefs depends both on the Precision um how unambiguous I think it is and on the degree of dis on the amount of discrepancy from what I'd originally predicted and that's the same idea that then underwrites a lot of um a lot of predictive coding and a lot of neurobiological models um of course even that needs to be weighted very carefully by the prior precision and um the equivalent errors that you get once you deviate from your prize uh and that probably brings us back to this notion of lexity and the need to keep things simple because one reading of of keeping your model simple is that you keep things as close as possible to your PRI if you have lots of parameters and they're all deviating from that PRI quite a lot in order to come up with an explanation then that's effectively a very complex model one that I think would be penalized relative to a simpler model where everything stays much closer to the prize you have thanks Let's uh Go a different direction there was a very interesting and ambiguous use of the streetlight effect so what is the streetlight effect and how how is it kind of a representative setting and what do we do I love the graphics oh there were so many fun fun representations so what what what is it is it is it a um joke how how did you find what how did you find one with a pie oh just typing in a different symbols to see underneath the spotlight very good yeah gbt 40 um so the the idea of the street light actually comes back to this idea of ambiguity aversion and it's an idea that um can't remember where it first came from but I first came across trust and psychology literature um I think possibly a reviewer mentioned it in a paper at some stage um but the the street light effect is originally set up to be something to describe an unhelpful bias um and the idea was that if you're sort of coming home late at night uh and it's a dark Street and you drop your keys the first place you might look is underneath the street light and that unhelpful in the original reading because your keys are not more likely to go on the street light than they are anywhere else and so it's a metaphor for seeking out those sources of information that are most easy to to um to acquire and and um that you can learn the most from very quickly of course from an active inance perspective that's actually a completely optimal thing to do that if you have some uncertainty to resolve very first thing you should do is look under the street light because then you you'll establish that your car keys or or your house keys are not there uh and then you might start looking elsewhere um so another way of putting that is that the least ambiguous place the place where there's the clearest mapping between the location of my keys and the visual data I would get is under that street light so if I'm ambiguity averse if I choose those places where that conditional entropy is lowest then I'll start the space under the street light and maybe I'll then f out from there just as we saw in the simulation results that you put up earlier on yes what a what a comical plot twist that it's like it's a joke because it brings us to the point of laughing at the person looking under the street light and then it leaves it open like but what should they have done and then this isn't exactly what they should have done um in this paper but just like you said it's a reasonable starting location especially if it's where one finds one to be so it's kind of interesting um Carl or Chris no I I I love that yeah yeah and I haven't heard it phrase like that before it's very nice Chris no nothing really I I I do like that analogy though let's how was equation three and this seemingly very general usage of of Information Gain on uh Bas graphs how did that come into play and maybe if you could just Define how that was used so the idea here was that that just as in um a lot of Bas and inflence problems where you're do relatively complex gure models um appropriately complicated generry models that help accurately explain data um that there's often quite a lot of sparsity in those models and there might be multiple different different nodes and edges um that one might need to take advantage of or sorry that represent variables one might want to draw inferences about and so there's a long um sort of series of papers and advances have been looking at ways to do that efficiently from an inference perspective uh and that that has led to many many forms of message passing algorithm um relying upon different approximations to to posterior probabilities those include um sort of um trying to remember what the very early ones I think the bound well Shard algorithm and things like that were sort of early versions of what then ended up becoming fully basian type approaches and the more General forms would be things like variational message passing which relies upon being filled approximation so the idea that I can factorize all my variables or the the posterior probability over all the different variables I'm dealing with and by not having to deal with all the the posterior covariances and conditional dependences I can come up with a much more efficient but still relatively good and useful uh set beliefs about about the causes of my data um then there are more um more accurate algorithms that were then uh developed as well so things like belief propagation some uh some product type algorithms um which implicitly do take account of pairwise dependences uh between different variables but still allow you to come up with good marginal inferences uh often these rely upon subtly different free energy functionals so for instance belief propagation implicitly comes from something known as the BET free energy which is B um and that that effectively assumes the posterior factorizes in such a way that that I can take account of all the pairwise uh dependencies and then renormalize by all the Singleton so the distributions of each single marginal in my system and that allows you to to achieve exact inference from the perspective of the marginal distributions you get out of the end the point of doing all of these sorts of things and you know there are many others you expectation propagation and various other based upon other free energy approximations and one more General is is kuchi free energy that can be factorized again different ways but the the purpose and the advantage of doing most of these things is that it allows us to um to make use of the conditional dependency structures the spacity structures um and implicitly the markof blankets in a um in a network and um ask questions about specific variables solve that in quite an efficient way to be able to say what are the beliefs we have about the idea behind the way this equation is phrased is that you can actually use those same messages and the same cursive structures where um beliefs depend upon the other beliefs in the network and the message passing the results of that to work out the information gain about specific variables so you can you can make use of exactly the same um technology that was used in things like belief propagation schemes and which underwrite a lot of neurobiological message passing schemes so this message sorry this uh equation um uses the notation CH to mean children so that things upon which um the things that depend upon the variable we're interested in uh the parents um which are the things that it depends upon um and those things and the parents of the children she'll see so in a in a couple of locations here which um together so the children the parents and the the parent the co-parents of of the same children together make up the markof blanket which is all the things that um that if I knew about would give me all the information I needed about the thing I'm trying to infer so I only need to know about the local blanket surrounding a particular node and not the rest of the network because if I know about those things that tells me everything I need to know and so this is just a simple way then of of um taking advantage of again that sparcity to be able to come up with efficient way estimating not just the posteriors but the Information Gain I could potentially achieve about those posterior given particular policy or the information I could gain about those variables Rel with posterior relative to Prior so the in the textbook there's an example where the generative process and the generative model have the same form like if they're both generating from a gaan then those distributions can truly lock but also it's possible to have the form of the generative process and the generative model diff so like the generative model is doing some categorical two cold just right too hot and then actually temperatures a continuous variable and so on so how how does the selection of family of distribution that's even used relate to the um in Information Gain like in table one uh okay I I think um I I'll just try and unpick what I think possibly two different things there so what one was this notion of whether the generative model that we might use are are prior and likehood actually match the way in which the the world rers generates those data and I think that's a really important thing to consider and exactly as you say there are often situations where um the best model is in a sort of lay sense the wrong model um in that it doesn't match how data are actually generated um but is a simpler explanation that it complies with o razor and the reason for that is you can always generate the same data in a more complex way so if you can get down to the simplest way of generating it that's often the best way one one of my favorite examples of this was some um something I found out a while back when when we were doing some work on modeling of ir movements and one of the things I found is that it's much simpler to model your eyes moving together as if you just had one eye and then make predictions from the perspective of both eyes because both eyes move together you know excluding sort of subtle things like convergence um most of the time when you want to decide upon a gaze Direction you don't need to decide on separate gaze directions for each eye you might have you a chameleon or another sort of creature that can move their eyes independently but from our perspective you can make do with a generative model that is much simpler um than than the physics of your eyes uh where actually there's nothing to say that you couldn't rotate two different spheres in completely different directions so by by doing that you actually end up with a model that is wrong but is also a much better model than one that includes lots of additional parameterization and degrees of freedom that are just never used in reality um so I think that that sort of um is is quite a a nice intuitive example of where your model might differ from that in the world um and then perhaps related to that is the other point you were making about whether the particular distributions in your model are consistent with the distributions in the world um or even how do particular kinds of distribution um forgetting whether or not they're they're accurate models of the world how do they affect how you then see seek information um and it it of course varies so um so one of the reasons for including the table that you showed is that the form for the information gain will be different depending upon which kind of you have and that's just useful to know in sensitive your writing down want to know an efficient way of sampling from it it's going to depend upon what format model has yeah so one of the big distinctions that we often see and often make use of is this distinction between categorical models and continuous models um so the the um two middle rows here are dealing with categorical or dis lay uh priors and they have a particular form in terms of the information gain and the way in which you do that updating pull out that the ambiguity bits and the predicted entropy bits um the lowest Row third row shows the form of a gy model and one of the key things you can see in the gy model here is that the um the covariance predicted so this Sigma subscript y um is the prediction of the predicted variance for the data and so that is going to scale with the ambiguity so you can immediately pull out from that how much ambiguity there's going to be just by looking at um at this term here exactly one you high highlighting um now if that depends upon Pi then the ambiguity really matters if that's constant as we saw in some of the earlier ones and it doesn't depend upon P then the only thing you need to take account of is actually the first bit which is the the um the maximum entropy bit of of these equations um so the form that you choose for your model is absolutely key in terms of how you then go about sampling the world uh and how you optimize your model is then absolutely dependent upon how you've subel the world so you get this sort of nice circularity that then determines how you build your model based upon what you see but based upon how your model currently is and what's uncertain about and what there is to resolve you're going to considerably change how you go about samping it that's a really fascinating point and it makes me think about learning like in a given situation one could over or under learn or however comes to be that their policies do matter for resolving uncertainty and then that could set up the balance between continuing to sample from distributions and or stopping them yes that's an interesting draw because you could start with a model in which um don't know about how your policy affects for example this Co variance but it may be that over time you realize that when I do this and end up in this sort of State actually it does affect it or when when I take a particular action and so if you then optimizing your model and then then you go one step further and you say well how do I how do I learn which um or how do I infer which parts my model will be affected by the actions I take uh and how do I choose the right action so that I can best learn about which bits of my model will be affected um and I think that's a a really interesting question as to how you might approach that so we could have parameterized here or we could have included additional parameters that are how it does the covariance depend upon policy and it might be that if those parameters will Zero there's no dependence all but as you learn more about it might then learn actually there is a a function that affects the ambiguity uh and if that's the case then I will change my behavior quite considerably once I've learned where the least ambiguous places to connect this to language interfaces and and chat and just different kinds of of language models that people can interact with so in human interacting with with other and with how we design these generative models to be this kind of awareness of of what parts could be influenced if I were to prompt this looking into the how much learning would be expected from a distribution of what responses are being generated from the chatbot so like the distribution that the chatbot actually fleshes out is its irresolvable uncertainty and then the question of which prompt to enter next or which prompt to provide could be guided by wanting to select the single best or from among the best next policy selections to reduce some uncertainty and so understand if it were a deterministic API on the other side like a chess game then there would be only the it would be a simplified decision of what to prompt like a calculator whereas with a probabilistic model there's sometimes small sometimes very large uncertainties on what it actually is out there so sometimes it it dwarfs what can be resolved but we don't necessarily have an awareness or ability to estimate in these different situations what that balance might be however these these methods whether the models generating process is like an active inference model or whatever happens to be a sensor this can help interoperate the different information resources and patterns by putting them in a common epistemic plus pragmatic Plus cost setting yes I think that's right and if you if You' got if you got a sufficiently expressive model that and a reasonable degree of uncertainty about that model space you can actually start to exactly learn how you even go about doing these things that as your model becomes more sophisticated of the world learn how to interact with it better in in all three of those domains epistemic sorry two domains you you mentioned three what's the third one epistemic pragmatic and cost ah okay so I normally think of the pragmatic as as effectively yeah compassing the cost but yes yeah absolutely but the cost playing a special clear role in this paper with a finite stopping threshold for sampling yeah but but in a sense it it the it's not a a sort so the cost is affecting the samping but it's not sort of just putting a clear time threshold on I'm going to stop at this stage it's something that sort of automatically reweights itself relative to the amount of information that might be left to resolve I think that'll be something very relevant that will be explored for example if there is a value that can be placed on a given piece of information and that could be how much it influences some trade or it could take into account like the cost of paying the employee for that much time or using the computational resources then it's possible to allocate fixed or variable costs and stopping times with completely separated from the epistemic and pragmatic generative model itself but with a cost as purely an exit boundary and not be not directly consuming the output of the free energy calculations yes and that that made me think of a couple of things so what what one is that um that one of the advantages of that sort of cost is that in a dynamic environment it may be appropriate at some points to rest and then as uncertainty accumulates Beyond a certain point you might want to go back to sampling so you have a sort of time or cut off after which you just stop uh then then it's no good for anything that might have changed but if you if you've stopped for a little while uh and decided not to put in those costs you can then decide when's the appropriate place to start S not just a stop the other thing I thought was interesting about about what you said is is um again this notion that that to some extent although from an active in perspective and certainly From perspective of this paper information in itself is valuable uh and so the the value of information is in terms of how informative it is regardless of how it affects what what um might gain in the future however the how much you might gain in the future may also be relevant in terms of that pragmatic Mar um and this brings in some of the ideas that have come through St at inference like schemes um where where you start to evaluate these things recursively so in in in a lot of reinforcement learning um in probably most of it where there is Information Gain it's often um seen as the information that will lead me to the uh the result I want it will lead me to more reward uh and so creatures May learn a path through their environment that um that is the one that more reliably leads them to reward because it tells them where to go earlier on something those lines um however that one could say that that and that is important but one one could also say that it's important to learn about your environment for its own sake as well and one of the things active inference does is it doesn't put any special um focus on either the Fulfillment of your preferences or the seeking of information but the interesting connection between the two is that often seeking out information will help you fulfill your preferences in the future and so if you're able to recursively evaluate if I knew this what would I do uh and if I did that what would I learn and if I knew that and keep going into the future you can also select uh things that will give you information that will help you satisfy your your pragmatic value as well so information ends up having a dual purpose first of all in being valuable in its own sake but also being Val in terms of what else you can can gain from it in the future both in terms of information but also in terms of in terms of the um pragmatic value to be able to achieve I think we'll we'll come to that with the um pragmatics of epistemic value but just one note on the message passing uh you mentioned how with the stop in Criterion it's possible to think about the information foraging setting not just as like where to look every time a metronome clicks but bout scheduling bouts like batching emails or something like that and that reminded me of our recent discussion with some of the RX and fur team about the RX environment package which uses the reactive message passing to uncouple the time in of for example the nestmate and the niche in a generative model so that they can be on different timings and the computations occur on demand like in a node local way and can have uncoupled timings so it's just very interesting how this pathway that is laid out in the paper about how on one hand we have the graphical semantic layout of what these variables are and then how many preparation stages before it all gets played out as the relationship of the equations that we get from the entropies so in a sense this is sort of setting out the process that that um someone might go through if they were trying to unpack this this sort of modeling in practice you might asse that actually a lot of it just starts from this message passing stage as it's the interaction directly with message passing and what you sample and a lot of the graphics along top really to set out the equivalence between different sorts of formalism which some people are very familiar with using one sort in principle you could tell this story without referring to a Bas of network without referring to a fact graph um and one of the big focuses really here was just trying to point out the way in which they they're inter related Carl or Christopher we can go any [Music] direction you I go in two directions because there two really interesting issues there just you I can't resist um perhaps in reverse order that you the notion of reactive message passing I think um is not only crucial and again very biometic uh but also um very relevant in terms of computer science and efficiency in you know in terms of implementing um any kind of active inference or learning scheme um it actually inherits from something called the actor model in computer science which is a big thing which led to the development of a whole Suite of different programming langu which is you know um one of the most popular nowadays is rust um that is predicated very much on this notion of um responding to to queries and um requests for computation and the products of that computation which it's very very comfortably with um the you know the maths of message passing on Factor graphs so I think the RX infer reactive message passing is exactly the right way to go not only in terms of understanding biometic Dy DCS and belief updating but also in engineering the next generation of um deep learning or artificial intelligence of a more generalized sort um but the other direction I wanted to come back to this I think really important discussion about is information is the value of information intrinsic to the information gain or is it always conditioned upon your ultimate goal um I think that's a really quite a profound uh point um and I have a very specific take on that that inherits from the physics you get from the free energy principle so if you just write down what is the probability distribution over paths into the future and then you take the logarithm um of that probability to create a potential that is the expected free energy so the expected free energy that can become carved into the expected Information Gain and the expected log PRI preference or expected value falls out of the physics of self-organization and that decomposition tells you that these things are linearly separable it means that there is no value of information about the reward that does not exist because about the reward suggests you can't linearly separate the expected information game from the expected value so this notion I think that you um only have to learn about that which you need to know in order to get your reward I think it's fundamentally wrong and one has to ask where did it come from and I think it's by putting a certain kind of teeology and semantics on the physics of sentience and you know senent behavior that leads you to that false conclusion because now what does value mean well everything has to be in relation to the reward or the goal but of course the value of information is not another another way of looking or if you like um dissolving that that sort of um disconnect between the intrinsic value of resolving your uncertainty about the world as opposed to the extrinsic value of avoiding expected surprise um where surprise is read as a sort of negative log of your prior preferences is just to think about uh to acknowledge that we're talking about um effectively lran or potential functions because they're log probabilities so when I say you can linearly separate them what we are separating in a linear non-interacting way are the log probabilities when you actually combine the two probabilities what that actually means is that I will only go for Information Gain if it satisfies my um my goals or equivalently and symmetrically um I am only going to satisfy my goals if I get a sufficiently high Information Gain so they both contextualize each other in probability space even though uh they are L they don't interact in terms of information or um in terms of potentials or log probabilities the final point is that you know this non theological compilation of the value of information and the value of conforming to the constraints supplied by the prior preferences is that that is exactly what you get when acknowledging that the free energy principle is dual to James's principle of constrainted Maximum entropy so in this instance you can just rewrite or re your reinterpret um the free energy Principle as an instance of constraint maximum entropy principle and you were talking before about you know the benefits of Maximum entropy and this is one very simple instantiation of it where the entropy in question is the entropy of your posterior or your measurement but it's constrained so now rewards are just ways of writing down constraints on an information maximizing system or a maximum entropy um uh system and where do those constraints come from they come from the generative model where does the Gen model what constitutes generative model it's just the prior preferences in your generative model from the point of view of the free energy principle so by if you like appealing not just to the free energy principle but James's constraint Maxim entry principle mathematically that speaks to a very very clear picture that the intrinsic value of information has got nothing to do with Rewards or goals or constraints it's just a way of writing down a constraint and you can actually articulate this in terms of The Grange multipliers um where you can either look at the information seeking as constraint by reward or you can look at reward seeking as constrained by uh information but in log space they're completely independent that's that's what came to mind I think it's a really interesting point which is you know confused me for years and and you know also is a recurrent question when you talk to people in reinforcement learning who have this if like gamblers fallacy about you the relationship between the two kinds of uh two kinds of value thank you Carl that was amazing a few notes the additivity of the log probabilities is the in the is the um multiplication of the probabilities non log so just properties of logs so it's fascinating how they are both cross calibrated and referenced and decomposed from generalized free energy or expected free energy considerations and in practice there's the parameterization of uh what the actual values are however you have given a very full-throated defense of the pure value of of Information Gain and that was something in the in the approach of this paper that Christopher and I had explored a little bit how it starts with the information gain and then the end of the paper brings in the pragmatic value as opposed to looking to expand the epistemic scope of reward and reinforcement and introduce some kind of variability or learning or or intelligence layer other than that starting with the information gain and then tuning the regimes of attention and action from there so that's very um interesting Chris what do you have any other thoughts or directions no I think that uh that really everything's kind of been captured here um very nicely and I'm still trying to process through some of the things that we've discussed now um I just really appreciate how the paper's written how well it has it you know mentioning what Daniel said how it builds from a single kernel of information uh Mutual Information Gain uh and then it just builds out the model from there into something that is more and more applicable to different regimes so you can kind of stop at each point that you any point at any point that you want um and then it just kind of grows into something that is you know immediately applicable to clinical trials which is quite wonderful and I really did appreciate that um I'm just going to have to sit through and probably process some of the things that uh we've discussed here today yeah I think that uh let's just in the last minutes think about what we might want to process over the next week and prepare some questions it might be cool to see the code so Thomas if you want to have it loaded up or we can uh try to prepare it on one of our machines Christopher um we uh yeah or Thomas what what would you look forward to discussing or doing if if if you'd like to it's it's it's all available on um on uh GitHub repository um and I think it's set up so that if it's run it will generate a series of animations showing process of sampling as well so I if if you could if you could yeah that would be awesome yeah let's um return on that and look at the code what's the easiest thing would you like me to send you something there oh if you could just prepare to share your screen with the code running and then we can start with the the just a little bit of looking at it just seeing what the code does and then in the in the second part talk more about the clinical trial setting and how it it um is all brought together in the section six sounds good cool any last thoughts from any of you I'm here thank you Thomas and Carl this is awesome to have you join thank you Chris for helping prepare this too so all right see you next time thank you guys have a good one bye for 
