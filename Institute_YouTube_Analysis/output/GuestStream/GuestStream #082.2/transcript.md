hello and welcome it is June 3rd 2024 we're in active inference guest stream 82.2 with Robert Warden today we're going to be discussing three-dimensional spatial cognition bees and bats so thank you Robert for joining again to you for the presentation and looking forward to it thanks Dan okay well as Dan said I'm talking about three-dimensional spatial cognition in small animals particularly like bees and bats for examples and what I'm going to be doing is showing you a demonstration program that does this and so you can find the demonstration program at this link at the bottom of the picture and you can download it and try it yourself or you can read a couple of papers about this work which are there on archive at that address so so that is the the introduction to get straight into it um what this work represents I think is a challenge for classical neuroscience and by classical Neuroscience I mean the assumption that all that happens in the brain all cognition is done by neurons connecting to each other by synapses and so on and the challenge which I think comes out of this work is that the main result is that neurons are actually not capable of that they cannot represent three-dimensional space because they're too imprecise and too slow so the resulting challenge for Neuroscience is to show this idea is wrong everybody thinks it's wrong everybody thinks neurons do everything so you have to show it's wrong by building a working model of neural computational model of 3D space and checking that it really scales and can perform somewhat like animals perform I think just writing papers and talking about is not enough you actually got to build a model and show it works and for building that model the FP neural process model is the best starting point I believe so that's where this talk is going uh it gets there by of 3D spatial cognition so what is that spatial cognition I just first say is a very important problem the primary task of any animal brain is to control its movements physical movements it limbs in 3D and that's a 3D problem and it has to do that at all times of the day and for most animals most of their brain is devoted to this problem and we believe being basian that they do this by building and using basian maximum likelihood model of three three-dimensional space and I my previous live stream was about the subject how animals build models in general but the particular 3D model of space that's been important since the caman era When Animals first started having precise Spence sense data like good eyes and capable Limbs and that there's been huge and sustained selection pressure on all animal species since then to do it well and what we believe is that animals do do it rather well for instance our own conscious awareness of 3D space must come from something going on in our brains and that is a rather precise model in our conscious awareness of 3D space around us so that must be quite a good model of space in our heads and going from us to small insects even a small insect can land very skillfully on the room of a coffee cup or any other surface so that's why I say modeling 3D spatial cognition is the top priority for neuroscience and we may look at all sorts of problem s in Neuroscience this is the one this is the hard heart of the problem the thing we really need to get right so how do you do it how do animals build a 3D model of the local space around them and the immediate problem is that most of their sense data in their vision is two-dimensional so how do you get from two-dimensional Vision to three-dimensional model of space there are some constraints and people obviously think of stereopsis with two eyes where you can tell the depth of things or appropriate setion and touch but those constraints only apply to restricted regions of the space around an animal so for the rest of it what I believe animals do is they build a model of the space around them by moving in space and this is a form of active inference if you like that you have to move in order move in space to find out about space and this is based on a very strong basian prob probability that as an animal move most of the things around it not do not move so the world is like a big rigid moving body around the animal so the animal can compute object locations by what is called structure from motion sfm and when you build a computational model of this is actually fairly simple computation to do uh what you can do is fairly simple 3D Matrix operations to maximize the likelihood of an object being in a certain position and that's what this program does does but if you're doing that shape from motion structure from motion it requires a short-term spatial memory a working memory positions of things and that's what this demonstration program does so the demonstration program B is have Vision some limited but not very bad resolution but to have e echolocation instead that gives them both the echo delay and it gives them a Doppler shift and we'll talk about that later so both of those animals move fast among static objects so this 3D shape from motion is an applicable way of working out where the objects are so the program we're going to show you builds a 3D model of space in three different ways firstly it can build an optimal basian shape from model model and that is uh what I call a Brute Force calculation uh I've discussed this in my previous live stream and it's very hard for animals to do it's not the way we think animals do it but the interesting thing about it is it gives you the very best possible Basin model based on the sense data so that's the first way the second way is by dynamical object tracking where animal makes an estimate of where each object is in three dimensions and then it keeps updating that estimate every time from every step along the track it takes it updates that estimate um from its sense data and the third model is doing that same object tracking but doing it in the presence of neural memory noise I should say this computational model is built at Mars level David Mars level two that is it's not a neural implementation but I have made it so that you can add simulate neural noise to it so if you want to see this program after you've seen it demonstrated download it from this web address and you can quite simply unzip it and start start it running so uh I will now switch to demonstrating this program so I end the show and there is the program that's what it looks like and what you see is three different windows left Center and right the left hand window is going to be a three-dimensional view of some space in which a b or a bat is moving the center view is always just health information and it tries to tell you how to use all these sliders and buttons and controls and the the right hand view is just various graphs and we'll see some of those as we go along so what happens when you press the start button is you see some three-dimensional space and inside it um there on the left hand side is an animal this time a b and the colored circles are objects randomly spaced in that space and so the the lines going from the B to the objects are lines of sight and this is a three-dimensional view so you can rotate it see the three dimensions and that's what happens when you rotate it the objects rotate and so there at the moment we've got nothing about the be's internal model of space we've only got actual space itself shown in the view in order to show the B's internal model of space we press the Run button and I'll do this and you can see what happens so as you press run the B starts moving that's the green line it gets new lines of sight and from the new lines of sight it estimates the positions of all those objects so I'll restart and do that again and what you'll see this time is that the estimates of the positions appear as small circles with arrow bars so the arrow bars are the gray lines the small circles are where the B thinks the object is so you can see the building B is building rather an accurate model of where the objects are from its sense data I'll restart again and this time we'll step through it one step at a time to see how the be's model of space evolves with time so one step and you can see in the very early steps we'll rotate it a bit to see show what's going on the B makes starts making really quite good estimates of where the objects are each little white circle is quite close to the blue circle but the estimates have error bars and the error bars are in three dimensions showing the uncertainty of the location estimate in all those Dimensions so it's those estimates of position that enable the be to to move where it wants to do suppose these are flowers it can go to the flowers and get pollen or whatever it wants to do now I said the program computes three different kinds of model and the three MO is always doing this as you go through the steps on the track and these three models are a full basian model and that's what we're showing at the moment full basian we can switch to a tracking model that's the object Dynamic tracking and there if I switch that you can see the arrow bars and the objects hardly move at all they do move a little bit but this is one result of the program that doing that tracking model which is simpler to do than the full basian model gives you nearly the same uh estimates and same error bars and the third model it computes is a tracking model with noise and I switch to this one and again it's not moving very much actually uh but I'll I'll move it on a bit and you'll see that noisy tracking often is very different from um tracking without noise so we're on tracking without any noise at the moment we step forward a bit and the B keeps updating its estimates of these positions as they go and I now switch from tracking to noisy tracking and you can see that noisy tracking is significantly worse than tracking they noisy estimates have drifted away from the true positions of the object that is really the second I've by the way I'm running this with a fairly small level of neural noise at the moment you can adjust the level of neural noise you're can adjust the bee's visual accurity adjust all sorts of things using these sliders and try running the program again this is running with a fairly small level of noise so I keep stepping and the noisy tracking estimate keeps getting worse but I go back to the tracking estimate the tracking estimate without neural noise is pretty much dead on so I'll restart again and I'll run again because then we can show the graph at the end of the run which shows what's happened to these errors so if I run again now you look at the ground on the right and what that is doing is comparing tracking which is the black curve versus noisy tracking which is the red curve and these are steps along the be's track those the same steps we saw there but these vertical axis is the level of error in the depth that the B estimat and you can see that tracking is rather small errors it homes in on true position of the object whereas with memory noise you get much bigger errors and in fact the errors are very unpredictable if you rerun um we just rerun it again you find that the errors coming from noisy tracking they do seem to vary quite a lot from one run to the next noisy tracking is pretty unpredictable basically so there the errors have gone right up and they've come down again whereas ordinary tracking without noise is nice convergence towards the true positions so that is really the second key result of the simulation that is that a tracking model is a realistic model of how animals estimate positions of things around them but if you add noise to it even adding a small amount of noise completely messes up the tracking model now there are more things you can show with this model you can show for instance how animals use their model to detect what is moving around them because obviously when an animal is moving it is quite hard for it to detect motion from its visual field because when it's moving everything is moving in its visual field so it has to use the three-dimensional model to work out what is actually moving and if you also plot the efficiency of motion detection you find that too is very much poorer when you ad memory noise so I think I'll stop at this point the program also does a simulation of bats but I won't step straight into that perhaps we could come back to that at the end of the talk if somebody wants to hear about it uh but for the moment let's just step back and find what we have concluded from the bees model now where is my presentation um now I've got to resume the presentation some how do I do that yeah you get back to the presentation so the key points that I think I may have shown you is that in the 3D view you can see the track of the animal you can see its lines of site you can see sorry my phone is ringing sh it up I've shown you the three I've shown you where the real objects are those are the circles the colored circles shown you the objects as located in the internal model I've shown you shape for motion I've shown you what the arrow bars are and you can rotate the 3D view I've shown you the three different models of 3D space the full basian model the dynamic object tracking model and the tracking model with memory errors um as I say when you run this program you can change all sorts of parameters to see how it's sensitive to the parameters and um I've shown you the B spatial model I've shown you how the arrow bars are particularly the the depth dimension and I've shown you how memory noise degrades the model so here are the key results that the best possible model any animal could build from its sensation is a very good model and in fact it's more precise than the sense data because if the animal can assume that objects don't move then over time an animal can build up a very good understanding of where objects are better than than its raw sense data animals can't do any better than that but dynamical object tracking works pretty well it's almost as good as the full basing Mo and it only works if spatial memory has very high precision and I didn't say the levels of precision that I put into the program which start to spoil the tracking model they're about one part in a 100 and that I believe is much more precise than the most neural representations of space can give so that's the next part of this talk how do we do that modeling with a neural model of the brain the classical neural model so how do you build a neural spatial memory and this is a changed quote from Animal Farm where they said two legs bad four legs good two Dimensions is easy three dimensions is hard because two Dimensions you can easily do a sheet of neurons representing two Dimensions whereas in three dimensions you don't have that option and there are several possible memory designs you can have a two-dimensional sheet of neurons and you can represent the third dimension by depth depth by the third yeah you can have some other variable representing depth or you can have a three-dimensional clump of neurons where position in the clump represents a 3D position or you can represent all three dimensions of an object position by neural firing rates now none of these work well for object tracking I think we can quite simply eliminate the 3D Clump model but the other two models have a problem with neural error rates and if neural information is encoded as firing rates of neurons typically you have a neuron firing n times in some inter time interval T and then the Precision with which it can represent some real quantity is of the order of one part in square root of n now n / T is typically five or between five and 50 pulses per second on most animal brains but for insects and small mammals the time they've got to H have to update their internal model of space is very small typically less than a tenth of a second so if the time is a tenth of a second you get n less than 10 and that gives you errors of the order of 30% which are much bigger than the 1% errors which I said are needed for tracking uh structure from motion so the conclusion of this is that the neurons when they represent space there's a tradeoff between speed and precision faster you have the less precise it is and this trade-off is just too hard so I believe there is no working neural model of 3D spatial cognition now the three the three points in blue I've said before um they say spatial cognition is very important and animals do it well and we've had this problem for a long time in his book Vision 40 years ago David Mah identified the challenge and he started work on it he defined what he called a 2 and a half D sketch and various other models now I believe that in terms of building new neural models of how spatial cognition Works people have really not be moved Beyond this why I think the main reason is that the memory problem is just too hard that memory gives them two big errors and is too slow and I suspect that over the years there been many people who look at this problem and they decide to move on and do something else instead but the result is that spatial cognition is the central problem neuros science we don't have a model of it and so this is rather like theory of planetary motion without a Sun or a theory of the atom with no nucleus so how does this relate to Active Vision I think Active Vision is one way to explore this problem Active Vision describes how 3D spatial model can be inferred from vision and there are quite a few papers on it they've focused on very aspects of it they focused on 2D scene classification they focused on the tradeoffs between various objectives like choice of visual sards they focused on 3D robotics as far as I know none of them have really focused on how animal brains practically build a 3D model and I believe that existing Active Vision models do not address the issue of neural error rates one reason for this is that the standard activ INF toolkit in matlb I believe it doesn't model neural error rates it assumes I believe an abstract perfect neuron with very precise representation of quantities and error rates are actually not an issue for many of these applications they're not an issue for Robotics and they're not an issue really for making discrete choices but as I've said in this talk so far the accuracy of 3D model really matters and neural error rates are the big problem if we set that problem on S one side for a moment there is the in issue of active inference tradeoffs and there are many many interesting tradeoffs you can examine in Active Vision and the key tradeoff I believe is one between freezing and moving as I've shown in the demonstration animal has to move by has to move in order to infer the 3D positions of things around it by shape from motion it also of course has to move to achieve practical goals like feeding and fleeing and meting and so on on the other hand it can freeze and freezing it may conserve energy it may be able to detect what's moving simply directly from its visual field which is much easier and it may itself avoid detection so these are very key tradeoffs they're absolutely essential for Lifetime Fitness for many animals animals have to make this tradeoff or these tradeoffs any moment of the day and so we've got plenty of empirical data about it and I think it'll be a very useful area to explore explore now I'm going to switch to something completely different having said that neural storage of spatial positions is a very hard problem I'm going to talk about an alternative possible alternative way of storing spatial data and so we if you assume that there's a round some round region inside the brain of a fairly large diameter D and this holds waves with a minimum wave length which I call Lambda and that neurons can couple to the waves both as transmitters and receivers and the wave can persist at least for fractions of a second so the wave can act as a working memory for positions and the number of object positions you can store in the wave can be up to D over Lambda Cube and that's a can be a very large number the spatial Precision which one object position is stored can be one part in D over Lambda and I think that D over Lambda can be very large so you can easily get Precision better than one part in 100 is what you need to build the spatial model so in summary wave storage of 3D position may have a lot of computational benefits it can give a natural fit to the problem it can give High precision and high capability it can give you very fast response times low spatial Distortion and some other benefits which are described in the papers so apart from its computational benefits is there any evidence for wave storage in the brain I believe there are two quite powerful lines of evidence one of which comes from the insect central body the central body of the insect brain is a very small part of the brain in the middle of it and it consists of a fan-shaped body and the elliptical body and it has this shape which is remarkably well conserved across all insect species and there's an insect brain database and I've gone to the insect brain database and pulled from it the shapes of the central body from a few typical insect species and here you can see the fan shaped body and the ellip iCal body and it's very constant across all kinds of insects and you can see it's approximately a round shape so it's well suited to hold the three-dimensional wave and it does multisensory integration and so it's quite likely quite probable that it holds spatial positions and insects have very few neurons in their brain to do it in any other way and what I think is significant is how constant round the insect central body is compared with all the other parts of the insect brain so that's one piece of evidence from the insect central body the other piece of evidence comes from the mamalian thalamus as you may know the eamus of most mammals all mammals is approximately spherical and it's connected to all sense data and all cortical re regions but the important thing is that the shape of the th is highly conserved across all species and there's an important aspect of the thalamus Anatomy that unless you assume a wave really doesn't make sense uh because the thalamus consists of a number of independent nuclei like the pulvinar and the lgn and so on and so forth and the connections across within the thalamus between these nuclei are very weak or even non-existent so you could have this picture here that the th where's my pointer here's my pointer the thalamic nuclei which white circles here they all connect in two ways to the cortex but they don't connect to each other so one thalamic nucleus here could easily start moving out towards the cortex and the the distance the length of its axons could decre incase and its other connections it doesn't need other connections so all the nuclei could migrate outwards towards the cortex and you could still have the same neural synaptic connectivity and the same uh computational capability if neurons only comput by synaptic computation so this way you could save a lot of energy in shorter axon length so in summary it compack Thalamus only makes sense if all the nuclei need to be immersed in the same wave so we now have three pieces of evidence for a wave in the brain firstly there's the computational Neuroscience that it's a very difficult problem to build a 3D model without it but you can build a 3D spatial mod if you assume there's a wave storing positions secondly the insect central body is nearly round in all insects very well suited to hold a wave and it appears to be in the right part of the brain to do that and thirdly the mamalian thalamus which again has this round shape very well suited to hold a wave and the important thing here is that without a wave the anatomy of the thalamus doesn't make sense so I would like you if you remember only one thing about this talk remember this slide there is quite a lot of evidence for a wave in the brain one thing I will say um is that the wave is probably not an electromagnetic wave because there's quite a lot of interest in electromagnetic fields in the wave from researchers like Miller and mcfaden and so on but an electromagnetic field can't play the role that this wave is supposed to is needed to play in other words the key thing that this the wave is supposed to do in this model is to store information for fractions of a second but an electromagnetic field in the wave and there certainly are in the m in the brain there certainly are electromagnetic fields in the brain they cannot store information for fractions of a second and they cannot represent 3D space like a HRA and just to say a little more about this if there's an electromagnetic wave in the brain it has to obey Max's equation so the wavelength times the frequency is equal the speed of light Lambda f equals c and that means that 40 HZ typical frequencies of these waves the wavelength is 8,000 kilometers as large as half the Earth so it's the conclusion is that at 40 htz electromagnetic field is not a wave it's a static field and it's driven entirely by neuron firing so it doesn't store the information so in summary we're looking for something not electrom magnetic and possibly some quanti excitation something a bit exotic um in the field of quantum biology I think we shouldn't despair here because we know evolution is a lot smarter than we are at discovering these things and explosing them so here are some take-home questions does 3D spatial cognition use a wave in the brain in other words in the light of the evidence I've shown you what is the basing probability of that hypothesis being true now I say these take-home questions because I didn't expect you to have an answer immediately but perhaps you'd like to look at the papers and see what the evidence is and try and assess it in your own mind or do you know some slam D killer reason why they can't be a wave in the brain if you do know reason what is that reason and how do brains compute space how do neurons on their own represent 3D space with enough Precision on on the other hand if there might be a wave in the brain wouldn't that be a a rather exciting and revolutionary development it would actually change the whole neuroscience and it could address this Central unsolved problem of how spatial cognition takes place so I believe that possibility should be explored particularly for young researchers is attractive it's Green Field research it's not a well trodden path of classical Neuroscience the classical Neuroscience model of molic pits neurons and heav heian synapses that's 75 years old now so I would like to encourage people to get out and explore or again come back to the earlier slide here a crisis in Neuroscience the result of this work I think is that neurons can't represent 3D space because they're too imprecise and too slow so the crisis is can you show this is wrong can you show it by building a working neural computational model and checking its scales properly so the FP neural process model is the starting point for that I think it's a good problem to work on because it is a crisis and big Cris big advances in science tend to come out of crises so what I'm advocating and this is my last slide is a twin track research program to build two different Active Vision models of 3D spatial cognition one is a pure neural model which is a classic Fe neural process model can this be made to work or are the neural memory errors going to kill it and secondly to try to build a hybrid wav neural model and when you're building those models we can explore the tradeoffs that active inference is so good at at Computing and particularly the trade-off between freezing and moving so there are a couple of candidate projects for of the active inference Institute okay that's it awesome thank you Robert I have some questions and some people have asked questions in the live chat so I'll uh I'll ask them so first just while I'm recro everything how would you connect this to the requirements equation earlier work because you mentioned that there was a requirements equation driven calibration of the optimal navigation so what does that look like to have the optimal navigation according to the requirements equation well uh to summarize on the requirement equation you can model how brains evolve and this is the previous live stream and you can show that they evolve towards making a purely basian calc calculation of their best model of the world from the sense data but that purely basian calculation is rather expensive and it's been well known in Fe that full basian calculations is intractable for most animals and so that is a very expensive calculation and it's probably not the way animals do it but it it is it can be done on digital computers and it can be done in this model I've showed you and the first model the full basian model is actually Computing the requirement equation from the bees or the bats sense data the second model a tracking model is a is an approximation to that which is a lot cheaper but seems to give very nearly the same results okay awesome let us dive into a few mammal and insect neuroanatomy questions so I'll start with a set of questions from the the live chat this is going to be about mammal neuroanatomy okay Tim Ritter asks do you assume this wave property for all thalamic nuclei primary and secondary or for specific ones EG pulvinar or mediodorsal very good question I don't know the answer I mean at this stage I believe the whole phalus is around spherical near spherical volume with the wave going through all of it so they are all immersed in that same wave so even the pulvinar the pulvinar certainly is even the lgn which is rather small and is a pass through nucleus I think they all are so I think for instance I think um people always say the thalamus is a relay uh sense data gets the cortex by Thalamus but people don't have a very good reason why it has to go through these relays nuclear in order to get there I think it's doing something about locating about I think the wave has some involvement there but this is very early days I don't know the answer tool okay another question from Tim on mamalian neuroanatomy what about 2D orientation would you expect similar waves in hippoc cample instead of thalamic regions or is 2D sufficiently easy to get by without yeah basically I believe 2D is easy enough to get by and the hippocampus is by no means suitable to hold a wave they have all sorts hippocampi have all sorts of different shapes so yes I think that was a Core theme between the mammal and insect areas is the conserved shape and then also the allometric differences over Evolution where the size differential of the insect central body changes much less than other primary sensory regions and that was in your paper yeah that's right yeah and that kind of implies that that small size of the central body it's only a few percent of the whole insect brain seems to be enough yeah and that the properties which it hosts or enables might be related to its physical extent or like to its surface area to volume ratio and not a function like for example in the uh antennal lobe where the old factory information are coming in there are these little glami and different species have from several tens to several hundred of these olfactory glami like ants have many and they have more old factory re ctors in their genome and they have more uh old factory glami in that region or insects with more compound eye sections they have larger optic loes so the primary sensory regions have very large variation amongst species but then as you get into the central body uh you get much more conserved anatomy in size and then the mushroom body on the top part of the brain is something a little bit in between that might have more of an analogy to like Maman cortex where there actually is the possibility to scale its cognitive capacities through size changes because it has some kind of like repetitive or stereotyped layout yeah I mean there are a load of fascinating questions in neuroanatomy which relate to this and and if you pursue the this hypothesis then there's all those interesting questions I'm not an expert on insect or main neuron Anatomy but there's a load of interesting questions in there cool so um about the be spatial cognition so we know that bees use a variety of visual cues ranging from the landmark and the landscape recognition to polarization of light and so on and also as you pointed out the central body does multisensory integration so how do we think about the possibly complimentary or redundant information provided by these different aspects of the visual fields and what does your simulation focus in on well I mean I believe that what the central body and the thalamus both do is multisensory integration in other words animals should they they need to make the best 3D model of space they can and they need to use all their sensat to use it apart from possibly smell that's an interesting question uh and so both of them do multicenter integration and ideally one would put in the simulation one would have things like stereopsis one would have object recognition one would have light polarization all sorts of sources this program only does simple Vision or simple echolocation at the moment but it should do all multi-entry integration in in a single basian maximum likelihood model of the whole um all sense data coming in at the moment interesting yeah with sound or with smells it it would be interesting to see how those come into play and and how do you think about in in the simulations presented here egocentric and allocentric navigation because you mentioned how the kind of simplifying assumption is that the world is a rigid fixed body so you can have these kind of Duality where like I'm moving and the world is fixed and then there's sort of like I'm fixed and the world is moving so how does that relate to that question to to that egocentric allocentric distinction yeah very good question I mean I think the frame of ref used for the model should be as much allocentric as it can be because the wave has to persist and if the wave just persists it represents an object at a constant position so you want to have a frame of reference where most objects are at constant positions so I think that makes allocentric but obviously has to change from time to time every few seconds it has to switch because it can't just stay allocentric H interesting so now to connect that to what you brought up about move or stay that kind of fundamental uh animal or fundamental mobile organismal nervous system question I thought about different body plans where the eyes or the visual component are unable to move separately from the body like a bee can turn its body but it it can't rotate its eyes whereas in humans for example we have optic Cade so they're that stay or move yes we have turning our posture and moving through space but also we see like this microcosm where when the Gaze is fixed there's High precision and then movement in the world is associated with movement of objects and then whereas when a iade is dispatched during the Cade our visual attention is alleviated and then it's because during that time all the movement of pixels essentially is ascribed to the movement of the eye so we see kind of like a microcosm of the two modes of movement and stability in motion detection in the cating but for other organ organisms that don't have isoc the only way that they can get that kind of alternating movement and stability is by moving their body yeah yeah I mean I always think of isad as particularly Predators if you like that want some high resolution in some direct some particular direction whereas for most insects as you say that there is not the option of a high resolution phobia um but I think of Cades as being cheap there's I mean the freeze move tradeoff is a real tradeoff that if an animal moves it can be detected as moving and it can't detect motion itself as well as if it's stationary so that's a real hard trade-off an animal has to make whereas Cades you can make them cheaply when whenever you like yes yes and also it's really interesting like how often the Behavioral Studies just look at the direction of movement but there's this whole timing of movement and so there's definitely a lot of empirical studies about whether fear based movement like in a predator prey uh or different kinds of movement choices um where would you say attention comes into play in the sense that the be or the bat was just kind of taking it all in it didn't have like some restricted scope so it's kind of like a uniform attention across objects and across space and time but then we know that we do have this visual attention phenomena well yeah attention is very important and uh I think naively it's a search line model of attention in other words the wave representation of all space represents all the space around an animal but the animal can focus attention on a region of the space and what that's doing is tuning The receptors in the thalamus so they are sensitive to wave vectors in a certain region so there's a whole load of issues there about how the wave works as whether it can how signals are rooted from sensor inputs to specialist regions of the cortex and I think atten is that rooting of information so again loads of big questions there yes with the wave I was kind of thinking about the insect brain visual input flowing in and also other potentially inputs and all of these are crashing on the shores of the central body and then there's this kind of stabilized dynamical wave representation such that information coming in differently changes the the the sh the resting shape of the wave and then that opens up like you're now suggesting recurrent connections or or other connections into that wave hosting region recurrent connections can modify the shape of the wave attentionally and then also the oh yeah the resting shape of the wave can route or augment or suppress other sensory information coming in that'd be like water kind of dumping to where there's already a high water Point versus water going to where there's low water yeah yeah Ian I think key role of the wave is to persist a background model of all 3D space and then against that background model you Center information that comes in particularly movement uh is best evaluated a piece of new sense data you evaluate it much better if you can compare and contrast it with what you had before and that is attenion and it it's it's the thalamus if you like telling the cortex here pay attention to here here's your old information from this place in space Here's your new information so tell me what's changed well this connection with frame differencing is very powerful predictive processing predictive coding algorithms were built by computer scientists and and compression Engineers looking to make video compression work and doing the frame differencing because that's the optimal way to compress video and then that got brought also back into Neuroscience where there's a lot of focus on things like Edge detection and these other 2D visual phenomena and then as you're pointing to there's this kind kind of Sun at the solar systems center that's not really being discussed which is like okay yes we have neurons in different uh visual regions that are excitable by vertical lines by diagonal lines and so on but this is all flat phenomena and the question of not just shape recognition but the question that's most approximately relevant for movement and the fitness related decisions for the organism in the niche has to do with its spatial navigation not its like eyesight at the eye doctor yeah not only spatial navigation how it moves its limbs you know how where it puts itot its foot next that sort of thing and the 3D model I think does all of that okay I'll read a question from the live chat how might the Cade relate to a matrix of inputs versus a human-based visual system movement on The Matrix may give different spatial Dynamics uh I'm not sure what we mean by a matrix of inputs there but um as you said during a cc visual input is kind of blocked while the I is getting from A to B uh whereas the wave persists and the 3D spatial model stays constant and after a cade the eye has to update the 3D spatial model in some different place so um I'm not I don't think I've answered the question but perhaps you what do you understand by matrix it's what you enter um it's making me think about the experiments where the for example the fruit fly is placed on a a pingpong ball in a harness in a virtual reality setting so it's getting custom visual input and its movements on the pingpong ball just kind of Scrolls the ball so it's basically fixed but it gives a lot of degrees of experimental Freedom around the the um orienting of the body and what visual inputs it gets so I wonder if anywhere there we know about the the time the time scale of spatial orientation updating because that would be very critical to understand the nature of the wave whe if it was something that was um for example closer to the diffusion rate of ions then we might be looking more towards like a Channel or pore based hypothesis if it was something that was faster than neural signaling it would suggest something more like the direct coupling or other kinds of of action so what what makes you feel as you suggested that it is not an electromagnetic stabilized wave Fields well as I say the physic I mean for electromagnetic wave we do understand the physics and the electromagnetic waves that measured by EEG for instance they are a purely passive consequence of neural activity they don't persist in the information for any time at all whereas this wave I'm talking about has to persist information for fractions of a second so this constant spatial model is kept persisted while the Cades go on while the animal moves well it computes shape from motion so a pure electric field we we know the physics is maxos equations and it does not store energy store information so it's purely a passive reflection of what's going on the neurons it's not a memory h so the neurons are especially if we think about the several like thousand to tens of thousand let's say in the insect central body there's too few and they're too sparse and noisy to in a purely connectionist neural uh framework to support the kinds of empirical results that we see on the other hand a purely field-based approach has some issues that you just laid out so it's it's very interesting that to at least of the well-known mechanisms the local field potential and the firing rate rate coding type models that both of them seem to have some limitations and yet there's very strong anatomical evidence for the functional role of that region oh yeah it's absolutely vital region but I believe that just looking at electric fields and magnetic fields in the brain is not going to give you memory and that's the key thing that I think is needed to do structure from motion you've got to have short-term working memory to hold a little model of space for a fraction of a second do you see that as a kind of special type of short-term memory or do you think this is the same memory pool that like short-term audio memory gets entered into oh it's it's special it's different from that yeah definitely say it starts from how do you how do you compete a 3D space model how do you do structure promotion you need short-term working memory for for that purpose specifically interesting um the kind that enables us to check for difference in v in visually changing systems or what what does this VIs visual working memory have well basically it's it's not Vision because it's 3D and uh checking for difference in the visual field is you can do it quicker you can look directly at the visual field whereas this model I think the 3D model it's um it's maintained by a loop between in in mammals between the thalamus and and the cortex and it's a 40 htz cycle that maintains it so it takes time to build the 3D model and it's a bit slower than direct visual change detection interesting it's this tension with like visual being what is seen versus kind of a broader imagine with the imagination of vision um what about action in your model so how were the paths set yeah I said that the model was a bit like active inference in the animal has to move in order to understand space but it's not really the program has not modeled the kind of active inference choices of how should I move to get the best understanding of space and you could do that you could make the be choose this tragectory to get the best on understanding of where the flowers are or you you make the B choose it tragectory for all sort those are the active inference tradeoffs that the program has not yet looked at and and which can be looked at and I think are very interesting that's awesome yeah it's almost like the be in this situation it's like on a train trying to reduce its uncertainty about the location of landmarks but it's just on a it's on a rail it doesn't have policy decision whereas once we start to close that Loop and ask which direction of movement or none given the costs would reduce uncertainty about resolving this kind of spatial relationship then that that's where the perception action start to like come into play in benefit of each other and potentially there's several Choice successive moves can greatly reduce uncertainty through otive sampling just as we see ining and all other situations and that would be like a heris or strategy that that really does work yeah I mean and another example that I use somewhere is I believe that predatory birds like Hawks when they're approaching their target they don't go in a straight line they move on a curve to reduce the uncertainty so that they continue to get uh more information continue to see the range of of of the target if they just went straight for the Target they wouldn't get a range fix on it h now what about the difference between the B and the bat whereas a b is simply receiving the reflected photons let's just say the b is sending out a invisible signal so how is this kind of radar echolocation setting similar and different to the vision setting oh it's it's very different I could show you that with the program if you like um what happens with the bat is from the delay of an for a particular insect that the bat is tracking from the delay of the echo it sees the range to the insect so it gets the insect constrained on a sphere in its 3D model then from the Doppler shift it actually sees the perceives the cosine of the angle between its directional movement and the direction of the insect so what it gets there is it constrains the sphere surface of a sphere down to one Circle in space so what the Bat gets is a series of circles in space which constrain the position of the insects and I could show that on the program if you like sure okay um well how do I do that how do I share again yeah I'm Cur is the circle something like is it like a hoop that you could throw a basketball through or is it well I I'll show you um now I can't get rid of this something on my screen it's the problem um I've got a big black screen with a two on it and I can't get rid of it end show SP end show right now am I sharing my screen or not not yet um now how do I find where do that how do I find where I am in space yeah that's an interesting question also how these mechanisms Google window somewhere the zoom window somewhere and oh ZM that'll be it how are these mechanisms repurposed for digital navigation semantic navigation narrative navigation uh just a minute let me find oh God get away oh share screen tube share okay right you got that we're back okay so what we do is we change from B to bat and we start again so what you have now is the bat and several insects which are colored circles here and what the bat has is its echol location take the blue insect it's EOL location constrains it to a sphere in space and that's the delay of the echo location and the Doppler shift constrains the sphere down to a circle so if we rotate these circles you see that blue insect is on a circle from one point in the bat's trajectory so as the bat moves it gets successive circles for the same insect so what we have is the bat is moving and all these circles highly confusing and it gradually locates all the insect better and better but what if I restart and step again if I step I've restarted right now if I step what I can do is focus on one insect if I focus on that insect then I only see the circles from echolocation of that one insect and I can step again and get a new circle from the new step and all the time the bat is optimizing the position of that insect to get the best fit to all the circles it has for that insect so it's very different from Vision but it can build a very good 3D model from its echolocation okay so to kind of confirm what's happening here there's there's for a given snapshot ping with the sonar what is returned is a circle of equip probable locations that are kind of like the maximum likelihood Ridge that constrains these really s of Gan gussian Donuts if you like okay likelihood and then success of pings enable you to look at the intersection point of the circles to find out through successive approximation the likeliest location so this is what's Happening Here on that light blue insect he's got these three circles and that's the most likely they're not very well intersecting but that's the most likely intersection Point that's if I go on this back it's it's like a gaussian mixture model you have those three Peaks and then when you summate those three Peaks that the the point that interpolates them or just is the the geometric average is the single maximum likelihood estimate Point that's exactly it yeah and again with the bat I can go from Full basian model which is this one full basian tracking or tracking with noise and again the noise tends to have nasty effect where I look back at this one here that's that dark blue insect and I can rotate to see how the circles go so how is this how is this similar or different than for example radar navigation algorithms for planes or ships I think it's quite similar I think yeah I mean they are making maximum likelihood inferences from radar signal and have you with this software looked at the computational like the runtime complexity or the resource use associated with scaling the number of insects or scaling the resolution of vision yeah you can scale the number of insects for in if I scale up here the 30 insects I can run the model with 30 insect and um it's it runs perfectly well so the model is quite efficient because what it has to do for each object um and for each time step it simply has to do this Gan optimization which turns out to be just a 3D Matrix operation three three matrices and that's very quick to do and the so that's one of the advantages of it that if animals are trying to track a load of things around them which they probably are it's quite a quick efficient computation but this is not the neural implementation the problem is going from here this Mars level two computational model to a neural implementation and that is where I think all the interesting problems Li at least you have this level two model as a kind of starting point and the Basin model would be trying to implement that in neurons yes makes sense this is kind of the as if basian algorithmic map and the question is what proximate mechanisms are capable of doing these algorithms functionally and neuroanatomy has basically localized to the region in mammals and in in vertebrates and so now we're in a space of winnowing possibilities and leaving the door open for unconventional opportunities for how those regions actually do it it's a very targeted specific agenda that that connects a first principal grounding about how well the navigation can proceed with the requirements equation On Through To to the empirical patterns that we see those empirical patterns might have some like behavioral experiments well sometimes can be hard to interpret as well though for example there's a common experiment where a wall that's shaped like an L is set up and then people will use it to test if an animal will go on the hypotenuse to reduce the travel distance or whether it will take the the around the wall however even if there was a conceptualization of the ability to move in the hypotenuse Direction an animal might like prefer to move along walls so then by the time you get to the real animals movements it's very tied up with not just trying to be the optimal Landmark resolving visual algorithm it's actually engaging all these other drives that can make it look like it's lower efficiency or even like supernaturally efficient on a given domain yeah I I think there's all sorts of animal experiments one could do but interpreting them is not easy but basically if you're looking in these experiments to to to measure how good the animal's internal 3D model is I would like to with insects for instance how good is that movement detection if it's movement in depth that needs the 3D model to resolve it rather than just the V visual field that's very interesting especially in insects potentially where there's little overlap between their two compound eyes yeah yes um okay I'll read a question from the chat as we kind of head towards the end use equal wrote how would perception of ability to conceive of other domains of time as a cognitive function change spatial awareness um other dimensions of time I'm not sure I understand that question other domains of time domains what's other domains of time um well this is all a very shortterm question it's fractions of a second and time outside that interval just doesn't come into it at all really does that answer the question you it might I think it might also be pointing to our awareness of how we handle our perception of time and space and what does that open up or enable for example for our perception of space if we have a different perception or conception of time well that is a very deep question I mean I think our perception of chronological time long-term time is something completely different from more anything else in the animal kingdom I think most animals live in the moment really they're aware of what's happening right now and what they got to do right now and the rest just doesn't matter whereas we exit what matters in the moments to ruminate and speculate yeah absolutely um well what other directions or or ways are you hoping to take this work well one way that is particularly important I think although particularly problematic is theories of Consciousness in other other words Consciousness most of our Consciousness at any moment is consciousness of the space around us and it seems to come from our internal basian model of space around us so this work is very related to why we are conscious and there's another paper and I'd like to give another live stream just on that subject because I think this gives a way forward to a theory of Consciousness that can be in many ways more satisfactory than we get from purely classical neurom models of the brain that's awesome I'm also personally very excited on the empirical insect neuroanatomy side to look oh there are a huge number of ways of look going forward and insects are particularly productive because you know they've got to do it with a really small number of neurons so it's really there's a lot of experimental work on insects that can illuminate this question yeah absolutely cool yes the brains are fun to dissect you can see it transparently all there and no backstage there um and then also this on on the more transferable outside of anology I think the octave inference Loop closure with policy selection of movements including stay go noo decisions and which way to go and then understanding how like well here's the trajectory that would have been the most Information Gain on resolving this location um here's the trajectory that maximized safety it's not moving or something and then here's the trajectory that that would have um done some other thing then being able to look at realized empirical trajectories and then break those down or look at their component loading according to safety visual Information Gain other kinds of heris or or impulses to understand moments or kind of fragments of trajectories like something looks to briefly resolve its uncertainty and then it doesn't need then the the overall demand to resolve uncertainty drops again and then it continues just with inertia and then maybe that's a very simple decision to make and then there's probably all kinds of cool patterns and ways to go yeah there's a huge amount to investigate another another topic by the way is we've looked at mammals we've looked at insects right at the opposite ends of the spectrum there's a whole load of stuff in between I think octopus and squid are particularly interesting but there's all the other species one can look at say how do they do space they have unique bodies and different bodies but there's also bringing in there the question of underwater or space or fluid media and then that and a bat flies but it has the mamalian neuroanatomy so then there could be like bird neuroanatomy with its slight differences from the mammal then there could be this question of underwater and maybe there's some mammals that have underwater navigation maybe Dolphins would be like bats but in another fluid media I hope people download the dotzip and play around so do I any last comments uh not really no I think I said it all thank you thank you Robert well I I greatly enjoying learning about the work and seeing about how the requirements equation Scopes a given problem setting and kind of puts a meter stick on whatever the inferential problem is inference plus action problem is and then from there the mar the Marian research program is just kind of laid out you can pursue it from the mechanistic or from the algorithmic elements but they all are connected through on one hand in theory the requirements equation and on the other hand the empirical results that we actually see yeah cool so thank you again um I I come back to the Fineman quote Richard Fineman on his Blackboard it said if you can't build it you don't understand it this is all about building it we can't build a bug not yet um thank you Robert see you for DOT three see you bye for 
