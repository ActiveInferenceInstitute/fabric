all right hello and welcome it's August 5th 2024 this is active inference guest stream 85.1 deep learning active inference with love with David blumen so thank you David for the presentation and we'll look forward to the [Music] discussion um yeah how we can uh use active inference to make intelligent agent and uh some of the advantages and disadvantages and some issues with active inference and how I think we can overcome them um and you'll see why would love uh in a little bit but uh my background is I'm a software engineer and an AI researcher uh I've uh worked for a really long time at the section of uh scalable systems and machine learning and I've been working on this project for about three years now um and I'm really excited to uh talk about it so uh I think uh the Holy Grail of uh AI research uh right now is how do we make an AI agent that's both generally intelligent and doesn't destroy Humanity uh and uh pretty much I think most people are working on some facet of these two U and uh I'm really excited about the generally intelligent part and also not destroying Humanity so um here's how I think we can do it but first I want to talk about just like get our definitions so what's an agent and there's lots of people arguing about definitions uh I'm going to use a really simple one which is an agent is something that uh G given a set of past observations produces the next action so basically given what you're seeing now and what you've seen and experienced in the past what should you do next so a very uh simple formulation of an agent as a function and by generally intelligent uh I don't have uh a rigorous definition of general intelligence I don't actually think there is one but uh as an observer observing this agent we should be able to say hey it's it's acting smart in a wide variety of environments that it hasn't seen before um and what that means is that it's able to figure out the environment Dynamics it's able to make mistakes and learn from them it uh doesn't get stuck at the local Optima it like knows when to explore but then when it figures out regularities about the environment it knows how to use them uh it does this efficiently it's not just like trying the same uh dumb thing over and over again uh it makes plans it can uh communicate and interact with other uh complex intelligent entities in the environment so it's a bunch of different behaviors and uh uh I don't think there's a really clear way to quantify or like talk about that exactly what they look like because different spaces and different environments uh are going to have a different idea of what an intelligent agent is doing there uh so uh for now just like think of it as this grab bag of capabilities so where does active inference come in well active inference uh is a way of thinking about an agent interacting with its environment as uh essentially a u it's a formulation where any agent can be seen as if it was uh acting in a self- evidencing way and what that means is that uh you can treat any object uh that maintains homeostasis over time as if the inside of that object was minimizing free energy uh meaning that it had some some you can think of it as having some generative model of its environment or rather from an agents perspective its boundary and it is uh trying to update its prior beliefs or is posterior beliefs over the boundary in order to minimize surprise and so the way it does that is when it uh receives evidence from the environment uh it uh adjusts its set of beliefs to Concord with what it thinks it's GNA what the environment is and what it's doing and where it is in that environment but it's also actively navigating the environment in order to both U Get gather more knowledge and improve its uh set of beliefs as well as to move into parts of the environment that it thinks it's going to be occupying uh and so this is a really beautiful way to capture what any agent is doing at any scale so this formulation uh allows us to think about uh a biological cell or a robot or a person or an organization a corporation a country uh anything that we want to draw a boundary around we can basically say hey this thing is kind of acting as if it was doing basian inference over its input data and had PR beliefs over uh a set of states that it's going to be occupying and it's trying to uh act in accordance with that so the problem with that formulation so the nice thing about that formulation is it's very general it's very beautiful uh it uh is able to generalize uh what we see at different scales one thing that it doesn't let us do is actually tell us how do we build such an agent and there are two reasons that I think uh that this theory is not uh is not useful for actually building an agent um one is that uh it uh treats every agent as trying to approximate basian inference uh so as the agent observes evidence it's trying to uh infer what that means for its generative model and trying to update its posterior the problem with basian inference is that it is computationally intractable and uh uh the uh active inference recognizes this and it basically says the agent is trying to approximately do basian inference by sticking a variational inference bound on it but in reality what approximately means uh is different in every situation so U uh all agents that we see all systems that we see they're doing some kind of computation and that computation can be viewed as approximate basic inference but in reality it's a set of puristic and hacks that have been discovered by evolution by natural selection by software Engineers whatever it is that uh can be thought of as approximately doing this um but really they're running some algorithm it's just some set of theistic and active inference doesn't really tell us how to discover that it just says says Hey anything it's doing can be viewed as approximately doing this intractable problem the other problem uh uh around trying to productionize active inference is that it says well the agent is going to be occupying uh a set of states that its prior beliefs said it's going to be occupying so you know why does uh an animal eat well because Evolution has uh filtered uh animals down to only the animals that believe that they're going to be fed and so then the animal is going to Via self- evidencing Way go seek out food and again this is a really beautiful way to capture uh uh both uh the action and inference in one uh formulation but it doesn't tell you what are the priors Over States for a given environment so if you want to build a a robot or an agent that has to interact with some environment what what are its prior beliefs what states should it be occupying does it believe that it's going to get energy or does it believe it's going to go repair uh you know the car that you wanted to repair how does it balance between those two um uh we don't know and active inference doesn't really tell us it just says some process uh will give you a set of priors and uh so with active inference typically the approach is well as a software engineer uh you build a uh generative model you basically code up what you think the environment requires and the agent in the environment requires and you give it some priors like you will be fed and you will repair this car and here's uh some parameters about how the environment works that are uh simple enough that you can then do a variational inference over it and get something done and then you U have a more generic free energy minimiz minimizing algorithm that then given that generative model actually controls the robot the approach I want I'm taking is well instead of uh someone trying to design this General model can we learn it and uh I think that's really the only way forward because any environment that we care about is going to be so complex that I just don't believe humans are going to be able to uh specify generative model for it that's at all interesting so uh why do I think we can learn it well uh one Evolution has and uh what does learning it mean uh learning means uh that we want to use some Learning System and uh neural Nets are a really good one and we want to figure out what are the sets we basically want to have some approximate active inference agents some approximate generative model that uh is able to do all the things we want which is that grab bag of generally intelligent behaviors and the reason we can do that is that neural networks are Universal function approximators so rather than building an algorithm we took a neural network we FedEd uh you know pairs of text and images and out comes dolly that given text is just able to generate uh images uh also generator all the slides for this presentation um so uh the way to think about neural networks is rather than programming an algorithm you figure out the training data and a loss function and then you take a Big Blob of learning uh uh learnable parameters and you keep nudging that blob until it uh does what you want it to do and this technique is really powerful you know uh we're uh language models that uh everyone is using now uh uh were also trained this way you uh took a data set that was a set of prompts and a set and then a completion basically the next word uh and uh uh you trained a neural network to predict the next word and you got chat GPT so uh this technique uh is extremely powerful it is very slow it requires a lot of data and a lot of training examples but over time uh you're able to learn a really complicated uh function approximator so uh the the question then is okay well if we want to train a function approximator for a generally intelligent agent what's the training data that we need to use and uh uh basically I think if we discover the training data and we uh throw large neural network at it with a lot of compute we should be able to in the same way that we got chpt or Dolly get a model that's like a foundational General intelligent agent model so to do that we need an environment and uh what an environment so the reason we need an environment is that we're trying to train an agent not uh say a language model so for language model you give it a prefix uh it gives you uh the completion and you can train it on say all of the internet text what we're trying to do is we're trying to learn a function that given a set of fast observations know what action to do next and uh that's a more complicated problem because sometimes the action it takes should be uh done because it's trying to accomplish something sometimes it's because it's trying to learn something um sometimes it's trying to explore uh and so what we need is a world for the agent to interact with that it can and then learn its behaviors from and so the properties we need for that environment is one we want it to provide continuous learning opportunities so there should always be something that the agent some new thing for the model to learn and uh then when it learns it when it does something intelligent uh one of these behaviors that we uh associate with intelligence we want we need a feedback signal so we need the environment to essentially reward it and it needs to be not uh an environment that cannot be mastered because um if it can ever be mastered then there's nothing new to learn and the model stops learning so the way to set that up is uh or one way to set that up is uh via multi-agent so if you have an environment uh even if it's a relatively simple one like say a go game where the rules of the game are really simple uh just by having other agents in that environment uh give it a really important property as the agent gets smarter so do the competitors and so uh the environment essentially adapts to your capabilities uh so this is a way where uh you can automatically get an environment that always gets uh something uh new for you to do because as soon as you learn some exploit or some way of uh being better than your competition the competition changes and now you have to learn something new and so I think uh so my Approach is leveraging uh this uh characteristic and essentially training AIS in a multi-agent competitive environment there is a problem though which is if the environment is entirely competitive uh you very quickly end up at this U kill or beill dynamic so you can imagine uh an agent learns some Behavior policy and then the other agent learns some counter strategy and then they kind of explore the area around those strategies and figure out the best way to be competitive and now they're stuck because if any agent ever tries to do anything that is not that it gets exploited by its adversary so um this setup makes it really hard to uh avoid local Maxima you kind of find something that works and you're stuck there and that's not quite true if you have many agents you might get some unstable symmetry breaking and uh uh things get smoothed out a little bit but um I think the solution to that is actually cooperation and what's cool about cooperation is U there are many ways to cooperate the space of effective behaviors when it comes to cooperation uh just explodes and so you end up with a u much uh denser more higher dimensional Behavior space where it's harder to ever get stuck at some uh Nash equilibrium of uh behaviors and this also gives rise to complex group dynamics uh like Coalition building cooperation uh cooperating to compete figuring out who you're going to cooperate with and who you're going to compete with uh building trust uh it's just like the set of behaviors that are available in Cooperative environments explodes and the problem with cooperation is it's really hard to learn if all the agents are selfish it's not possible you can have things like reciprocal altruism or even just non-aggression you can learn that you know always picking a fight is not great because if even if you win the fight now you're weaker and someone else can take advantage of you but uh in general uh uh learning cooperation when everyone is non-cooperative is also full of these um ntion equilibria of local Optima and nature solves this using kinship so when an animal is born into a world it is not surrounded by uh other selfish organisms it's surrounded by a lot of kin uh so uh for cellular organisms it's surrounded by more or less identical genetic clones of it uh for Animals uh it's surrounded by organisms that have half of its genes and uh this uh kinship uh via genetic Evolution gives rise to organisms that are not selfish because uh from a gen's perspective uh having a copy of you in one organism or in another organism doesn't matter and so what that does is it gives rise to organisms that basically care about the success of other organisms almost as much as they care about the success of their own organism and there's a whole Spectrum in nature from hive insects like ants where from a soldier's ants perspective the only way for those genes to make it into the next generation is for the queen to reproduce and so you end up with ants that care a lot more about the hive and the queen than they care about their own safety and well-being and uh you get a whole Spectrum from fully altruistic Hive uh insects to you know a few perfectly selfish sociopaths but uh most animals fall somewhere in the middle and being in the Middle where you're partially selfish and partially uh uh altruistic gives rise to all of these social behaviors that we associate with uh being an intelligent uh being so uh animals coordinate they uh negotiate they build trust they love each other they show kindness they show jealousy they have social emotions like shame they uh learn how to exchange exchange uh knowledge with each other via gossip and reputation building they do division of labor and all of these things happen again at every layer of complexity animals do it uh biological cells do it uh organizations do it uh these set sets of behaviors are present in uh pretty much all entities that we think are intelligent and so my proposal back to this uh AGI recipe is that uh if we can have the right training data and enough compute uh we can train a large model on an environment that uh essentially balances U cooperation and competition and so I've been trying to build and design uh these environment this environment and the training infrastructure for it and that's been uh essentially uh my research Direction so this is uh the current version of the environment uh and I'll just uh talk through some of the components so you can see there's uh a few agents here and they're uh they're parameterized by our neural network so all of this was trained from scratch using reinforcement learning uh there is this heart alter in the middle and uh the only source of reward that these agents have is they um can walk up to the heart alter and they can use it which requires energy and if they do that they get a reward and what that means is that these agents have to get more energy out of their environment than they're using for interacting and competing with each other uh and if they can extract more energy than they need and they can essentially waste it by putting it into this heart alter then they get a reward so this is the competitive Dynamic that's set up so everything uh doing everything in this environment requires energy to get energy you collect these resources so you walk up to a green or yellow Crystal and you collect a resource um you then have to walk up to these converter stations that look like little batteries and then you put the right set of resources in there and you get energy out you can also attack each other and if you get uh shot you turn into this egg where you're stationary for a while you lose some time and people can steal whatever resources you're carrying but you can also turn on a shield which also costs energy uh but then it deflects the attacks so these a have to from scratch learn how do you navigate the space where are the resources uh how do you manage your energy budget who do you fight with who do you uh not fight with when do you need to keep your shield on so a lot of like relatively intelligent behaviors uh that are learned from scratch and all of this is done via this competitive uh Dynamic and then on top of that is a uh whole system for kinship where uh every agent shares its reward with some other agents based on some uh reward sharing uh system and so what that means is um whenever say this red agent gets a a reward uh the green agent might get 10% of that reward and so this environment allows you to train uh agents in very different environment so um I'm going to pause this and switch over to uh this other tab so um yeah here so here you know you can train in larger maps you can uh uh set up different uh game rules and different game scenarios um and you can also uh set up different kinship environments where you can train fully selfish agents you can train fully uh Cooperative agents you can uh train complex nonsymmetric uh social dynamics and once you train an agent you can actually start treating it like an intelligence and you can do Behavioral Studies on it so here we take a trained agent that was trained in this environment and we just like put it in an empty room and we're like well what what is this agent doing and you can see it's like it's exploring its environment it's not getting close to any walls because it knows that there's not a lot to be seen uh and it doesn't want to waste its movement um you can put it in an environment of uh where uh you know you give it some energy and you give it U and you put it in a world where there's nothing except this heart alter and you'll see that it like searches the world until it finds the heart alter and then it puts energy in there um and so uh you can start doing uh you can essentially study it and do experiments on it it's like what will it do under this circumstances um and you can start approaching some of this as an observer observing an an intelligent entity um this is all early days so this is just a few uh evals and experiments that I've been setting up but this environment allows you to do all of these things um and I think what's cool about this uh whole approach um and I'll switch back to this slide is that uh the idea is that uh you you end up with an environment that is unlearnable because anytime some agent gets some competitive Advantage it uh the environment around it changes since all the other agents now Arbitrage away that Advantage it's also uh highly uh highly dense in Behavior space because there's so many different ways to uh cooperate and compete with each other that uh there's always some new trick that you can discover and so as this neural network learns to achieve Fitness to this environment uh and the environment is always changing it's also learning how to learn it's learning this set of adaptive behaviors of exploring uh forming theories testing things out uh because that's the behavior that gives it an advantage when the environment is always changing and so uh the hope is to end up with a uh with an agent that you can drop into a new environment that it hasn't seen before and the thing it'll do is first figure out the Dynamics of the environment to start exploring it figure out how to uh how different things work and then what it needs to do and then start doing it um the other cool thing about this setup is because these agents are trained in a Cooperative setting they have to cooperate with they have to learn how to cooperate with other agents and that means they have to learn how to communicate so there's a uh signaling and a language component uh so if you are surrounded by kin and you have to compete with another group you have to learn what information is necessary to communicate to your allies in order to effectively organized so these agents are developing a language that we can then um be able to reverse engineer because we have a fully instrumented world uh and we can do experiments on them so we can basically figure out what is it is that they're saying and we can then talk to them and they're uh learning a policy that's conditioned on this kinship score uh so they know how to cooperate with close skin which means they are capable of Cooper they're learning to be capable of cooperation they're essentially learning to care for other agents and so the hope of This research uh program is to build a model organism of um an active inference agent essentially an agent that is performing active inference in a complex environment that you can talk to and that cares uh about other agents in that environment um and if that works we should be able to uh take these same ideas and um take it out of this simple 2D grid World environment and essentially do the same thing but in complex 3D World physics environments uh and get something that is smart that cares about other entities and that we can talk to um so that's basically what I'm trying to do I'm calling this project meta learning um meta is the poly word for love and kindness uh because I think that um Love is uh love and uh what I think I'm building here are agents that are capable of love in the same way that genetic Evolution um the selfishness of a gene being expressed in uh a multitude of other organisms gave rise to organisms that love and care for each other especially their K I think that similar Dynamic uh is what's necessary to uh create um a an environment that's complex enough to learn uh generalized intelligent behavior and also is going to create agents that U are alignable that basically care for other agents that we can get along with so um I I'll stop uh talking here and uh uh take questions uh and uh yeah happy to dive into anything uh that I talked about or if you have questions about active inference or how or free energy or deep learning uh basically anything that this made you think about I'm so happy to take questions awesome all right anyone in live chat please feel free to ask a question just meanwhile maybe just H how did you come to be applying it the neural network approaches in this project just what what L you want to explore this project and then how do you see active inference playing a role yeah uh I mean uh it's funny because um I uh I've moved in and out of uh machine learning my entire career uh uh and uh the experience I've had was I would go into AI I would spend a bunch of time working on it and then I'd be like okay the field is not ready like we don't know what we're doing and I took a long pause uh and worked at a startup for 10 years and um after leaving that startup I came across uh Carl friston's free energy work and um had been out of AI for uh almost a decade um missed the last two revolutions uh and reading it uh it just kind of clicked for me I was like okay like I can see how because this describes intelligence at every scale and just very naturally falls out of what it means for uh objects to maintain homeostasis in an entropic environment and I can see what the math uh looks like the math looks like uh essentially uh the math from reinforcement learning it's uh trying to um do basing an inference on your uh evidence to compute a posterior uh and navigate an environment in order to balance exploration exploitation um and that's kind of what the reinforcement learning community came up with bottoms up uh just like trying to be like how hey how do we train uh an entity that learns in its environment and when I when I saw just how close those two approaches were to each other it really made me feel like okay there's no secret sauce in it's like there's not something that we're missing that's key to intelligence that we just like haven't been able to capture really it is like uh this uh kind of exploration exploitation inference combination uh and uh now it's just like how do we build it and the more I uh started thinking about this the more I was like well active inference doesn't tell you how to build it it tells you that uh a bunch of these things have to be either filtered for we discovered uh and green forcement learning kind of starts out with the idea of like well we don't know what we're doing how do we build it how we discover it and so then I started exploring the whole spectrum between the two uh you know do you start out with a world model that you learn but then you have a a set of algorithms that use that world model in some predictable way or do you learn the entire thing as a behavioral policy end to endend uh uh and I see these as essentially pragmatic de pragmatic engineering decisions uh but uh just like having the confidence that this approach is essentially what all intelligent entities are doing really pulled me back into this field U and once I realize that uh uh what an agent is really doing is not some idealized version of uh infer active inference because the idealized version is computationally intractable that really led me to be like okay it's not doing this perfect thing it's doing this approximate thing so if everything is doing an approximate thing and we have the technology to learn an approximator uh we we should just be able to do that uh just like have a thing that learns the set of hacks and heris that any actual agent is going to have to uh discover uh and uh we have the existence proof of being able to do this in other problem spaces like language awesome that's super informative I'll just let you have a sip of water and kind of reach a few of the key pieces because I think these are really key so because free energy principle and active inference describe like any SL every scale it has come across some of these same issues that reinforcement learning has come across on the low road that's why active inference is like between the high road and the low road because of how the architectures are it's not just a specific architecture it's almost like as you're pointing out because of the fact it's already an approximation because even the analytical part is an approximation the analytical part is variational inference so already talking about an approximation to a perfect map so at that point it really is pragmatic like you described whether you approach this in like a well structured way and there are often an active inference people like talk about like structure learning and adding and shrinking parameters and hooking up different things doing combinatorics with Bas graphs and that that may be super promising for certain pieces and like keeping it well typed and well defined and then the other approach which is resonating with with how you're saying big problems and other domains have been solved is like actually an interestingly open-ended approach which is the like you said reinforcement uh Universal function approximator so it's a complimentary approach to try to use a statistical approximator of the variational inference so my first question is H how about that reward function can you specify it different ways or have you specified it different ways for example the reward could be when two people interact and don't have a conflict or different things um because just from a sort of uh predictability perspective it's like it's it's actually a power preference centralization so you've taken a really flat uniform informative approach to any distribution except for one and loaded all the hopes and dreams on one so what do you think that does with the architecture yeah great question um I W to uh uh respond to that first I want to respond to something you just said a little bit earlier uh in terms of uh like structure uh models U as the generative model for active inference uh I think what's cool about the free energy principle is that it just like lets you rephrase anything that a system is doing as if it was it had some generative model and it's uh minimizing free energy on it so uh any any Behavior can be rephrased as a uh as free energy minimization over some set of uh some generative model over some set of beliefs uh and so it's like a really nice way to just go between two uh two ways of formulating uh uh problem and uh I think the people that try to take the more principled less design a generative model for it uh the challenges they run into is well whatever model they're designing whatever graph they're designing um has for in order to be computationally tractable with variational uh uh minimization uh is some sparse graph of random variables uh with some sparsity constraints and then they're like well what what are the what does this graph look like and uh either they're making it up or they're trying to learn it uh but uh if if you're trying to learn it you're kind of back to this thing of like well we don't know what the generative model is let's try to learn it uh but uh so I I I I see uh these approaches as kind of being equivalent is just like how much prior structure do you uh Force onto the system and then when you actually use so uh when you do uh say variational Bays what you have what you're saying is look I have this structure and I'm not saying this structure represents reality but I think this can represent reality uh to some eror and then I have this inference mechanism uh that I'm applying to this structure and together we are representing reality uh some approximation of reality uh with some air term and um what's cool about uh being able to do uh doing an end to endend thing where you're just like ah throw your hands up and be like I want to learn the whole thing is that uh the set of uh the set of inference algorithms that you get to explore are also more powerful so for example you can imagine uh parts of the model where it's like really important to get uh some detail right whereas other parts of the model it's like it's not that important you know what this random variable says it's like it doesn't have a lot of effect on the downstream task so when you're doing free energy minimization uh you uh you don't get to choose which part uh of your model is important how much compute to spend on different parts of it uh whereas when you're learning an algorithm end to endend part of the thing you're learning is when to do how much inference and how much your inference matters so I think this is another uh kind of way in which uh the the more principled approaches uh pull in an assumption that maybe is a reasonable assumption maybe is not but is definitely limiting uh yeah if I if I can just rate I think that's very farsighted of a of a way to understand this because let's just say we're using an RX and fur. JL strongly typed Julia language model the decisions about when and how to optimize the message passing timing and when would itself maybe it's irrelevant to the the question or maybe you don't need the most efficient but if you need open-endedness on how the graph is orchestrated then it just shows that in realistic even small settings the structured method is embedded within an unstructured method now that's either a manual curation like which might happen for small scale but if you're talking large scale generative model then even that outer Orchestra of level is essentially statistical so that's that's a really important thing to remember and all these other like kind of pragmatic considerations about the data infrastructure and all these types of things which are like again they're not in the textbook but to get above even a small size run they start coming into play and now I want to talk about the reward question that you asked and I think that's a really great question uh this is actually uh so uh right now and this is early days for this project but right now the reward is this heart alter and uh uh plus kinship uh and uh the thing that I wanted to do with reward is we need some reinforcement signal from the environment to essentially uh uh reinforce the policy so the policy needs to learn from something it needs some learning signal and what I wanted to do was uh make a system where you don't have to keep messing with the system you just get to scale things up and so you make it environment and then you just scale up the size of the models in the environment and as you do that the environment gets more and more complex in an open-ended way because the environment is really the minds of the other learning agents not the physical game and so uh the thing I wanted out of the reward was uh something that binds the agents to each other and to the environment and so it had to be some kind of uh competitive Dynamic uh essentially there has to be something that when one agent is doing better than another agent uh essentially there's some Zero Sum uh uh finite reward to go around so that uh there's pressure on the agents to uh be smart smarter and by smarter be meaning figure out a way to behave that gives it an advantage over some counterfactual uh other agent uh so that's the idea behind the this uh uh reward the other thing I wanted from the reward is I wanted it to be really sparse so that you're not constantly U tinkering around it's like Oh I wanna uh you know I want these agents to do something more interesting I want them to cooperate more let me add a reward for cooperation or like for this interaction I basically I don't want to be in the uh in the game of environment design and reward tinkering I want something that's just and I think it's this is a really beautiful reward it's essentially free energy it's uh you know from an agent's perspective their job is to extract energy from the environment and then use it to compete with each other and then if you ever find yourself with excess of energy that's your reward and this is uh you get this in evolution where it's like hey if you can uh get enough energy to reproduce uh if you can muster enough energy to make a copy of yourself that is the reward for the genes that's how the Gene gets into the Next Generation so you can think of this as a reproduction reward you can think of this uh partially what I'm doing is kind of re uh trying to pull the ideas from uh biological natural selection into reinforcement learning and so this is um from a genetic perspective when you make a copy of yourself it means you got enough energy out of the environment you made a copy of yourself and so the genes get reinforced those are the genes that then persist similarly here when you got more energy than the competition and you went and put that into the heart alar U uh it means that the policy is reinforced and so those are the policies that you're going to see originally I didn't even have the heart alter I just had an action that an agent could take that burns energy and gives it reward so it was essentially self-rewarding and uh I did the hard alter just because it was like easier to visualize as a human watching it it's like oh yeah they walk over there they do thing uh you can think of it as like laying an egg uh essentially and so the other aspect of the reward is the the kinship so when you get a reward uh other agents are getting some of that reward uh as well so your source of reward is not just walking up to the heart alar it's also helping other agents do it uh and learning who those agents are and under what circumstances you should help one agent and not the other and uh at what point should a resource go to some other agent uh and not to you uh so the reward function is actually really complex from an agent's uh perspective because uh it is this U social uh social reward rather than just a selfish one and then one more thing I'll add which is like really inspired by active inference that I haven't yet implemented but um is very close on my road map is um one problem with a sparse reward is it's really hard for agents to learn because a lot of times you're doing stuff and you're not getting any feedback uh and then sometimes you get a reward but it's not even something you did it's something some other agent did that's your kid uh and so this is a really hard learning problem uh in reinforcement learning and one thing that active inference gave us is this idea of surprise minimization as an auxiliary reward so um a reward that I plan to add to an agent is that there's going to be some small amount of reward that you get just from being able to predict your observation at the next time step and what that allows you to do is and it's going to be a really small reward because you don't want to overwhelm the competitive landscape uh you don't want the agent to just be uh you know all I care about is predicting you know hiding in a hole and predicting my environment you wanted to engage with the rest of the world uh but what this gives you is a very dense learning signal essentially at every time step even if you're not getting some reward from the environment you're able to learn some environment Dynamics because there's this uh reward signal about just like um uh figuring out how the world is working and I I actually think that uh at the limit uh you don't need this because at the limit being able to uh uh learning a policy that is able to get this competitive reward requires that you uh learn a policy that's also a good World model that is always predicting what's going to happen uh with more weight on the things that actually matter so it's like a pragmatic World model instead of just like you know I want to predict all of my observations uh World model which is why you can't really give it uh give the predictive reward too much of a weight because not all the things that you're observing matter uh there's an Infinity of things you can predict about your environment and you don't want the agent to be trying to predict them all you want it to be trying to predict the things that matter but I do think that it'll make agents that uh learn faster uh at least initially when they don't know how the environment is behaving yeah again a huge Point even epistemic value you at some level have to do statistics to figure out the right balancing between epistemic and pragmatic all right I there's a bunch of questions in the chat so I'm just going to give you these questions in the order so feel free just to give response as you see it okay board Guy 112 wrote in systems that lack the architectural capabilities for temporal or counterfactual depth how does the process of minimizing free energy relate to their form of epistemic foraging Behavior how is collective epistemic foraging different with increasing like agent level sophistic ation in terms of not being able to plan just like one-step planners versus like being able to do more planning yeah I think this is an interesting question uh and I think maybe I I missed uh the first part of it but I can definitely respond to the second part I don't know if there is much of a difference um an agent is something that takes its past observations and then produces the next action and I don't think there's a lot of difference between an agent that is planning versus an agent that is not planning the question is how sophisticated is the action going to be so uh and you can think about this you know you can have let's say you have a thermostat right uh people don't even want to call a thermostat an agent but you have something that's trying to control you know maintain temperature in the room and you can have a planning thermostat that says okay actually you know uh I'm going to make a plan if I increase uh the heat by this many then here's my differential equ you know here's my model of how that's going to change the condition so I'm going to instead or you can have a thermostat that just like learned a really good uh function that has uh that essentially it unrolled the planning it's just like you know what in this case I'm just going to increase the temperature by this much because that effectively is going to uh turn into the right uh homeostasis maintenance so um you the way I like to think about it is you have a system and it has some compute available to it and it has um past observations and it's going to produce the next action and it's going to use that compute in some way to figure out what uh that action is and planning is a way that we can kind of characterize what is it doing with that with that compute does it have some World model that we can understand and it's doing some future rollouts on it or uh we don't know what it's doing we can treat it as if it was planning uh but really it's just like is it able to leverage the computer how much compute does it have and is it able to leverage that compute into uh achieving its goals better or worse uh so I kind of think of an agent that um is uh uh making decisions just for the next time interval uh as not that different than an agent where you can explicitly say oh it has a world model and it's doing planning on it um it's just it's doing some computation and it results in some competence in achieving its goals yeah great point it projects on to the next action however you see it and however it's done okay Courtney wrote really cool work reminds me of things like The Sims without humans messing up their lives might the agents eventually have some kind of introspection in their model in a more advanced simulation get more and more uh uh uh like a bigger and bigger part as we train bigger and bigger networks so an agent that's trying to do something uh needs to uh essentially have some trajectory uh right if an agent is trying to get food it needs to keep uh uh it's going to do a step towards the food and then it's gonna have to do the next step towards the food so it already has some kind of uh internals that uh uh give it consistent behaviors it has some set of beliefs around U or can be modeled as having some set of beliefs around what its state is what its goal state is in order to pursue that goal uh it has to have theory of Mind of other agents right the environment it works in uh like whether or not it's going to get its food depends on what all the other agents around it are going to do like is it going to get shot or not when it's trying to go over there does it need to turn on its Shield does it need to turn off its Shield um so uh a lot of the things that the agent is modeling are uh the states of other agents and the states of other agents depend on its state you know maybe if it has a shield on it's going to get shot but if it has a shield off it's not going to get shot so it has to um be uh in order to predict what's going to happen to it it has to know what other agents are going to do which is a function of what it's doing so I think that's um that's key to uh just like the whole thing uh working at all so I think introspection is there the the real question is how do we uh how do we as external observers start characterizing it having introspective abilities uh and uh what experiments can we perform to start uh quantifying you know how much introspection does it have uh and I think that's really a challenge is not does it what is it doing but how do we figure out what it's doing how do we uh distinguish an agent that has introspection from one that doesn't or has more introspection or not and that's like a whole research agenda that I think is like really interesting and I would love help with awesome all right John clippinger wrote can you have Cooperative learning and reward rather than zero Su allaha Martin Novak shoply value uh I mean yeah I think Cooperative learning is what's happening here because no agents reward is uh a zero sum reward I mean there is a zero sum reward in over the whole system because there's finiteness of resources and I think you can't get away from that uh all all of existence all of reality is essentially allocating finite energy to infinite potential compute so there is a zero sumus nature to everything what's cool about this Cooperative learning Dynamic is that from every agent's perspective it's not an individual agent it is some uh smeared agent over other agents what it cares about what it's trying to maximize is uh some linear combination of other agents uh uh including itself so all of the learning is essentially Cooperative learning uh from an agent's perspective uh a close skin doing something that gets it a reward is the same as it doing it uh so we're kind of blurring the line between you know is this other agent just uh your left hand versus your right hand or is this your uh child brother parent or is this a complete stranger uh and uh I think the learning gets smeared across all of those uh regimes yeah uh there's so many more things I'll just make a short comment and ask one or two more questions um I I see it as as you're you're using modern methods to bring in a kind of mixed or multi-game Theory because there might be some Nash Theory or or clean analytical solution to like some single dimensional game like a just a this kind of game or just that kind of game but when there's like tool use opportunity cost co- visibility it's it's not as simple as just like as like a just buying and selling on one uncertainty so it and then also when you consider the niche and Niche modification through phermones as well as through the collective defense in like you social insects and naked mol rats and all that kind of convergent ecologies leading to evolutionary transitions in the multiscale system and then the further like canalization of that for example the nestmate workers who don't develop ovaries at all so at that point calculating these partial reproductive scores is is nonfunctional okay I'm just going to read read a few of the comments and then um but first just what uh what are your last or I mean sorry what are your next uh moves and then I'll just redo these the rest of the comments and you can respond to anything but just where where does this go SL where do you where do you hope people can participate in everything yeah oh great thanks for that question um and I'll just throw out like I don't have a hard end time so I'm to go for as long as you're down um yeah uh so uh my general plan is I'm approaching this as an engineering problem it's an open source project and what I want to do is uh iterate on the environment iterate on the training infrastructure and iterate on the agent architecture uh and iterate on the evaluations uh and uh I'm kind of uh approaching this like very improvisation Al I'm just like okay what's the what's the current bottleneck where do I think I can uh get uh progress um and I would love collaborators so part of where this Pro like part of how I think this project is going to work is based on who I can get to help along with it but the surface area of the project is huge it's uh every every aspect of it is a bunch of uh open-ended research directions with uh like and what's really exciting is like it's full of lwh hang fruit it's like for every one of these directions I have a bunch of ideas that are all relatively easy that I'm pretty confident are going to lead to really cool results uh uh so there's a bunch of stuff with environment design that is really fun so uh uh the environment you saw is really comp really simple it's just like a few little game objects U I have uh a whole uh library of uh new game objects that I want to add that will really change uh the dynamic so the new version of the environment that I've been building is entirely editable so the agents can uh build and destroy all objects while preserving uh total energy in the environment uh I have objects around like long range communication uh I mean there's just like so so many uh little game objects that you can add that really alter the nature of the envir environment uh there's also because everything that you do costs energy uh you can give so right now when you train every uh training episode the energy costs for all the actions are sampled from some distribution so sometimes movement is really expensive and you have to learn how to live in that world sometimes uh attack is cheaper than defense sometimes it's more expensive but one thing that uh I'm really excited is to U actually give different energy profiles essentially different Bond IES to different agents so that the environment then becomes heterogeneous where different agents have different capabilities uh and the system has support for that um I'm really excited to add sexual selection where uh instead of just uh agents uh having their kinship determined uh at the beginning of an episode uh they can actively choose to become closer reward share uh and I think that's going to un block a lot of really complex uh Dynamics where now you have to uh uh essentially convince another agent that hey they want a reward share with you and not some other agent which gives rise to signaling and counter signaling and uh lying and I think was also like really instrumental to Runaway intelligence and human and primate uh Evolution so there's uh that that is to say there's a lot uh to do on the environment side uh with like lots of really lohany fruit there right now I'm uh working on environment performance I'm rewriting the environment in uh using a much more performant architecture So currently I'm training my models at around uh 40,000 steps per second uh which is pretty huge uh on a single machine for reinforcement learning for uh for people that are not really involved the new environment uh I'm hoping to get more uh like a million uh steps per second so it's just gonna mean that we can train bigger models faster and really improve the iteration speed for a lot of these experiments um I have uh so um that's on the environment side there's training infrastructure there's a a few really cool algorithms that I want to uh Implement for distributed training so we can train over uh many machines uh again just trying to ramp up the training uh uh evaluations is another thing that I'm trying to develop is like how do we once we have an agent how can we tell you know is it smarter than the last agent we built is it smarter than an agent that has more or less a World Experience how do we quantify that so there's a bunch of stuff around evaluations and cognitive tests and behavioral tests for these agents that I uh uh I'm working on uh and I'm just kind of uh picking out from this giant collection of low hanging fruit and going after that um um in not a very principled way that's epic that that sounds so cool okay tree harer wrote are there any scalable Solutions like with model-based RL for inferring actions using deep active inference instead of evaluating many policies which might be computationally intractable it's related to what we discussed what what would you say yeah well one thing that I'll say is U I'm not evaluating many policies uh if you think of a neural network as a function approximator uh you're like well what function is it approximating so uh when someone has a modelbased architecture uh essentially they're giving you a function they're saying I have two functions I have the model function and I have a planning function so the model function uh is going to produ uh produce roll out in some hypothetical environment uh and uh that's a function that's like modeling environment and then a planning function is an algorithm that says given a model function how do I decide what to do next and so with modelbased uh RL uh you've uh partitioned the function into two components uh and then you try to either learn or engineer them separately so typically in modelbased RL you try to learn the model function but you keep the inference function uh algorithmically fixed uh uh in an endtoend model fre AR you're like look I'm not even going to bother separating the two functions because uh if you think about it uh uh keeping them separate having them separate might give you an easier learning problem it's easier to learn a decomposed fun two decomposed functions than a joint one learning a composed function uh gives you a lot more degrees of freedom so it's harder to learn but it also means that sometimes you do more planning sometimes you do less planning sometimes as you're planning you might decide oh actually uh in this part of the planning space I want to do a lot more I want to focus a lot more on uh roll outs so um I'm pretty agnostic to whether the right architecture is modelbased versus model free and my framework uh allows you to play with both and that's a whole research direction of like okay what is the right agent architecture right now it's a model-free uh single neural network uh uh that's uh essentially internally in ways that we don't understand is doing whatever modeling and planning that uh we want to do I think it would be really cool to say hey will we get better architectures if we break it up into a modeler like a model and a planner uh can we actually get better performance but I think these are all empirical questions uh I don't think there's an a priori um you know better architecture for it I want to play play with both uh and I think either way what you're try to learn is a bunch of heuristics awesome great points and and super interesting and so again it's the flexibility of of active inference as the topology of the compute goes from the sparer more interpretable more kind of single node semantic more definable Etc on through the dense Universal function approximator open-endedness many many many architectures already existing and many to come in that area and then together they're approximating an approximator and then you have to engineer it pragmatically either way so then in practice these considerations like are are empirical because they just relate to how you build it and so um it's on your GitHub like what repo should people look at just to be like clear with that answer what repos should people look at what will happen when they just get started with it yeah um let me yeah show show it if you can thank you yeah um so can you see this uh yeah so the re uh the repo is uh under GitHub daveta um and there's U there links to the Discord probably the best way to get I mean uh so there is a an overview and getting started uh I'm in the middle of a big environment rewrite so some of these things are going to change soon uh and if you're actually seriously thinking about um working on this uh I'd urge you to reach out to me uh and uh uh on Discord or email me because I can help you get started more quickly uh there's also a research road map uh uh here where um I just have like a bunch of ideas of things that people can play with that are of low hanging F uh um but yeah the repo is uh on GitHub dve meta that's epic thank you okay next questions um Courtney has two questions first one regarding the question about how you test whether or not has introspection and that you would like help with that research what skill set are you looking for in collaborators um I mean I can use collaborators with a lot of different skill sets uh I mean what I would really love is someone uh like uh at like a co-founder collaborator level someone that thinks about this stuff day and night like I do that would want to uh basically be like a deep collaborator so like software engineer uh ml experience so that's like on the most uh intense like co-founder level but then there's so much other research to be done so uh uh I I shared this like Behavior Uh Gallery here uh so this uh you know here you know these agents are trained under different scenarios and then we take them out of context and we're like okay what do they do in an empty room what do they do in a bigger empty room what happens if we uh you know took take uh two agents and stick them in a room with finite resources like how do they behave doing this kind of research is uh requires almost no technical skills you basically have to um uh think about like what kind of uh environment do I want to design and then you edit some configuration files to be like okay let's make you know five mty rooms put two agents in there put some food run the simulation and then measure this thing so uh that can be someone with uh very low technical uh capabilities but just like someone that wants to think critically about uh uh like Hey how do we study this new life form uh or this new intelligence um uh I can also use uh people that are uh excited to be like technical communicators and like write and share these ideas and get them uh into written form uh to be shared more broadly I can use help with u uh people that uh want to do like uh uh find collaborators or do like Outreach to other developers uh around uh getting these environments used or getting people to help with them so I think uh just like um uh a broad set of skills uh but uh the ones that are the easiest for me to leverage are going to be uh people with a coding and software experience that want to actually help uh build some of these components and that can be just like uh that that that does not need to be like AI experience just software uh being uh excited to write software is sufficient thank you great answer what over this process has surprised you the most slash what have you learned in this journey so far like about the model output or how it related to some of the other engineering just any aspect about it yeah I mean one thing that uh and this shouldn't be surprising but things are hard things take way longer than you think they're going to take and it's usually the hard the hard stuff is not some like complex uh algorithm it is just like building stuff uh getting it to work uh before I dove into the world of reinforcement learning I had no idea how uh nent that field is pretty much everyone working on RL is like RL is really hard it almost doesn't work uh there uh it's just like it's a very young field and uh so on the one hand uh uh like that was surpris surprising it's also really surprising how much of mainstream RL is working on stuff that I uh I have hard time getting excited about like most of RL is trying to play uh pong they're trying to uh you know be like can we play Atari games better than we played than these other algorithms play Atari games and that's not fair to the entire field but there's very little research going into uh using uh reinforcement learning in these complex open-ended environments it's like almost no one is working on that uh and RL uh and then U yeah I guess th those are some surprises for me this is just like a Green Pastures very interesting it it relates to something I've heard many times in the kind of ACM area is what are the benchmarks so other than just the computational resourcing and all those kinds of performance like how how do you think about benchmarking when arguably that will have open-endedness in certain ways or conditionalities or or possibilities yeah great question uh and I've given this a lot of thought and I I don't have like a perfect answer but I have a direction um so I would say two things one uh I think uh uh what I started with is behavior catalog but I think one there's like a behaviorist empirical study that you could do to these agents so you could be like okay we trained these agents in these open-ended worlds now if we take them out of their um context and actually try to do cognitive tests on them how well do they do you know can we uh offer it poison berries uh in addition to normal berries and will it learn to avoid them uh can you uh uh how in the same way that we try to quantify the intelligence of animals it's like okay can you train it can you talk to it can you convince it how much training does it need can it Sol like does it have memory like if you set it up in a situation where it needs to remember five things uh does it work what about 10 things what about 50 um and so a lot of this is uh up to the researchers intelligence rather than the entities's intelligence is like can we be smart enough to figure out how to test for cognitive ability so that's One Direction and then the other direction is pure comp uh competitive Fitness so if you have two uh policies you you have two different brains um and you have one control 10 agents and the other one control 10 agents which ones will get more free energy so essentially like an ELO style competitive uh uh evaluations U of policies is the other direction and that gets tricky because maybe uh you know in a world where brain a controls One agent and brain b controls uh uh 10 agents uh is going to have a different outcome where brain a controls 10 agents and brain one B controls One agent so you uh there's still some uh settings you got to test uh head-to-head competition under a variety of different settings and you can imagine that some do better in resource Rich environments where others do better in resource poor environments so you have to be careful around uh trying to quantify um uh superiority but I think overall we should get to a point where you can say hey look under a lot of conditions this new brand that we trained um is able to outperform this old brain that we trained u in a competitive setting and then we have a bunch of evals that are measuring things like memory trainability uh ability to uh solve problems and not get stuck at local Optima and it's uh performing better on this set of evals so I think it is a quantifiable uh approach uh is just uh it's not it's not trivial yeah great answer how do they do it in other systems okay here's a question from board guy do you make there's a few pieces of clarification do you make a distinction between concepts of peie wise versus nonlinear learning in the context of emergence more specifically do you think of these types of measures as a lack of knowledge about what the system is doing on our end as in the nonlinearity is simply the peace wise measure not tested right do you think of these two measures the the continu the continuous nonlinear and the peie wise as representative as two surely different learning rules or are we simply testing the model in the wrong ways if I followed all of that but uh uh and feel free to rephrase the question if I'm not answering what you're asking but um I uh the part that really resonated for me uh is uh uh and I I've moved really heavy in this direction uh a lot of what we're talking about is not I don't think there is you know what is the system doing what is its learning rule there is what what are our beliefs about its learning rules so a lot of what we're talking about is as observers what do we uh how do we model the system uh and we could model it as having uh learning rule a with like uh a lot lot of competence or learning rule B with less competence or a set of learning rules that are interacting uh in nonlinear ways uh so uh partially this is the way we uh we would talk about the model rather than what is the model doing itself and so I think with that formulation I'm uh I'm not that I guess interested in like what is uh like what is the model actually doing I'm more interested in like can what can we measure about this model's Behavior so uh what um when we look at the behavior of an agent is it acting in a way that we think is intelligent and uh uh and maybe that's what you're asking uh rather than you know what is the neural network doing internally and how much and uh uh it's interesting because uh the Dynamics are highly nonlinear what the network is doing is reacting to what the other agents are doing which is also what the network is doing so it's essentially uh uh conditioned on its own conditioning so there I don't think there is any kind of like tractable linear way to decouple these behaviors it's highly recurrent and highly nonlinear that's a a great answer a few other aspects is the the concept of figure and ground kind of like agent-based modeling in an environment that is like a grid World environment that is one topology and so there's kind of that statistical dense click that is the input output whether that's trained end to end or in the two separate sections and then there's that's like a bumper car with either the niche read and WR Andor the long uh scale Communications like you brought in later so then the then especially when you bring in all these fluctuating knobs like let's just say that we learn certain behavioral policies when we're in a bright room versus when we can't see then environmental discontinuity which is something that that experimentor has done to again reflect that relational piece it would discontinuously change it but then it's just like yes but the Neal networks continuous value numbers Etc it's like that's like a deflationary account because we already know know what structure our map approximator is it's just the statistics so it's it's like interesting about is this like applying active inference with reinforcement learning or one or the other and then it's like but you just look at the open source code and there's like and then it could be composed another way and having the ability to swap out different kinds of methods for each other helps highlight like that flexibility MH he wrote I guess the way you think about these has implications about whether you can simply scale it up if you add more variables um I guess more variables uh there's uh there's two places to add variables uh one place is in the complexity of the environment so as we add more uh uh Behavior like more objects and more uh diversity into the environment uh there is essentially the environment becomes more complex but really the main source of variables is the number of parameters in the models and I I assume that's what you're mostly talking about and really again from the agent perspective the environment is all of the other all of the other models on the other side of you so for every agent you have a neural network with say 10 million parameters but your environment is 50 other agents Each of which has 10 million parameters and what you do uh and what you perceive is going to depend on what those networks are doing and so yeah the idea is to scale this by uh training larger and larger neural networks and uh the larger the neural n the more parameters your neural network has the more uh it's capable uh the more complexity it's capable of representing but the harder it is for you to train it and the more uh comput and uh you need so yeah the whole point of this project is uh so in RL uh uh typically the models we train are a few hundred thousand to maybe uh a few million parameters uh in language models we're training you know 70 billion 100 billion uh parameter networks uh and uh the idea is to try to make infrastructure and an environment that is so fast for generating training data that we can uh train up these really large High parameter neural networks so that they can generalize uh over dealing with really complex environmental spaces and essentially that so that they can learn complex learning algorithms uh themselves uh so yeah the whole point is to add more and more variables to the neural network great point and that highlights that the in the in the agent versus the niche situation is averted because the resourcing of the niche is so much greater because it's only 10 versus versus if there's 100 other agents or something then it's only 1% of the resources per agent that that even that kind of structurally prepares a very different environment than a dietic even dietic collaborative stationary training Paradigm so there's a lot there with the movement and also the very cool so um any last comments otherwise I hope this has been exciting for people to learn more and to to look at the GitHub and get involved anything else oh sorry are you asking me or are you asking do you have anything else oh um no thank you so much for your time uh feel free to join the Discord uh here I'll bring the link back up uh uh and also uh feel free to message me if you want to chat or collaborate in any way um uh yeah I mean for me this has uh been a really fun project and I'm just like excited to keep working on it and finding other people that want to work on it with me epic okay thank you David bye yeah thank you Daniel bye e 
