In recent years, thereâ€™s been a growing interest in understanding how biological neurons can outperform traditional deep reinforcement learning (RL) models, especially in terms of sample efficiency. A collaboration between Cortical Labs and Monash University has set out to explore this very question using in vitro neuronal cultures embedded in a simulated game environment.

The researchers aimed to leverage the brain's inherent capabilities, which have evolved over millions of years, to compute and learn from sensory input efficiently. They designed a system, referred to as the "dish brain," where human and mouse cortical cells interact with a game of Pong. This setup allowed them to examine how neurons learn to adapt and improve their performance over time.

What they discovered is quite fascinating. The biological cultures showed significant improvements in gameplay metrics, such as longer rallies and fewer missed shots, compared to traditional deep RL methods. Notably, the neurons exhibited a form of learning that aligns with active inference principles, where they adapt their behavior based on minimizing surprise in their sensory experiences.

This research raises important questions about the efficiency of biological systems compared to artificial intelligence. The findings suggest that while deep RL algorithms require extensive training and data, biological neurons can achieve impressive results with far less input. As we continue to explore these systems, we might uncover not only the mechanics of learning but also insights that could lead to more efficient AI models inspired by biological intelligence.
