hello and welcome this is actin guest stream number 87.2 on September 10th 2024 we're continuing on with the cogar ecosystem series with John Bo so John thank you again go for it and we'll look forward to people's comments and questions great thank you Daniel uh my name is John Bo I'm a active inference Institute research fellow um I have two new papers published or excuse me posted as preprints and um that's what this live stream these two live streams are about next slide please Daniel so on September 5th we had our first one I encourage uh listeners to check that out the links are in the uh PDF and today we're talking about the second paper which is uh the cogar ecosystem preliminary thoughts on a story graph meaning representation uh next slide please so the the uh next several slides are Rec uh brief Recaps of what we discussed before uh this the cogar project is new and it's large and it's fairly complex and um it might help a little bit to set the stage for this talk just by review quickly reviewing some of the main uh components of the cogar system so cogar means cognitive narrative and it's it's a it's a it's a um the cogar system is still a concept it's it's new it's very much being created as we speak um it's conceived to be an open-source on online ecosystem of tools and services Etc that facilitates group cognition especially for large groups and especially with regard to um deliberation strategizing and Collective problem problem solving uh cogar helps users in the phrase I like to use is tell their story so um I use the word story and I use the word narrative and in in the paper and I will in this talk but I don't I don't mean story as in Fable or uh I mean story as in actually what I mean by story is is the a belief model of a user so the idea is that relative to some problem or situation of Interest a group is having a online discussion you could say and the users are contributing their an externalization of their belief model to this discussion so they are they are in this in in this story they're they're describing what's the situation or problem what happened why did it happen What might happen next uh what what might happen next clearly is predictive right so there's predictions are part of a generally are part of a story who does this situation affect who was involved what does it mean what should be done what can be learned all those kinds of questions uh are are can be addressed in a story and a story really is a a model an externalization of beliefs about how a user sees the world and sees the situation and thinks that the world Works um next slide please um so uh stories can be rich and by Rich I mean potentially long complex nuanced informative conditional Dynamic Etc so maybe a typical story might be the equivalent of a few pages of text perhaps but but could be even be longer could be like a equivalent to a book chapter for example or a story could be very short equivalent to a paragraph that really depends on what the user wants to communicate but they can be rich um and this is much different from other uh online services that are aimed at collaborative decision- making and and similar similar tasks this is not a um this is not the a you know choose multiple choice sort of uh uh system you know where you where the user can contribute to the conversation by choosing A B C or D or filling in a blank with a couple of words this this stories can be much much richer richer than that uh there is a back-end computational system and that system uh assists and guides users in constructing their stories um uh assess the stories for quality but for example is the story on topic is it comprehensible did did did the user follow whatever rules the group set out for submitting stories um and primarily the backend computational systems helps a user to D helps the group to digest and make sense of the many stories that might have been submitted and when I say many stories and group and and large group I mean potentially a group of of thousands to tens of thousands to hundreds of thousands even to millions of peoples so um you know potentially very very large groups um and the backend system also performs inference for for queries that users generates for example I'm part of a group we're all sharing our stories and I might have some question about some some particular story like did does does that in that story does the economy um you know are there does U unemployment go down you know some question like that and I can pose a query or even a more specific query to to the either to the Storyteller of that story or to the story itself and the system can also generate uh queries all by itself the system might ask when you're creating a story the system might ask you I'm I'm confused do you mean that the uh unemployment is going to go down because of this or you know some some such question like that but the system this is quite uh important the system does not predict the future so it's not going to assess a story for how useful that will be in real life or how accurate it will be um that's the purpose of a story a user does this in their story the user says I believe that if we take actions a b c and d then unemployment will um go down for example and uh the system does also not assess the realism or accuracy in of a story or a group of stories and that's really uh the responsibility of the group that's that's that's collecting the stories uh next slide please importantly those last two bullets means that the the the system the backend system does not have to have its own model of how the world Works in reality like um it does not need to know Mechanics for example or biology it just it really just needs to communicate stories and help users digest stories and help users create stories in such a way that users can communicate their belief systems and share their belief systems together so if the purpose of uh the cogar system is to facilitate group cognition you know obvious question is well what is cognition and in this project I'm looking at cognition through the active inference lens and active inference is a normative basian description of cognition there in the previous PDFs for the first series there's links that users that listeners could follow to find some literature on this and obviously there's references in my papers also uh so and uh an agent has in in active inference an agent has an internal probabilistic model about how the world works and that is the model that is conveyed really that is externalized Into The Story So cognition is uh idealized here simplified as a four-part cyclic process of predict act sense and learn so I I underlined the word act here because often that's really not considered a part of cognition but in the active inference uh world it is so um for example I'm I'm learning how to play football I predict that if I jump up high and and raise my arms I might catch a high pass I I act on those predictions I sense whether I actually caught the ball or not and if I and if I didn't catch the ball I learn from my mistakes I learn that I need to jump higher or reach higher or or do some take some other action and then we would repeat the process and over time I I get better and better at catching that ball um and an an in action an agent favor choices that ensure the greatest resolution of uncertainty under the constraint that's constraints that are preferred outcomes are realized so my pref my preferences are that I catch that ball and I really want to reduce the uncertainty that I'm going to miss it so I repeat this learning process over and over and my certainty about catching that ball goes up and up as I get better um and active inference uh is it's normative so it offers a description of offers a description of functional cognition and that's what we're after in the cogar system is to facilitate functional cognition as opposed to dysfunctional cognition so for example dysfunctional cognition would would not ensure the greatest resolution of uncertainty under the constraints that preferred outcomes are realize that would be the that would if if if uncertainty does not decrease about preferred outcomes then that's a sign that that uh cognition is is is you know may have problems um next slide please so what is kogar really then um I in in this project I I look at a group as an organism as a cognitive organism so just like any organism it needs to needs to react in the world it needs to do those that four-step process of you know sense act learn uh a cognitive architecture uh integrates information and coordinates action and every organism has a cognitive architecture and so do groups so do groups viewed as organisms and the cognitive architecture for an individual human say is the human biology central nervous system the body and also uh tools that a human might use uh computers for example could be considered a part of that cognitive architecture notes to remember is an extension of the brain's capacity to remember for example and the cognitive architecture for a group it might include rules methods institutions sensors uh you know sensors for air pollution for example for a group that might be a nation and essentially all the components by which and through which cognition occurs as an aside in a previous series of paper three series of three papers that we discussed in a previous um uh guest streams a few years ago I I look at a society's core systems that is governance systems uh economic systems Financial systems legal systems as part of that society's cognitive architecture and this cogar project is is kind of a a closely related or maybe you could say an offshoot of that initial series um so cogar is a component of the cognitive architecture of the groups that use it and its purpose again is to facilitate facilitate functional cognition and I and as I've mentioned a story is a potentially Dynamic externalized reput representation of a user's internal belief model about the how the World works next slide just a couple more of these Recaps and then we're on to the meat of today's talk um so um under the hood this is how the cogar system works um I've used the word story I've used the word narrative but um I do not mean textual uh information so it's not users don't convey and discuss and and share their stories in in text form but rather in a very special kind of meaning representation that I call a story graph um and story graphs really are the core innovation of the cogar system it's it's really what uh allows the cogar system to function well um this diagram shows how story graphs are constructed a user might either speak or write uh short text passages in a sentence a few sentences maybe with some metadata associated with that perhaps a table perhaps a graph you know something like that perhaps a a few words about how the sentence is supposed to be interpreted um uh the system accepts that uh those passages uh translates that into a a story graph fragment and then merges that story graph fragment into the growing story graph and the story graph fragment and the story graph that's that's that's the meaning representation uh for the cogar system once the story graph is constructed then several things can happen it could be translated into a natural language potentially so uh one day you should be able to you know instruct the system to take the story graph and then read it back to me you know play it back to me so I can hear how sounds uh or translate that story graph into some other alternative meaning representation that's may be better suited for a particular logical or inference some kind of inference model or some kind of analysis or translate it directly into you know either in part or in whole into some kind of probabilistic model or logical model or other kind of model to conduct uh either analysis or inference next slide please here's an example of a meaning representation that's used this is a very popular uh uh meaning representation called abstract meaning representation this is for the sentence a similar technique is almost always is almost impossible to apply to other crops such as cotton soybeans and rice uh you can you know you can see at a glance from this meaning representation that it's graph based that's one thing and that the edges in this graph represent some kind of relationship between words and the sentence the the rest of it is not important and I don't you we don't use uh AMR in the cogar system we're actually the the the purpose of this second paper and is to is to explore what is the best meaning representation for cogar um what does what should it do what should it how should it be constructed what should it be able to accomplish those kinds of questions but this gives you a this this graphical this graph picture gives you a very rough idea of of what we might be after next slide please so um the purpose of the of the story graph meaning representation is to represent a story in a format that's less ambiguous the natural language natural language can be very ambiguous and uh it should be a representation that is readable and understandable both by humans and computers as a as a creator of a story I should be able to look at that at that graph and and confirm or validate that yes that the system understood what I was trying to tell it you know things like that it provides a structure for inference and Analysis it it it serves as a showed in the previous slide it serves as input to various kinds of inference and analytical models and it serves as an interlingua which is a a natural language independent representation that's suitable for translation into other languages and and models next slide so now we now we can really begin with today's talk so what is the cogar meaning representation that's the question but whatever it is it's fit it's fit for purpose and what does fit for purpose mean Daniel if you back up one slide this is the purpose so the task at hand is to is to uh design a meaning representation that that is capable of performing in these tasks okay not let's go forward two slides all right so um I've already showed you a an example of AMR abstract meaning representation um and I've mentioned that that's a common uh meaning representation used in in um in research so uh there's others so before we before we jump into you know questions about what should the cogn um meaning representation look like it's useful to look at the meaning representations that are used in the research World particularly in natural language inference and natural language inferences inferences falls under the general category of natural language processing um there's many tasks that that fall into the categories of NLP and uh nlu natural language understanding uh and natural language inference is one of them and perhaps it's uh n is the in a sense the most difficult because in in in nli for inference this the the system or the model really needs to understand the meaning of some text so this is the this is the way that n is conducted uh in re in the research study uh some text is selected usually that text is just a single sentence or maybe you know short a small number of sentences at the most um and that text might be selected from some some database of of sentences you know where the meaning is is well understood and that text then is sent to a natural language processing system so for for parsing word sens dis ambiguation um the result of that is then sent to some meaning representation such as AMR and then the the meaning representation is used within the inference method so this is um this is typically how it's done but it cogar the the cogar is is much different this the setting is much different in cogar numbers one two and three here happen almost at the same time so and it's and it's interactive so uh cognit doesn't select text out of a database to make a story it users are are contributing text to make a story and they're doing that for some reason they want to they want to a user wants to convey some information that some aspect of their beliefs about how the world works so um so in in a in a nli study that text is fixed it it you know you you you're given a sentence and then the whole system is going to be scored on how well it understands that particular sentence but uh in cogar if the system doesn't understand what you try to say it can it can ask you for feedback it can say I don't understand who who is the word he referring to for example or do you mean flame do you mean flame as in a hot uh burning candle or do you mean flame as in uh you know a ro a romantic interest so the system can ask you for ask you questions give you feedback uh maybe suggest phrasing or or just simply say I don't understand what you what you just entered and then the user is able to adjust that text to something that the system can understand uh which of course can reduce ambiguity and error and uh and you know improve the quality of the story that the text that is being converted into a story um and uh and as I showed in one of the graphs a few slides ago the system then takes that text and and converts that into a story graph fragment and and uh depending on the use case for the cogar system say in the in a use case that involves group decision making um the editing of that story might occur in over a series of rounds where people edit their stories share their stories the system helps digest the set of stories in a given round and what you know how they've changed and what they entail and you know how they're different and how they're similar and um then at the end of each round users would have the opportunity maybe to read other people's stories digest the information from other stories and maybe change their story if they see an idea that they like or they think oh that's I should have included that I meant to but I didn't and I'm going to include that in my story now so in that process of iteration in rounds the quality of the story increases even further so that's that's really quite different from the research setting that is typically used in nli and it gives the cogar system a lot more flexibility as to how it operates it because it can interact with the user and ask questions and get feedback you know give feedback to the user uh and tell the user that it doesn't understand something when it doesn't understand something um it really has the capacity at least in you know the potential capacity to create much uh higher quality uh stories uh and and perform even more complex forms of inference than would be possible in normal nli studies okay next slide so in the literature over the years many uh meaning representations have been proposed this is this this is a table from the second article but there's even more than this this is just a some selected meaning representations and actually u meaning representations are being developed and created uh regularly so it's it's not that there's three fixed meaning representations and you have to pick one of them that's not really the way it works people are creating new ones as as new situations arises new tasks arises that they're trying to address um we will focus uh on a few of them today and the and the second paper focuses on a few of them uh particularly discourse representation uh Theory um U which is I don't know about the fifth one down or so and um uh there's a few others type Theory with records the last one and we'll also look at discos Circ towards the bottom there which is not exactly a meaning representation but it's related next slide this is a t this is a uh graphic from Lou 2021 and uh he was interested in in meaning representations for document length text and so are we you know document length text means you know potentially the the equivalent of a a book or a book chapter for example um so we have the same interest as Lou uh he's also interested in graphical meaning representations not all meaning representations are graphical and the reason that you might prefer a graphical meaning representation is because graphs computers understand graphs very well that's a it's a way to encode uh information such that computers are able to process it well and I might I might back up just a minute here and say like why you know why why is it even necessary that a that a backend computational system uh assesses stories and and and and you know does inference on stories why not just why not just write your story in text form and and you know share it with the group why why not that well the reason is because uh we're interested here in group in uh group cognition in the large group setting so if if you have a group of say a 100,000 people and each of them is submitting some long story that's complicated and and nuanced and um you know dynamic that they're going to change it you know maybe in a next week as they learn something new there's no human that's able to digest you know 100, long stories it's just not going to happen so the the purpose of that backend system is to help use help the group to digest all of that information and make sense of all that information and that's why uh the computer needs to understand what is written and that is one one way to do that is to use a graph-based meaning representation so uh in L's table you can see that discourse representation Theory really scores quite well you know for all the many things that are necessary for understanding U you know aspects of te text and um and for the very same reasons uh you know DRT is is uh looks interesting for the cogn our system also next slide please so this is an example of discourse representation Theory it's it's this is the Box form and DRT is not not graph based inherently graph-based but people have proposed ways to convert this into a graph this is a DRT for the two sentences Max fell John might have pushed him uh we have some reference reference X1 X2 uh X1 is a a person named Max uh and some event happens he fell and um he fell before some event uh and and the that event being being pushed by Bush by John and the the two boxes represent you know roughly the two sentences and they're related in the sense that this word because because of K1 K2 yeah Max fell because John might have pushed him um uh yeah that's that's that I think it's it's kind of straightforward how you would read a a DRT box next slide please as I said DRT itself is not necessarily not inherently graph-based and Lou came up with a version of DRT that he calls discourse representation tree structure tree structure brts that is uh that is graph-based and that really is uh is aimed at document length length text and uh this looks particularly interesting you know in the cogar setting because you know it's graph-based it's it's it's really constructed for long text and it has all the benefits that uh that DRT has so you could imagine a a story graph looking a little bit like this but still um far more sophisticated um next slide please that by the way that was a that was a um drts for the very same two sentences Maxell John might have pushed him another another T this is not really a meaning representation per se but closely related something that would be of interest in the cogar setting is type Theory with records so um type Theory actually is is closer closely related to category Theory uh types are really like category objects and um in type type Theory type Theory with records is a hybrid type Theory but in type Theory with records there's records uh that is you know in a sense a abstract description of some potential situation the potential situation here is uh a boy hugs a dog that's that's the event that is this hypothetical that could happen boy hugs a dog and for that to happen there has to be a boy there has to be a dog and there has to be some event where the boy hugs the dog so that's written as a record and um then there has to be uh um if that event is perceived in Life or in a story or in in the imagination there has to there there becomes a witness for that event so type the records has both record types and Witnesses and a witness really is like a proof and the proof the proof of this boy hugs dog event is that uh you know I witnessed a boy and I witnessed a dog and I witnessed this boy hugging the dog so there we go that's the proof and uh this is actually uh type theth records really kind of captures in a in a way the the way human cognition works um because I we have in our minds these kind of Poss possible things that could happen and then we witness the world uh and and process our s sense information and we usually make then probabilistic judgments about whether something happened uh it's dark I think that was a horse in the front yard it looked like a horse it was big it sounded like a horse I don't know why a horse should be in my front yard but I think I'm pretty sure that was a horse so that would be a potential witness for a horse in the yard type event uh TTR is very flexible uh Records can be nested it can be as complex as you would like uh you know it's it's kind of closely related to DRT but it can do some things and has some capabilities that maybe DRT doesn't have uh and it's inference using type th records is potentially fast uh in that it's depends mostly on type checking so you know was there a boy you know a individual of type boy was there a dog uh and so on and because it depends mostly on type checking it should play well with proof assistance for which there's um you know um sophisticated um um software programs to do that Serv as proof assistance logical proof assistance so inference with TTR could be potentially fast and as I said it mimics human cognition more so than interpretation of logical formulas it's more flexible than that next slide so what what I'm doing here maybe I should have introduced what I'm doing a little bit but I'm just going through some of the some of the uh you know kind of um meaning representations that that look uh you know potentially useful for the cogar setting so I'm going to take a a a quick diversion from that though we'll get back to it uh I'm going to take a quick diversion and talk about large language models in this setting um I think you know probably most users are familiar now with long uh large language models in the paper I call them pre-ra pre-trained language models but large they're large in the sense that have billions of of of parameters as the top graph is showing and the the number the size of the models is increasing over the years so they they essentially ingest an enormous amount of text that's available via the web and other sources and then process that text and find find associations between words between words and and between words in a sentence and between words in different sentences and things like that so that U through those ass you know it learns those associations and then can perform various tasks based on based on the information in those associations and um the large language models typically use some kind of Transformer attention mechanism which showed in the you know high level view is shown in the bottom graph there there's a some text comes in at the bottom this is a French a few French words and uh that's encoded through an encoder that information is sent to the decoder uh there's attention mechanisms mixed in there and then the output of the decoder is is going to be some U some you know answer to a query and in this sense in this in this case it's the um translation of the French into English uh I won't go into any you know it's not necessary to go into any more detail about how Transformers and large language model works but that's the high level overview um uh next slide and the reason that I'm injecting this llm discussion here is that I really wanted to talk about Vector edding because that'll be important to the next meaning representation we talk about so um you know in using large language models um words are converted embedded really into Vector vectors to capture their meaning so um this is just a really simple sort of example but suppose you have the word cat and suppose you have a list of u a v a vector really a list Vector of of 30 other words like Mom and dog and tree and R run and you know a variety of other words well cat is going to be more similar to the the meaning of cat is going to be more similar to some of those and less similar to others so cat might be very similar to Tiger so if tiger is in this uh is one of the possibilities in this in this list then that's going to get a high similarity score so from that process you could imagine constructing a series of numbers that are reflecting how similar some uh query object is to some reference objects or at least reference vectors so that's basically how uh word words are the meaning of words is embedded into vectors and then these vectors can be used in a variety of you know of comput computational tasks um it also opens up the possibility for what's called a vector database where on the left there the there at the bottom left the user has some kind of query for example how do you translate this sentence into from French into English maybe has some extra content uh that you know that's that kind of gives more information about the setting so maybe maybe the content in this example is a the French phrase has to do with the petroleum industry for example it's like a little hint as to you know little hint as to what this phrase might mean those uh you know that information then is in embedded in vectors and those vectors could be saved in a vector database very large Vector database and then at the bottom right um the the llm could communicate back and forth with the vector database looking for uh vectors that are good matches for uh what the query is and then eventually the llm llm spits out some kind of answer uh so that's brief overview of vector embedding and that'll be important in the next meaning representation we discuss next slide please now I I'm calling the Disco here a meaning representation but that's not actually true that's that's it it's it's not a formal meaning representation it's really more a computational method uh so uh discos distributional compositional circuits is kind of a mixture of category Theory and dist distributional SM semantics dist distributional semantics is really what I was just referring to referring to with a vector embedding of word meaning and it presents a a graphical calculus via string diagrams an example of a string diagram is shown there and this is not just a uh a diagram that sort of shows relationships it's actually a computational diagram it's the way the the the way string diagrams are processed really implies that there's some computational aspect going on here so let's just look at the uh at the very top left there we have some entity Alice and Alice is sober and the meaning of Alice and the meaning of sober should be merged in some way to create the concept of a sober Alice right and how is that merging done well it's depicted as a string uh uh string diagram is shown here but the the vector representations of Alice and sober are then multiplied uh you know you can think another word for vectors or complicated vectors is tensors so the tensors might be very high dimensional and then you have some multiplication multiplication happening in in a high-dimensional space and that is merging the ideas of Alice and sober and then it continues on through the rest of the graph uh multiplication of high dimensional tensors and then the answer is whatever the you know the output of the last um um tensor product uh U the the last event and that tensor product line of events so um just a little bit I don't want to really get into category Theory too much but it OB category Theory obviously is important in this sort of approach and um and as we'll discuss later it has category theory in applied category Theory really may be applicable in many of the many of the computational tasks uh that uh that cogn has to perform so I'll just say a few quick words on it uh so D string diagrams represent morphisms and mon monal categories uh uh category objects are the nouns the strings so the string for Alice is a that's a object and uh morph the boxes are morphisms morphisms are like Maps like functions sort of and they transform one type into another so we have the type Alice and we have the types over and then we're going to do something to merge those two types together um and then funs map the um mon monal category of string diagrams into another category vector vector of vector spaces and that's where this multiplication uh linear Maps can happen multiplication so uh again this string diagram is really a computational diagram more than anything else and the other thing maybe is important here is that um since these are since we're talking about um U you know linear multiplic multiplication operations in high-dimensional space um this system becomes a little it's really really best suited maybe for U quantum computers quantum computers could handle the you know when they're when as they become more and more available and as they improve quantum computers could handle this kind of situation very very well it can still be done on on on you know current systems but uh would really be ideally suited for Quantum um computation next slide please so there's variations on Disco Circ uh actually on Disco cat uh which is the the the ver discos Circ is really aimed at understanding um you know how multiple sentences fit together disco cat is aimed at understanding single sentences uh so a higher order version of disco cat uh as uh uses Lambda calculus as as a word in this higher order version is a diagram valued higher order function via Lambda calculus and then the meaning of a sentence can be given by composing Lambda terms together and then that ends up with a diagram that can be interpretated interpreted in the category vect as in discocat so the the this is again a computational graph that we're looking at it's for the sentence no man is an island and the uh Pink areas in there that is the areas where um the actual Vector multiplication is happening but because it's offering this higher order uh function uh uh it can it can do things that discusser can't do it can easily handle adverbs propositions negation quantifiers and more so um this is really interesting uh it has potential uh it it may offer a way to use the system that is um less that is more applicable to Conventional Computing rather than Quantum Computing and um it's you know obviously more flexible and can convey can can process meaning you know new other kinds of meanings that disco cir has trouble with um but um it's new and it it this higher order version there's not a higher order version of disco cir yet this is a higher order version for disco cat nevertheless quite interesting next slide please um okay so so we now I'm going to jump back and and talk a little bit about the potential roles of large language models in cogar but just to kind of summarize what we've already discussed we've talked about a couple of U of of meaning representations or things like disco Circ and higher order disco cat that are Clos closely related so we have a few ideas now about maybe what a um a cogar meaning representation might look like it's probably going to be graph-based it might look like it lose uh you know tree structures uh potential for um you know especially with quantum computers potential for kind of using in a very different approach of U of the Disco cat and higher order disco Circ when it's available so that we have a few ideas that we might work from and now the question arises well why why do why were you even want to use a meaning representation why can't you just take text you know some sentences and feed that into a large language model and and get the kind of inference that you seek do the kinds of analysis that you seek so I want to address that question um uh so large language models you know they're pretty amazing they can do a lot of things and they've really in in a you know in a sense taken the research World by a storm in the last few years and they've been used in nli so there's a numerous studies that have used a large language models to perform a variety of nli tasks and there's many nli tasks and and different databases and you know complexities of sentences and uh types of you know of of linguistic phenomena that are used in for various kinds of nli tests in various kinds of nli tests so um from from everything that's been written so far that I've that I've digested um llms can be amazing but they can also really struggle in certain situations and it happens that that's some of those situations are the situations of cogn so llm struggle with complex text uh long text uh if it's not domain specific they can struggle uh you know stories can be about anything they can be about industrial processes or environmental issues or social issues or political issues or whatever you know story can be about anything um uh and llm struggle with that kind of variable uh you know setting variable domains and also struggle regarding logical reason reasoning especially over long texts and uh in some uh uh nli t uh tests llms can really do quite poorly you know some they do okay and some they can really fail LM llms can also hallucinate they can kind of make up make up people and situations and and uh they don't they don't generally this I'm really talking about vanilla llms here but they generally don't provide explainable answers and that's kind of important because for example I'm writing a story and the and um maybe the system scores my story quality in some way maybe I get a low a low score for story quality because perhaps the story is off you know in theory perhaps the story is off top IPC or it's uh not just not comprehensible or it's incomplete or you know some kind of problem like that and I maybe want to know why why is why am I getting a low quality score in the story what's what's going on why did you give me this score and in general at least a vanilla llm wouldn't be able to provide that answer would its answer would be well I multiplied all these numbers in some high-dimensional space and came up with this value and that's why you got that's why you got a low score uh and in contrast to that um this is just one of many contrasts uh using a story graph as an intermediary in the computational process uh provides you know several utilities one of which is that that story graph can then be inspected visually by a user to see if the system has understood the story correctly I can if I'm as I'm creating a story I can view aspects of the staph to see that um that you know these two persons in the story are related one works for the other for example the system understood that correctly um so uh you know the question here is it's not whether llms should be used at all in cogar it's really more the question of um there's two approaches uh an approach to natural language inference that does not use any kind of intermediate meaning representation just just text as input and the approach where uh there is a intermediate meaning representation in this case a story graph and uh from what I can tell um in reading the literature the llms are just not capable of doing all the things all the sort of inference and analytical processes that would be necessary um in the cogar setting so using story graphs as an intermediary in that computational process could be uh I think more useful and and also keep in mind that suppose that the group is very large suppose that hundreds of thousands of people are are gathering for some some event to you know explore some situation um and and many of them are submitting long stories long complex stories well you know that you would have to somehow compare all these stories to each other you'd have to have llms ingesting very large masses of textual information and then comparing their various you know parts parts and uh you know LL and then following The Logical line of reasoning through a story and really llms are just not up to that task you know as it sits today and as as it looks even in the near future next slide please but but that doesn't mean that LMS couldn't still be useful so um uh the combination of llms and story graphs or story graph fragments is potentially useful and makes sense you know from what's in the literature today and what looks to be coming down the pike so for example um uh as a user starts to create a story graph and and inputs text or spoken language into uh into the system to create a story graph fragment an LM might be very useful to to help understand the the words of those passages and to put them into a proper story graph fragment that might be one example U and another interesting really interesting uh approach is to use a graph a story graph as contextual input to an llm so in in in theory llms could digest graphical information and use that to provide a context for the query and uh LS might be useful for example in transforming story graphs or story graph fragments into natural language to you know to to uh export a written story of you know what the story graph is saying and um uh several new more sophisticated approaches are are being developed for llm there's many of them uh some of them are actually what I've just mentioned using graphical information as input to the llm that's that's one of the approaches and another is um is uh processing the outputs of the LM through some kind of other program maybe a logic program to enforce semantic constraints and you can think of that actually as tool Ed by llm so that's really an an interesting possibility too and there's there's many others and these newer more sophisticated approaches you know could be useful in in many areas of cogar potentially but nevertheless it still appears to me that using a meaning representation as an inter intermediary either as input to an llm or as something that you know that the llm is creating from uh textual text or spoken language would seem to be the most useful um and productive U path to follow at this point uh next slide okay so now we've talked about several uh possible meaning representations uh graph-based and and not graph-based well most well yeah graph based and not graph based TTR is not uh uh HP Theory with records is not really graph based but potentially it could be converted to graphs um so now we get an an idea of what's out there and um now the task is so what is the cogar meaning representation and I said in a in a one of the early slides whatever that meaning representation is it's fit for purpose and then we talked about what that purpose is you know be for for example be readable by both humans and computers and facilitate analytical and inference computations Etc um so in the paper I offer a deera of about 20 or so items roughly 20 items um that this you know that's these are the things that we would want a cogar u meaning representation to to to do or to qualities that we would want it to have and these seven that I list here is just kind of a smattering out of that that larger set of about 20 or so um uh we would want the obviously would we would want the graph to be readable by humans and computers uh for inference and Analysis and and for computational reasons we I think we would want the meaning representation to be based uh it should be capable of handling document length text stories of very stories that contain varied linguistic phenomena and there's a lot of linguistic phenomena if you're if you're trying if you really want to understand natural language there's a lot of linguistic phenomena to understand and some of it is quite challenging um I I give examples of that kind of phenomena in the papers in the two papers uh the story graph should be reproducible in the sense that similar stories should have similar graphs um otherwise comparing graphs would not be so meaningful you know comparing two stories by comparing the graphs wouldn't be very meaningful it the system should be able to handle metadata for example if I'm I'm typing in a passage of text but I want to refer to a table I input a table of data or maybe a you know a figure or something and the the the meeting representation ought to be able to handle that it ought to be be able to handle fluid beliefs because we're not creating a story that is static we're creating a story that is that is live and amenable and if if this happens to be a group event that's going in rounds I might change my story parts of it um you know over the next hour or the next day or something um and it should also be able to handle beliefs about beliefs right so but uh in this story I'm telling you I believe that such and such person is acting the way they're acting because they think that such and such is the case uh so beliefs about beliefs uh you know uh and non-symmetric information like in the story I know that JN is a murderer but my neighbor doesn't know that um and this the the meaning representation has to handle uncertainty in all its forms uh in in particular uh uncertainty about uh predictions so part of the the inactive inference part of the cognitive process is uh anticipation of what will happen if um some action is taken or not taken so these are predictions anticipations of what will happen if certain actions are taken and a story stories in general will have predictions in them that you know typically will have predictions in them for example if we uh if we tax uh Rich wealthy people more the economy will get better or will get worse or you know something will happen so stories often usually uh typically have predictions in them and there's uncertainty associated with those predictions uncertainty even about what the story is saying like I think I saw a horse in my front yard but I'm not sure and uh if it's a horse I really better go close the gate to the Garden or you know sue my neighbor for letting their horse eat my corn or you know whatever um so how so now suddenly you know I I showed you the picture of the AMR the little graph for a sentence and I showed you the sto the graphs for two senten put together Maxell John push them now we're you can see that we're talking about something much more sophisticated here we're talking about you know predictions and models model you know the output of of of probabilistic models and um we're talking about beliefs about beliefs so that so one could imagine a meaning representation that maybe has multiple layers to it uh a graph-based meaning representation that has multiple nodes and multiple layers and uh that that a user should be able to zoom in to to look at a particular part or zoom out so there should be very you should be able to have various views of this um meaning representation uh and as you zoom maybe as you zoom out the it should be possible to kind of summarize what you're looking at if you zoom out all the way out of a story maybe you end up with something like a similar to a tagline to a movie like uh this is a story about uh how how once how a man who was once a thief turns his life around and it's you know he says it's possible for anyone in his position to do so you know maybe that's the overall meaning of the story but then you can zoom in to the different uh details um you should also be able to confer nodes and edges in this meaning representation with with meaning so maybe a group of nodes and edges really represents say a bank uh you know a bank uh has some complicated transactions that occur within it and there's ports to this Bank the bank that you know people can uh put their savings money in this bank or take out loans and various things so you could imagine some kind of compositional or you know um set of nodes and edges that have meaning in in and of themselves and are also reusable so to make it really useful if you if I want if if many of us are telling stories about Banks well it might be very useful to have some Bank objects that we have in a library that we can uh include in our story that makes it much easier to construct the story and uh ideally this uh whole meaning representation would be compositional in the in the category sense right so in category Theory G of f there's a there's a function or map f and g uh mapping um and you apply one to the results of the other that's called a composition uh that and in category theory that that composition preserves algebraic structure and then there's functors between categories that preserve structure including the composition of morphisms so you if if if it's possible that sort of approach should be uh could be used so that it it makes sense when you link uh you know computational events together in this graph uh you can that the in a sense the meaning is is is preserved or conveyed through some series of computations and Transformations and that's different than just modularity modularity really means just the ability to chain things together so uh compositional is um is you know kind of a more sophisticated setting and as I said um this should be able to reuse objects to make it really easy and fast to create graphs for some particular story so these are just a small selection of the many deada that are listed in the paper and we whatever the meaning representation is you know it would be really good if all or many of these could be fulfilled next slide please so um I've already mentioned category Theory applied category Theory so just a few more thoughts on that um it category Theory can be used to reduce the complexity of a of a you know of a system to understand the complexity of a system and to work with the complexity of a system to manage complexity and compositionality is part of that uh uh and particular there's a it is I think possible to use cets which is you know an object from applied category Theory as uh as property graphs for as the basis of story graphs so you know I've already showed you graph pictures of you know like say AMR of what kind of a simple simple meaning representation might look like a simple cogar meaning representation might look like but that could be stored under the hood as property graphs where a node has very you know flexible properties that can be attributed to it and same with edges edges can have properties and all of that could be stored at could be really uh constructed as cets uh C sets from applied category Theory um and then there's also functors for various kinds of Transformations graph to graph graph to text Transformations that pert that kind of preserve meaning through the Transformations uh I've mentioned zooming in and zooming out in summarization uh uh that as there's an interesting paper uh where an applied category approach has been used to take a system a this in the chemical world but a a system of a a systems Dynamic model of of a of a of a situation and then convert that into causal Loop diagrams and caal Loop diagrams are kind of a simplified version of A System's Dynamic model and um caal Loop diagrams are sort of more uh you know approachable for the lay public and uh but still convey some of the main information that's that would be contained in the system's Dynamic model so that would be that offers kind of one possibility of how you might zoom in and out and summarize uh meaning for story graphs and then there's also mathematical models of group interactions and belief sharing uh that could be constructed using you know Concepts like sheath and topos Theory from from category Theory so that's that's C the the application of category Theory to the cogar um meaning representation and and cogar computational process processes I think is a really rich area for for exploration next slide so given all of that uh in the paper I I suggest maybe the best approach for a meaning representation is to base the meaning representation on the on the tree structure the the discourse representation tree structure and then with TTR type three with records and higher order disco as being also potentially interesting uh but uh if drts is is used sort of as the as kind of the main idea then it would still have to have numerous extensions as per the deera so layers compositionality and so forth um and as also I think it might be useful to use a categorial grammar like com combinatory categorial grammar ccg and that kind of grammar is for example patterns in a language so uh you know verb noun adjective noun big dog uh dog runs you know those sort of patterns are would be uh U contained in a categorial grammar and then the system would understand if you use those patterns the system would better understand what what it is you're trying to say so that's a form of restriction of input user input to a categorial grammar but uh it it probably makes sense to do so to improve the capacity of the system to understand what is being said and if you were to use some kind of pattern uh in text that wasn't part of the categor of grammar then the system could say I don't understand what you're trying to tell me can you rephrase that do you mean this or do you mean this you know something like that and then one inference one potential inference pipeline is to use a categorial grammar create the story graph and then send uh uh convert that story into a logical forms like for example suitable for for natural logic and then send the results of that to a proof assistance a proof assistant for a fast inference these are just some ideas but uh at this point it looks like this this approach might be uh the most productive of of of the approaches that we've discussed next slide um so some research questions um there I should say that this is a this is a it looks like a long list but this is a short list of all the possible uh you know uh toos and questions that could be raised about uh about the cogar system and about the cogar meaning representation I mean there's there's really all kinds of questions like uh you know questions that don't have anything to do with the meaning representation per se like how do how do you ensure that um the information being shared is is you know not manipulated how do you how do you um you know ensure the quality of information that's that's being submitted and discussed in in this cogar setting uh what are the social implications of this what are the legal implications of this you know there's how do you protect privacy of of you know person's information and their stories there's all kinds of big picture questions like that and then there's detailed questions that need to be answered too by you know how do you construct what is a what is the meaning graph uh representation what is the role of of U of category Theory and developing this system um what kinds of stories are going to be applied here what are what are the categories of stories uh so just looking at the second bullet there you know maybe one early step is just to create a uh or excuse me the first bullet is just to create a small set of very simple stories maybe the stories are only you know a short paragraph in size and those serve then as the kind of uh you know proof of concept or that as the as the as the core of what the rest of the of the Machinery is constructed on um so perhaps that's a good start to generate a small set of simple stories you could even be like three or four or five simple stories just to start and then you using those simple stories perhaps develop some kind of simplistic meaning representation perhaps building on the discourse representation tree structures you know convey these simple stories in that representation and then maybe that process is even done manually at first not not in some automated fashion automated fashion the automation would come later as you as you know we get more familiar from comfortable with the approach approach and the chosen meaning representation um uh as I mentioned category Theory and in particular algebraic Julia which is a software in the Julia system uh that is aimed at uh applied category Theory uh could be useful there's various kinds of models in algebraic J Julia that could be useful uh explore cets as property graphs for meaning representations that's that's would be I think useful um uh given a simple the next bullet given a simple meaning representation explore Version Control Systems for that how do how how are we keeping track of edits to stories so that they can so that we have a history of edits and so that uh we can construct a uh you know we can regenerate a story from the various in original inputs in the edits and then how are stories uh stored what kind of database is used for storage and recall um what how do we zoom in and out of stories uh and summarize meaning of when you know when you zoom out what kind of query language is used for story graphs you have a graph and you have various questions that you want to uh ask about the graph for inference what kind of how do you query a a story graph how are llms used what role do they have could they be used for generating a story graph fragment from a a small text Passage um what is the initial simple inference pipeline does it involve for example natural uh um natural logic and and uh and proof uh software proof assistance um uh how about a simple analysis on story graphs how do you compare the similarity between two story graphs uh maybe how do you what is the first simple probabilistic model for handling story graph uncertainty and predictions how does that occur you know there's various software U programs that handle a probabilistic models so you know choosing one of them and um and you know incorporating uncertainty and uncertainty really means too you know in predictions maybe the future comes out this way maybe the you know and this this time I run the story I execute the story computationally the result is that you know events A and B happen and then I execute the story again and because it's a probabilistic story events A and B don't happen in the second round they they happen in the some other event happens event C happens then I run the story again and again A and B happen and I run the story again and again C happens and then I run the story again and now D happens so uh you know the outcomes of a story are are variable because stories are probabilistic that's the way we understand the world so all of those are research questions to uh to consider uh ideas for um uh moving forward ideas for getting involved next slide please and uh that's really kind of uh where we're at right now we have a in the series of papers I've laid out the ideas of cogar sort of the vision of cogar really um and that's all that's been done on the cogar project to date is these two papers there's there's no there's no uh you know GitHub repository for cogar code there's you nothing has happened yet this is fresh I I hope that the vision uh is interesting to not just to other researchers but the public at large and Civic organizations and people who are interested in improving the ability of society to Cog functionally cognate and make better decisions and um be functional in the group setting uh so uh I I I hope this project is of interest and uh cogar really needs your help to move forward um there's numerous interesting questions to explore so please reach out to me you know if you uh have interest um upcoming uh I I I believe that I'll be submitting a project to the active inference Institute so that'll be kind of one Hub to work through uh I'll be involved in some additional talks the active inference in Institute Symposium coming up and so on I think my email address is in the papers so people can reach me that way and or I think they can reach me through the Institute and I I encourage uh any who was interested to reach out so that's what that's all I got Daniel thank you lot of places to uh continue for people who are watching live feel free to write a question how about just to start it off what did you get more curious about during the course of this path oh well um you know I obviously was a process to create these papers and uh I I did you know at first when I started I didn't realize what I was up against I didn't realize how complicated this project was and I thought that I could I thought that I could create a working model of you know some code of a of a of a story graph and doing inference on a story graph and things like that and I suppose that could still be possible but I realized as I went through the process that there were so many there was so much to uh you know think about there was so much meat to this whole thing that it would be better for me to write two introductory papers that lay out the vision than actually constructing some initial version of the cogn system so you know that it was kind of humbling and the process of humbling in a sense in that I realized I really didn't know enough to actually start constructing the cogn system I really needed to like lay out some of the fundamentals before that step could even be taken yeah the the papers and the presentation it's like a a whirlwind tour of linguistics and semantics you're dealing and invoking with well you said like natural language is ambiguous and then it's like well what does that mean when it's precisely what it is and then here's the vector representation here's string diagrams and all these different like proofs what and all these interacting pieces like the back end could have multiple parallel or or related processing methods one just doing like how many words are there over under 300 words defined by a space between the two words and then some other part could could go through llms it's like it's more like a Linguistics Fusion approach and also you're really focusing in on the representation not so much on the the social process around how people would convene and narrativize around what it is right right right that's a that's a good point the the papers really are focused on sort of the nuts and bolts parts of this but clearly there would be a large if if cogar were to be successful there would be a very large uh potentially large social impact and then there would be all kinds of questions some of which I already raised about you know how do you protect user privacy and how do you how do you ensure that the you know quality information is being shared and discussed you know all that kind of stuff um so my thinking is is that uh really I think maybe it's better to focus on the nuts and bolts Parts first to get a simple uh you know to get simple meaning representations constructed you know convey a few simple stories in them figure out some of the inference methods that might work best for this those kinds of nuts and bolts questions and then once you have something to kind of show and tell once you have a little small working model then start to explore you know how this how you use this in the social setting how do you how do you engage people to participate in cogar you know how does that work and and even there I imagine the process that'll happen if if the pro if the project moves forward is that there'll be you know maybe the initial work on this will be of slow and detailed but uh as things as artifacts start to be developed and there's you know starts to be something to kind of put you you know something you can hold in your hand then there'll be a little Wider Circle of researchers and you know others from the Civic Society uh and just ordinary people who become interested in the project and start to contribute and as that grows eventually they be a you know start to be a little bit more of a critical mass that oh this is really starting to be a thing we have you know there's we're starting to get a repository for this we're starting to understand how it could be used what it would look like how we engage people how how how we make this you know you uh easy to construct stories story graphs out of stories how you know how we ensure that this the dialogue and the and the discussions and the sharing you know are healthy and and uh useful for cognition and and and also also numerous questions you know as that happens you know there'll be more more and more of the social questions will come into play but there's also like all kinds of other questions like how do you like in in the nli setting it's really uh straightforward how you measure success you know you you you have a fixed set of sentences you know you know this is they're gold standard so you know what the actual meaning of these sentences is and then you score the model or system that you're constructing on how well they reach the actual meaning of these sentences well they convey the actual meaning well how do you do that in the the cogar setting like how would you actually do a study to prove that the cogar system is understanding properly what you're saying it becomes a little more complicated because now you're engaging the user in creating the story in an interactive fashion so it's a you know even studying the cogar system you know from a scientific standpoint becomes a little bit more complex right because users are actively engaged in in the Dynamics and and for that matter you know how do you how do you how do you assess the quality of cognition that's a really interesting question I've in one of the slides I early slides I suggested that active inference can be used as a means to assess what functional cognition looks like versus dysfunctional cognition and and maybe you can think just off the top of your head of a few a few kind of examples where that would clearly be dysfunctional you know symptom of dysfunctional cognition for example if I had a pattern if I if I as a as a person if I have a pattern of of making really poor predictions about what's going to happen next um uh you know I see a a lion in the in the road and I think oh that's a pet lion I'll walk up to it and pet it on the head that's a really poor prediction right so that's a those kind of a pattern of poor predictions might be a sign of dysfunctional cognition well what is that how do you assess functional versus dysfunctional cognition in this group setting right there's many interesting appro Avenues to just in that question alone many interesting Avenues to explore um uh for example the brain seems to uh you know information flow in the human brain seems to occur at that edge of chaos concept you know it's it's not it's stable the system is stable but almost unstable and because it's almost unstable potentially an input from off to the side an in intuition from off to the side can rapidly change the setup so that I switch gears and now I realized that that lion really is dangerous and I'm going to uh you know jump out of the way really fast so that a system on the edge of chaos really is in many ways a uh the most efficient processing system you know it's most amenable to input it's most flexible and yet it retains its you know kind of core stability so these ideas ideas of chaos and other ideas from complex system science could actually be used for uh assessing information flow in the cogn system and and kind of the the quality of information flow that's just one idea but you know there's dozens of related you know Avenues you could follow just on that question alone I'll ask a live chat question and then anyone else write a live chat question or I'll add anything if we have time Andrew writes Dr Bo thank you for presenting your work it's a very interesting project do you have any thoughts on implementation terms of code EG any particular programming languages or libraries um well um I'm a I'm a fan of Juliet so so in the papers I I I uh I uh suggest various probabilistic software programs and uh algebre Julia system that I've already mentioned in this talk uh Julia is fantastic uh for a variety of reasons speed and other reasons now that being said this project doesn't have to be constructed in Julia but I see I see Julia as a potential resource for constructing this and and um maybe as a place to start you know initially you know certainly Julie is an open source project so as python but um you would want to engage the open source community and as much as possible in this project because there's you know around the world there's just so much talent and so you know so many good ideas can come from that um from that world just on that topic on that topic maybe this is kind of a an aside but um this is this is conceived to be an open source project and one of the reasons I conceive it to be that way is um you could imagine suppose that cogar was successful and suppose that you had a repository of stories and again each story is conveying the belief system of a human in relation to some situation suppose you had a repository of a million stories you have this really rich information about how people think you know in in different situations in a variety of different situations and um that kind of information is very powerful very powerful and you would not necessarily want some private entity so you know like a corporate entity to have control over that whole thing I think you would really want some kind of you know open governance you know an open source project controlled by some governance uh uh you know framework is to ensure that this whole thing is used for the common good that it's really used for purpose for its purpose intended purpose which is to facilitate a group cognition a a comment in the chat sasia wrote Life is thriving on the edge of Chaos and Order so it's this kind of paradox and the the precarious nature of the body like how much oxygen storage does the body have very little how much water how much nutrition like there's different kinds of reservoirs but it isn't just trying to hold on to flows so that gives a streamline nature adaptive and on on the critical Frontier and then at the same time also precarious over different time skilles and this is making it explicit with the narrative and cognitive infrastructure as the information processing semantic linguistic narrative elements of our ecosystem become like more and more intermedia of more and more active more and more powerful yeah yeah you know as an offshoot to that comment um I I I I I think that democracy the idea of democracy could be framed in that edge of chaos uh uh you know view in the sense that in a democracy what you would really like is that each person has the potential to influence the the group as a whole this you know the condition of the group as a whole and that's that's what that edge of chaos you know allows because it allow it's it's it's as I said before this a system is stable but just on the edge of of of maximal flexibility so that an input from some person like a you know an an IDE an idea comment you know in a democratic system can actually change the system you know or or a group of a small group group of people can actually change the system with some good input so I think that uh you know Not only was is that edge of chaos model may be useful for cogar but it might actually be useful as we explore you know the the interplay between you know what is a political system what really even what is an economic system what is a political system what is an economic system what is a legal system in the context of the goal of group cognition either societal cognition or otherwise yeah another I think really important point with the Jos you made of graph-based more formalized non-d distributional semantics and using llms like to project onto those formal structures generate from them first off how incredible that from a semantic graph we could generate materials in many natural languages many lengths to many different audiences if only for starting points so just on the generation side that's that's very powerful and then an interesting thing to consider is like the information supply chain for example for someone's voice in a group so then if People's inputs are taken as a blob of text smashed into a very large model and then it comes out then the information supply chain of that perspective has been collided with a very large billions of parameters lot of tokens of training and then the supply chain of the resulting outcome is like group perspectives provided and whatever the question mark unknown training and inputs supply chain that are obscured for a proprietary llm or even if it's an open source totally open LM you'd still have a complex information supply chain so this opens up an opportunity for end to end or in early stages or later stages or parallel stages to be able to have the kind of for example counterfactual reasoning like here's how this graph looked with and without here's the distribution of outcomes with and without what this traceback had done and then there's kind of a a trade-off if there's a more simple more symbolic system like more Cod like more formal semantic then the traceability will be deeper explainability yeah and then if you make it more statistical it'll start getting blurred in with more and more other factors just probabilistically but it still could be disentangled so it's like the llm route has the fluency and a lot of other advantages and and strengths and then also having more traceable like rails and flesh in in tandem could give it's just a design palette it's not that one or the other or the blend or it's situational but using the two together good point you you actually raise numerous good issues there that are worthy of you know prolonged discussion but um one of the points you rais and it's a really good point is that there isn't just one model in in cogar to you know to digest information from a group of stories and present it to the group as a summary that's not just a single model doing that there can be many different views many different questions many different queries you can just like uh you know just like the in like how do you summarize the information about a nation well there's all kinds of you know ways to do that there's graphs about the economy there's graphs about health there's gra you know like there's there's many ways to to to put together the information from a from a body of information to make sense of it and cogar would you know most likely would be multi models some some of which would be maybe more explainable than other models maybe you know some logic models might be very explainable some results of llm models might be less explainable but nevertheless there would be a uh a history of you know edits to story graphs a history of how story graphs evolved there would be there would be the possibility of looking backwards in retrospect to uh assess how the evolution happened how what how ideas developed uh you know how different models might you know might have looked at that information differently then you can assess you could assess the the the capability of cogar in retrospect about how it how it handled various situations you could assess the functionality of the group process like was this functional cognition were did did were many voices heard was that information used did was it did it represent anything on the edge of chaos where you know a single voice could have an influence over the whole you know um the the the U the uh you know because there's a history say you could examine that trajectory in in in various ways and you know assess your system and assess the group process one interesting side inter point1 to point two discussion was around like classical and Quantum information fungible and non-fungible information and how like the single source of truth that you invoked last time even though that's a fractal concept just this get Branch as its own sort of time cone of of traceability of classical information there are certain operations that you can do like the diff the merge the rebase and all those kinds of different decisions however the actual non-fungible cognitive entities it doesn't work like that so it's this interesting tension between how enabled on certain Dimensions these representations are but of course those are not the first person perspective on semantics the whole point of of the second paper is to to throw some approaches to this question of what can you make that represents some Antics but then immediately to whom SL how and and they do they they represent and are different things to different views like for example just the visual view of a figure and then the more statistical analysis on the graph that is that the same semantics is that different semantics even though it's like the same representation and so how does the representation do relevant work in a specific cognitive ecosystem and not sort of like the systems design and the metacognitive engineering question especially if not too strong of even though this is hard to to peel back to but what what biases and what priors and what biases and those are those are the same distribution right and and clearly the system would have to be very transparent to be able to you know assess all of that to assess biases in the system to assess bias in you know the way cogar assesses similarity between stories or something like that so there's there's certainly a lot to unpack with that I I do want to just to briefly mention something that we talked about in the first uh talk and that is just to emphasize that um you know we've been talking about uh groups forming to you know explore some topic that or situation of interest to them cogar actually has a wide range of use cases some of which don't look like that at all cogar could be used for example in customer service for a corporation where people um or maybe submitting a story graph to explain why their Gadget broke or didn't work or you know why why it broke or how it broke or whatever so there's really a wide range of use cases for this and secondly uh as I conceive of cogar it's uh it's a tic to what the group how the group wants to operate right so uh it's not forcing a group to use you know if say it's engaged a group is engaged in decision-making is not forcing a group to use majority uh you know you know in voting use majority rules or or rank Choice voting or some other system of voting or some other you know arrangement of how the group is structured you know whether it's an autocratic group or a Democratic group or whatever the cogar is intended to be kind of agnostic to the group setting and so that means that there's a variety of types of groups with different interests and different structures and different rules and different methods and different approaches can all use cogar in the way that they see fit you know there obviously might be some restrictions on how it's used but by and large it's flexible and and a kind of an open system so that a so that you know various kinds of groups can use it for various purposes but whatever you know the the the system is whatever group uses it for whatever purpose the the system is learning from the engagement the system is you know the com backend computational system is learning you know how people think what they what they what they think will happen next when this and that happens how how better to con how better to summarize information or you know present information or encourage a group to move towards well supported decisions when they're trying to make decisions or you know like all of that kind of stuff the system is learning in the background you know uh just like a an intelligent agent would learn in a in a new setting cool so maybe just what are some of the possible anticipated milestones for the rest of the year oh um I I most likely will try to these are these two papers are in pre-print so I most likely will try to um submit them to journals for publication although they are um kind of conceptual papers they're they're you know not the normal type of research paper con they're concept papers uh uh so that'll happen um I'll start that uh uh a group a project at the uh active inference Institute and I encourage people to reach out to me on that um and really will kind of I I hope to pick one or two of these research questions myself and focus on that next and so hopefully there'll be a couple of more papers coming out at some point here and um you know really it's up to the community as to what happens from there I I'll do what I can and if people like the idea maybe someone will jump in and you know participate and and maybe just in closing we're almost done now at the end of these two sessions and maybe it's a would be just a good idea to kind of look at the big picture a little bit as to like why are we doing why is this of any interest in anyone right it's an interesting academic Excursion for for certain there's all kinds of you know basically touches on on numerous scientific Fields so there's all kinds of good research questions but why should we even do this you know and uh to that I would say there is a you know at least I'm I'm proposing that um groups can be thought of as organisms cognitive organisms and as cognitive organisms groups you know whether they're Societies or corporations or Civic clubs or whatever um need to be able to be functional like make good decisions to act in the way that AC of inference kind of lays out in that the you know functional group is going to reduce the uncertainty about achieving the kinds of conditions that are good for it you certainly would want that for society and you would even want it for human civilization as a whole right it would be good if humans it reduce the uncertainty that we live in a the a good kind of world that people flourish in right we would want that for everybody and um when you look around today there's obviously uh many many examples of what appear to be poor decision-making on the part of humans because we face really a poly crisis climate change and and uh ex Extinction rates are skyrocketing and um we are in danger humans are in danger serious danger and um new ways of of looking at our situation our condition I think are needed in order to maybe steer a different direction to inspire a different kind of organization for society that is more functional that that does exactly what a functional organism does it senses its World it makes decisions that are good for it that reduce the uncertainty about achieving the conditions that it needs to thrive and continue and I think that a highlevel approach to uh the to you know small groups large groups societies holds great promise great potential to to suggest ways that where we might not be doing well and where we could do better and and opens up a whole host of really I think useful questions about how could humans humans groups whether we're talking about small human groups or nations and civilization as a whole how can we reorganize ourselves to achieve what we actually want but to function you know purposefully ACC fit through through organizations and institutions and rules and mechanisms that are fit for purpose that actually are quality functional processes through which we can sense our world uh you knowe anticipate what's going to happen next make good decisions and reduce the uncertainty that our future conditions are really what exactly we need to thrive okay so there's there's there's nitty-gritty detail to this project that that are important and interesting uh and then there's this big picture that I think is really really useful to to think of you know uh of of how how viewing groups as organisms that are cognitive and that cognition has a purpose functional cognition can be you know uh described and used as a target for moving human groups towards a future where we are simply more functional and I and I suspect that through the process of evolution each of us or most of us Crave You know are missing that maybe we don't even know what we're missing but were but once you get a whiff of what we what functional cognition and good decision-making and and fairness and all that other kind of you know things that go with it once that what what once you get a whiff of what that feels like I suspect that there would be a great sense of relief you know for people they would go like oh God I I'm so glad we you know we can get together as a group and be functional and make you know good decisions and we have a we have some clue as to how to assess the quality of our interactions and our in our Cog cognitive process so that we we know that we're aiming towards a future that is really substantially better than the poly crisis that we now exist in all right thank you John great times thanks for having thanks for having me yep okay thanks everybody in live chats too thank you e e 
