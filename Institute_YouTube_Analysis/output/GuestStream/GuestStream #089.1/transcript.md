hello and welcome it's September 30th 2024 we're in active inference guest stream 89.1 generic framework for a coherent integration of experience and exposure rating and reinsurance with Stefan berer and also joined by Rolan bori so thank you both for joining Stefan to you for a presentation we'll look forward to that and the discussion okay thank you Daniel welcome everyone um let me quickly give an overview of what I'm going to present it's about re insurance and reinsurance maybe some of the people on the call are familiar with what re insurance is if you are not no problem I'm going to explain everything you need to know basic what insurance reur do they provide coverage to insurance companies and when they provide a contract to cover the risk they of course need somehow to rate this risk and there's two different approaches which are traditionally used one is experience rating the other exposure rating and what I'm to present the new framework which is integrating the two into one um integrated framework basically and just to what you can see what is highlighted there are three comp components to it the first component is a generative model the second is I mean the generative model he will create a artificial or simulated claims and what we need to do before we can do an analysis we need somehow to reduce these high dimensional complex objects into some uh smaller number of what I call reduced random variables and then useing basent inform to basically calibrate the model this is basically what it's supposed to do so and what you see on the bottom I made a reference to some people in the history so the traditional approach where you use a experience rating you're taking the claims from the past you somehow project this into the future and use this data as a model for the future claims activity so that's why I put there Kepler who basically took the data from T bra and then he derived his laws for the motion of the planets and then contrast is Newton which basic can derive these laws by some much more fundamental principles so basic in one case you you model the date and the other case you model the world so this is the difference between the two of course in biology would have the Aristo on one hand side and then um uh Darin on the other other side and from the statistics what you see on the left side this is the father of the classical statistics of Fisher and on the other side to put the base so this is where the frame where we are operating now to my talk I will first give some background information and explain why this is relevant then I will somehow make a link to active inference then also point out to some of the challenges which we are facing in this uh attempt and then I'll provide with a road map for a solution and then I will show some examples and just for your information I will not show any formulas here so whenever you want to look into the formulas we can go back to the paper you find all the technical details so I'll just describe um with the help of numbers and some charts or some graphics what I'm going to or the basic ideas are so just if you expect any formulas you will be disappointed okay so first um what I've done I've taken some real numbers from a reinsurance company and this is ss3 and from ss3 I've taken not the whole business but just one segment which is the PNC reinsurance which is uh property casualty and Specialty lines um two other segments which this three has is uh corporate solution so this is the dire Direct Insurance provided to large corporations and there is also life and health so what I'm Pro showing here is only PNC reinsurance re contracts with insurance companies and swiss has developed about 20 or not 25 years ago an internal framework for economic valuation of the business and it has started in 2013 to publish the numbers so evm results which is an economic valuation of the business contrast to the traditional Financial numbers which have been published and what is relevant for us here I'm not going to into details of evm but what we have is so-called present values of future cash flows this is the basis for it and then it's um the analysis is done by underwriting year and not by Financial year so these are the two main um the most important elements of evm and what we have is let's say the gross premiums and then commissions and the commissions are the amount of the premiums which goes to the intermediaries so Brokers and or goes back to the insurance company in the form of a contribution to the claim H to the internal expenses so what matters is what they've shown here as net premiums so this is it has been increasing for about from 13 billion in 2013 to roughly 18 billion in 20 uh 23 and on the right you see the mean and then I use the the mean over the of the net premiums as a base so this is 100% And then you can see what happens to these premiums so the more the largest part of it is used to pay claims so this is about 85% then you have about 10% is the internal the expenses of the reinsure of swisser then we have some other elements like taxes taxes depend on the on the profit so it's linked to the bottom line then we have U Capital cost Capital cost is linked to the risk but these are relatively small numbers and then the finance people have introduced some small details which they put on other so there is a lot of work going into these small elements but what is really matter what really matters for the success of the company is what we have here in claims and benefits and how much premium we the company earns to cover these claims and just as a what we see here is the numbers um published at time one so one year after Inception so the 2013 numbers are published in 2014 based on the information which is available at that point in time but claims they have a relatively long um they take a long can take quite some time until they are known and paid and reported so there is only little information available after one year so there is some um development taking place later and all the developments of Prior years is shown at the very bottom and we can now see two important things one is if you go here to the evm reals you see kind of a cycle so there is some positive results then it becomes very negative for a couple of years and then it becomes positive again so this is one thing this is the nature of the business and the cycle has two components one the premium rates go up and down and second also the terms and conditions of the policies they might be tighter or uh they might be released so there's also the claims might change during the during the cycle but what is what we see here is the bottom line result and then there is another element here the evm profit the evm profit of pre pre prior year and here we can see some quite negative numbers and then here again this is something which uh you want to avoid this is the big surprise which at the time of underwriting you put aside a certain amount of the premium for the as reserves for the claims and then it turns out that this is not sufficient and then you have to uh find some means or take Capital to increase the reserves in later years and this is basically what what reur wants to avoid so of course the what is not shown here is the the initial assessment of the underwriters at the time when they look at the business so they also have an estimate for the claims this will be the claims at time zero this is are the initial reserves what we see here is claims at time one um and what is important to know of course the estimate is quite on a contract or a transaction level is quite High the uncertainty but the problem is that underwriting functions they tend to underestimate the claims during the soft market so they're too optimistic and then they adverse developments and they overestimate the claims cost during the heart market so they're may be too conservative and then might miss some opportunities and if you just look at this picture here here then you can say there is a potential for improvement in this case of CIS I mean this is my from the outside you maybe the internal people will say something different but just looking at these numbers I would say the adverse development so the numbers which you have here which in the order of 1 to two billion this could be improved potentially by about 10age points of the premiums so about let's say one or two billion per year so there is quite some potential to reduce this uncertainty that's one motivation to do this and the second motivation is of course by better risk selection you could also increase the bottom line results so this 470 million the average but let's say one which is 3% maybe put one or two percentage points of the premium volume so these are is the motivation so we talk about quite substantial numbers in the case of the profit maybe 100 or 200 million per year which additional profit could be done but better this collection and it comes to adverse development which is basically The Unwanted uh development in future years maybe one or two billion this is my claim people can prove me wrong but this is what I think is possible now sorry now if you look at this slide there is three uh charts the temporal development and what we have I've done here I've taken the numbers by subline of business this so this also published by SS I'm not inventing anything and let's first look on the left hand side what we see here is different lines of business so the red one here will be um liability on the top we have here property and we see they develop quite differently now what is this development which I'm showing here this chart shows which percentage of the CL claims is reported after one year and we see in casualty it's only about 10% in property it's maybe 50% and then it takes maybe in casualty 10 years maybe or even longer until all the claims have been reported so this just a reporting pattern and which is lined bu dependent then reported means that there is a an initi the insurance company they have notified the insur there is this this this claim and we have put up some case reserves and reserves for this but then it can take another uh couple of years until the claims are paid and this payment pattern is show here on the right hand side so you see again here in the case of liability it's very slow so the payments can can take 10 20 30 years in the case of short tail or property this is normally settled within a few years and then what I have shown in the middle this is the key topic of today so one point is the initial estimate of the claims so this is what underwriting is doing and the question is what is happening with this estimate down the road and this is shown here in the middle and here in the for the red which is liability again you can see here at time one it's only a to depending 70 80% of the latest numbers so there is in time one the reserves are maybe here 80 or 70% and then these reserves have to be gradually increased until they reach a stable level in the case of property or xent lse at time one we are higher and then it decreases so here we have kind of a favorable development so reserves can be released and here we have an adverse development which means you have to constantly increase the reserves in order to be able to pay the all the claims and if we go back here we can see in the first after one year it's only 10% of the claims have been reported so whatever reserves we have here is the estimate of the underwriters there is nothing else no other information so here we can be very confident that the initial estimate of the underr was more or less on the same level because there's no additional information coming in so they start 10 to 30 or 20 to 30 percentage points below the ultimate which is quite substantial in the case of short tail lines it's not so clear because you see here about 50% of the claims have already been reported so it could be that the claims manag they're quite conservative at the beginning they Reserve more than what is necessary and then it comes down here we don't know we maybe we started higher it came down maybe started below and it came up in the first year it's not so clear here but overall we can say some business starts to high which have a and then it has a favorable development some starts just at the right level and it's flat or you have an adequate estimate at the beginning and in other cases it's adverse and this is basically what we want to avoid now there is another complication and this is that the market is not stable there Market is goes through cycle as we have seen at the beginning with the numbers of Swiss R and what I'm trying to show here is if you take each and every contract you can put a tag on each one and say it either contributes to the economic profit or it's just adequate to cover the expenses and all the costs or maybe some business you even if it is negative but you want to keep it because you want to keep the client relationship so this then and the the problem is nobody knows what the true color is so basically the underwriters they take each contract and they do an assessment and they say one contract is green one is Amber one is red but it could be POS false positives or false negatives this is not known and of course if a contract is red and you say it's green then you write as much as possible of it and then you end up having this adverse an adverse development and of course you also then you have the winner scur so you're much much more likely to get this contract than having one which is positive and everyone else also sees it positive so this is kind of the um broader context in which the underwriting or the market is operating and if you say the underwriters are biased and you can say they um are so there is a true let's say that the true border between the three B buckets but the underwriters they believe that the border is may be up here um in the case of a hard market so they are too conservative they say some business which actually is already Amber they put it into the red bucket and some business which is Amber they would put it uh which is green they would put it into the Amber bucket and why is ver during the soft market so this is the problematic and by improving the uh underwriting and by avoiding the bias and minimi or reducing the uncertainty you can get this dash line here closer to the true border and here as well so that's the aim of the whole thing so this is where I'm making a claim that there is a quite some potential for improvement um for reinsurance company now there is of course also different approach in reinsurance so if you're a big company like ssy then you have to do this Assessments in other cases if you are a small you say I don't do this myself I save this cost and I just follow the leaders yeah then you just basically can do it for free and you try to write the same business as the large ones and then you have it the same at the lower cost ratio this is also a possible approach or you can say I don't care over the cycle the market will correct itself and I just write and that I'm sure that I can survive the sof market and then I will earn during the heart Market that's also a possible approach so we talk here about a company which does a fundamental assessment and that's why it's necessary to go through this analysis okay now just a few thoughts about how to link this to active inference and what I've course been thinking about the corporations so they corporate agents they act very similar to humans um I mean there are also competing for resources they have an incomplete knowledge of the external World um they also use the internal representation of the external world with the help of models and so on then we have multiple departments which in our Corporation and they use of course they have a different picture of the outside world and they also have different part interest so they are not always pursuing the same the same goal so this is kind of maybe we feel the same as humans we are also not always sure One S wasn't wants to go in One Direction on the other direction so I think this is very similar to corporations and of course corporations at the end they have people there at least this is still the case so we have also biological brains driving them and I think active inference is a very good tool to also understand the behavior of corporate agents in many different levels but what I'm doing here I'm just focusing on underwriting or inurance and um what I'm going to show how gen um generative models can be used in this process and maybe just if you want to look at uh a broader role of uh active inference so in the in economics then there is one link which also introdu the term of cognitive economics so this is where it in the broader picture where it would fit okay now what I'm showing here is what you need to understand about how re Insurance works so we have the external world where we have U objects at risk this can be buildings this can be people this can be airplanes this can be corporations which maybe could default and so on so these are the objected risk then we have the range the insurance company which I call here the sedent which issues policies to cover some risk for these objects then there is the external world where you have the generative process which creates all kind of events and some of these events Like An Earthquake or Windstorm or an accident or a defaulting company results in insured claims now the reinsurer doesn't get all this information which is available on the site of the seeden company what the seeden does it creates a submission and the submissions you can put it into three buckets in one bucket we have the claims which are reported to the insurers normally this is claims about for certain threshold the second bucket contains the contract terms so of course you have the uh what were the insurance conditions in the past but maybe also some insurance conditions so insured retentions or some limits and the like so what is which act like a filter on the claims and the third bucket is the exposure profile which is basically some statistics about sometimes also details about the insured polities and objects at risk so in the case of earthquake you maybe want to know is my the building which I'm covering um in California on top of fault or is it somewhere I don't know in in the center of Africa where there is no earthquake risk and so there it's important to have also some additional information but what exactly we find in the exposure profile the depends very much on the line of business the G of claims this is more much more standardized basically what you have is the amount of the claim and the payments so you can put it into cash flows and some outstanding claims so this is some it's much easier to standardize the claims but in exposure you need specific information depending on the line of business then what re insurers then have they as I mentioned already before you have two models one is experience rating the other is exposure rating experience rate takes the claims and contract terms the exposure rating takes the exposure and the contract terms then there is a quite heavy involvement of the underwriter or the ACT which is depending who does it and in both models you then get some results with some statistics and since these two results can be quite different there is some blending process that keep place and then you end up with the projective statistics for the cover year then you take it into consideration other uh things like you have of course underwriting consideration client management this has some saying and risk managements and finance so there is now this information is just one element in the decision process and at the end of the day you end up with a reinsurance contract for the future so this information is about the past and the present you and you use it to project some statistics and the and the expectations for the future this is basically where you have the expected loss cost which I mentioned before and which you want to be unbi be unbiased and um as minimized uncertainty so this is where the circle is closing you could also represent it with a mark of blanket you have um some some things happening inside of the blanket information coming in and some action going out so this is where active inference is basically uh visualized okay now the step to the integrated approach and is so basically we're looking now into these two parts or these two models The Experience rating and exposure mate uh model this is the focus area and what we do here we have two things first we have a generative model now this generative model is very similar to the exposure rating model but the difference is the exposure rating is only applied to the Future to the cover year while the generative model is basically taking the same sorry model and you apply it to the past so not you don't only apply it for the cover year this is then comes later but you first do a a calibration of this model and this calibration is done with the report claims so you use the claims not to project them into the future but you take the model and project the model to the past and then you compare the outcome of your model with the actual claims and then you calibrate the model and once it's calibrated you can take this model projected to the Future and produce all the statistics you need for the future cover so this is the basic U uh secret of this presentation so as I mentioned before it's a generative model you have reduced claim statistics and you have Bas in inference which you apply to calibrate model maybe you have to go through multiple C iterations but this is basically where the maging is happening and what I have done in my paper I've basically looked at this part here because here the generative model you need to come up with very specific model for each business segment so for each line of business you have different data available you have different models which you are using so this is where really the experti of the rer comes into place so they will have to provide these exposure models or generative models for each segment but once this is done then you can basically standardize this part here the model has to produce some claims in a standard format you compare them with the reported claims and you then come up with some calibration parameters which you feed back into your model and once this is calibrated you then can then apply to the Future and do everything you need to do on on the right hand side that that's basically the the magic here and we will see it's much then it's much you need much less um input from the underwriter because here in the past when you need to when you uh do experience right you have to project these claims to the Future so there's a lot of assumptions going into this process because the if you go to the past the portfolio was different maybe the claims you had in the past are no longer representative so there's many many um assumptions which come from the underwriter and this can be avoided in this approach here okay now so far it sounds relatively easy but of course if we we easy maybe it would have been done so there is some challenges which you need to um resolve and to understand this what I've done here and just taking a few simulated claims and what we have here is the time AIS um in the vertical we have the incurred amount so the estimate of what the ultimate is going to be and in the y axis I have the paid amount and let's assume here at the end it's settled so you can see the estimate so first it um goes up and then it comes down and then there is some adjustments in the end it goes up again so you can see this in this curve here the paid first there is nothing paid and then it goes up and then eventually we have the final payment so this is one example but what can also happen is that at the beginning you know nothing about the claim so the claim is reported with some DeLay So it the accident date is here in 2016 but the reporting date is 2020 so in this time you have to put up socalled ibnr are in incurred but not reported reserves for all the claims which will be reported later we have also seen this at the beginning the case of C the reporting pattern was the chart on the left so this is the the nature of the business now if I take one claim so the whole graph here is one claim and you can either represent it with a graph in a threedimensional space as I did here or you can take each payment and each incur adjustment and say that's just one element and then you put it into a high dimensional space so you can say the each claim is an element in a high dimen sample space and the dimension here is maybe some some between 10 in the best case or can I go be up to 100 so it's very high and it's just High dimensional space where it's very very sinly populated imagine you get about maybe 100 or in a uh or 1,000 claims per submission and in a um if you have a claim with 100 or 50 or even 20 Dimensions you completely lost there's nothing you can do so what is done here and this is maybe the only formal um um symbols I'm using here I say the sample space is high dimensional and what we do with the compos it in M onedimensional spaces that's basically the whole idea behind so we need to find some variables which are more or less independent and we project these claims to these uh subspaces and then we can solve the problem in in a onedimensional space a couple of times but it's end with a onedimensional problem which is much easier of course because then we have 100 to 1,000 claims in a onedimensional space and this is easy to handle so this is the the second part so the reduction and I will show you how this is done but just to understand that this step is necessary else you there's nothing you can do now if I go here how can we solve this so this this is again another representation of the standard approach so what you have down here you have claims which are in this case 10 years old so I use minus for relative to the underwriting year so it's underwriting year minus 10 here these claims have been you have a reporting pattern of 10 years so you know uh the first 10 years of the development but for the most recent years so which is minus one you have only one year of development so and this is now if you try to do traditional experience rating there is two things you need to do you need to project all the past to the Future so this is one step yeah so you need to Trend the claims and say on as if basis a claim which happened in the year minus 10 what would it be if it would happen in the year plus one which is basically my cover period and and then what you see here is you get these kind of triangles so some claims you have 10 years of development but the most recent one you have only one now the problem is that these recent claims are may be representative for the portfolio but they have very little development the old claims you have a lot of development but they're no longer representative and now what you need to do is you need to take this triangle and project it to the uh to the end and see what is the end of these things so there's a lot of as I said a lot of underwriting assumptions going into these to these fundamental steps and this is the experience- based approach in the case of exposure rating what you to take the information which is available at normally the minus one so the market experience these are the market models you have the exposure profiles Insurance conditions you project everything to the cover period and you apply your model and you get an exposure based uh result and then you take the two blend them and that's then the result um I can take the same now and represent what is it in the this gen integrated approach so again the three steps so first I now use my exposure model not only for the future this is then the last step this is what I will have to do but before I do this I'm calibrating my model so I apply the model to the past so I take all the market experience which I had in the past the profiles from the past the the models which can change over time and the insurance conditions so I have a one model for each year some of the information might be the same but basically I can now run this model for the year minus 10 and I can create not only one but 100 or thousand or as many as I want simulated sets with claims um and I do this for the entire period so I have here now a large number of sets let say it's K different sets and I have one observed set and now I can say the general statement is my model is calibrated if the if this one observ observe set of claims is not an outlier in the set of all um Sim the claims so if this is one um element or basically one um member of this family here then I can say Yes um I cannot reject my model because it is compatible with the data and once this is the case then I can project it to the Future and um run the model now on the problem is now doing um comparison on this level so compare the all the claims which have in here with all the claims have in here we are operating in this High dimensional space and there is basically no way to do it so what we do in a first is we reduce the claims so this is happening here and we represent the simulated claims in the same way as we represent the observed claim so we use the same statistics so from the latest year I could of course I could simulate it till there end but I'm only taking the first year so the system knows behind what's happening till the end but for the analysis I'm only taking the first year and if I go back 10 years I take the first 10 years of the simulation so basic exactly the same representation of the claims in the simulated sets and in The observed set now I projected as I said before to this um one-dimensional spaces and then I can do the analysis and I can up with calibration parameters this is the um how this basically works now we can have now a look at the how do we reduce the claims and I have shown here two very simplified developments one is the reported pattern and the other or the incurred and then the paid amount the in the case of the incur is again it takes a couple of years until the or some time until the claim is reported so I've here a reported LA and then the claim then first uh estimate of the incurred and then here there's an adjustment and here again and then it go this continues into the future until the claim is closed but of course this information I don't have so what I'm taking here is the latest estimate for the ultimate this is what I call I for the incurred uh star for the case simulation for instance uh on the right hand side I have then the payment patterns and what is important to make sure that the payments are always smaller than the incur because usually um um as there will be this is one of the boundary conditions in in this information but we don't really care about the individual numbers what we do is we take here an average leg it's basically the area which have here above the curve so this gray area here if I divide it by the amount then I get the incured leg this is this part here this is one of these reduced numbers so I'm not looking at the details I'm just taking the average leg which I have here so this is one figure I'm taking and and then another statistic I can use is the number of adjustments which I have S this is one 2 three that's also one number I'm using to calibrate so to say how many adjustments is an insurance company making for the claims is it large or is it small so these are the reduced statistics here so the reporting lag the um the amount of course the incurred leg and the number of adjustments and on the paid side I can take very similarly the paid amount or the ratio which is paid I can also take a uh a paid L and also the number of payment so these are some of these reduced random variables which I'm using for calibration so I'm simplifying and these num numbers are considered to be largely they're not completely independent but they are largely independent now here is another um representation of the of the process um so we again we have now at the bottom the events which are leading to insur claims we have the set with claims and the the part which is observed this more or less what the seeding company is reporting to the insurer and then we have a set of reduced claims on the S other side we have now the generative model which is creating the K sets with simulated claims this can be very detailed there maybe in the first step you align this claims representation with the observed claims and then in the next step we reduce them and then we take this information here to come up with a conditional distribution functions then you take the obser ations and you apply base and you end up with the calibration parameters which you can feed back into the model so this is the another representation of this process and once the model is calibrated then of course you can go and apply to the Future and do everything you need to be able to rate your business okay now let's look at some results um first what I've done here is some very simple I've used in this case not actual claims but I've used the simulat simulator for the generative process and what I've done here it's a Lo normal process and I assume there is some Trend so I go back the years and I have this the true probability distribution for each year which is represented in the vertical axis here this you see here the probability and I've drawn one random number for each year which is shown here with this uh small diamonds and what we also have here is then the projection to the Future so assuming this trend continues we have then the this is then the true in this case uh uh um for the generative process which simulated but this is the true distribution in the future now here I have my generative model and you can see my initial model is is flat so there is no Trend in there and what I can do in a in a simple approach simply say okay let me try to adjust the level so I'm just moving this model up and down and try to align it with the data this is done down here so here we have then the again flat model but you can see at the beginning it's too high because the observations are on the lower side and at the end it's too high because the observations are on the higher side so actually there seems to be some Trend in the data and which of course reflects the trend in the underlying model which is unknown now if I can also do is to say okay I'm Trend I'm I'm calibrating the level and the trend and this is what I've done here in this third picture here now I have you can see a model which is increasing over time so this is the generative model and you can see now it's well aligned with the data and if I compare now just the the first two moments so the mean and the standard deviation then here I'm quite close to the true hidden model so this is basically unknown but this is what I've put behind the scenes the true model is 83 with a standard devation of 8.3 and here I have 88 with the standard deviation of 8.8 so it comes pretty close here my initial model was 60 with Sigma of six if I calibrate only the level I end up with 72 and the standard Sigma of 72 7.2 so you can see if I use this approach then I can get very close to the hidden true model here is a second example which is now a compound model so I have a uh simulating the number I take a negative binomial distribution to simulate the number of claims and then the log normal distribution to simulate the severity of the claims and this is again shown here and the same approach um you have the just calibrate the level and you calibrate the level and the TR TR and you get pretty close to the initial model so just a different process behind but the principle is the same now here I have another mod approach where I show you a little bit more in detail what's actually happening here I have again a negative binomial distribution for the generative process now here the I've shown the pro U probability Mass uh function so the probabilities for each uh outcome and what you show see here in the second part is then a log normal fit to this distribution and I could I could do the analysis with the discrete distribution which is possible but it's could be computation a little bit more uh um challenging but it's doable but what we can see as long as we we want to we assume that the observations are in the high density region of the model so we can easily replace it by a Fed distribution which is a much easier to handle because here can use a um um um oh can I find the ter so the whole Basin analysis basically much easier if you take a l normal distribution then compare to a negative one now here again done the same now here what I have the probability distribution is not the model behind directly but the simulated uh model so I have the take my generative model I'm simulating a large number of claims and then I take the empirical distribution as the basis for my analysis and here again I fit a lock normal distribution so we're just fitting the first two moments and then I do the analysis with this um fitted distributions what I've also shown here is then I've aggregated the whole 10 year or what is 12 years here into the uh into the mean and shown the distribution of the mean um and I can use this distribution to calibrate the level if I want yeah so the level calibration I can do by using the annual data but I can say I'm just looking at this overall distribution and I calibrate it so this is when you see here then you see the observation uh and the calibrated observation is more or less in the center of the distribution but still if I look then at each and every year then I still have this uh Trend which is missing so if I calibrating level and Trend then you can can see here I come I'm up with a model which is very close to the initial or to the true hidden generative process so this is um the how it works so you take your generative model to the create a um simulated distributions you fit uh discrete dist uh sorry a continuous distribution to it and then you apply B by using your priors and you find out can find out the calibration parameters now here what I've now put everything into uh one chart to say uh at the end what we have is we have seen we have different um reduced random variables so here is the number of reported claims then we have the reported lag and the incurred amount the incurred lag the number of incurred adjustments then for the paid we have the the paid lag and the um number of payments and what I've shown on the right is some then additional have a strongly dependent variables which are we can use as a to observe to check if the model is okay but in this case I've only calibrated this green area here and here I've just shown one uh hand where can change the level and another one where I can change the trend and maybe I run multiple iteration so in this case I think I run it five times to basically uh be able to deal with the dependencies and what I've shown now is the if you can see here this is now the the distribution which I'm using in as a uh as a prior and then behind this is in light gray you can see the true distribution so you see here there's an offset and after calibration you can see these two distributions are basically overlapping which means there is no Gap anymore and this is here is another big gap here it's already at the beginning okay um and so you can see the calibrated model is basically very close or the deviation like here is smaller than one standard deviation so this is basically as good as you can get so this is the um how the whole framework works so you apply to multiple random variables so the ones I'm introduced before so the which have shown here you can have some additional random variables you which you just Monitor and then you run your s your calibration model until you are satisfied with the result and then you apply the model to the Future now just to summarize I've looked here this also can find in the paper a comparison so the integrated framework what are the pluses and the compared to the standard approach and you can see it is when you use all information in a coherent way you don't waste any information in the standard framework there's a lot of information you cannot use and it's not aligned so it's uh this approach here is Superior in all regards but there is of course it's not for free the price you have to pay is the the computational effort and Al the effort to set up the whole system because you need to be able to run it to the past of course this effort is much higher but you have the benefit that maybe you can sub substantially reduce the uncertainty and the bias in your estimates so this was the motivation which we addressed at the beginning okay here just the summary so basically we have this the process and the traditional model and over here the integrated approach and it's a kind of a paradigm shift which requires that you have as a new mindset to approach it because the actes especially in certain um in the angle section region they have well defined processes is how they are supposed to rate the and assess their insurance Bas this is you will not find in any of these uh um um documents which are used so it's completely different approach and this will the market will probably uh is some areas maybe not open to it so you need need you need a different mindset you need the different skills so I think once you want to introduce something like this you need to run this in parallel to traditional approach until realize that it has a benefit and maybe it's a similar process when as when swis introduced the evm framework it took about more than 10 years until they went public with it so these things can take quite some time but maybe there is the new uh technology which is around in artificial intelligence maybe there is some way to shorten this transition period okay this is my part um so thank you for the attention and as I think we're open for the discussion and questions thank you for the great presentation anyone in the live chat can write a question otherwise Roland it would be great for you to say hello give any remarks in a first question and then Scott thank you sure so first of all congratulations Stan so Stefan used to be my boss at swissy and he's one of The ogs Who who came up with the uh with all the pricing Frameworks that are still in place today and um uh so I'm also I I was super happy when I met him again and he he showed me what he's been up to here and I think that's really where to take it because when I look back in the last 20 years or so there were like almost no developments in this area and uh I think uh nowadays uh with all the AI happening it's it's really time to do something like this but now comes my biggest question to you Stefan do you think that the world is ready for this do you think that anyone will look at this that there is any any form of acceptance for this you see I'm very I mentioned before I'm very skeptical because we have um the standardized approaches and I have already tried also to approach some I mean left swisser about 10 years ago and I was doing something completely different and this then came up somehow as a side product of other activities and then I remembered I had this problem in the back of my mind which I wanted to already address at that time I presented some ideas at the time and nobody was open for it and if I by talking to some of the former colleagues I have the impression they are not ready that's why I mentioned here it requires a new mindset maybe a new mindset requires new people and I don't know what has to happen to the industry maybe there is some smaller companies which say oh great this is an opportunity for us so let's jump on this but I think the old dinosaurs um so far I don't see any um willingness to change their approach okay Scott if you'd like to give any initial remarks Mar S I know you wrote a novel in the chat but if you'd like to give any remarks and maybe a question somehow concisely Daniel knows that when I write a lot in the chat it's excitement so Stefan uh first of all thank you for fantastic U presentation and it's a I feel like first saying where have you been all my life because you you given voice to a lot of the things that felt ever since I've heard about active inference felt like they needed to be thought about in risk and so um I was a a corporate and tax attorney for 30 years um and so I see these things and I so my I think my main question is and one is one of my latter questions in the chat I'm always looking for ways to introduce new things where you don't have to rip and replace because people who uh remember talking to the head of security of AT&amp;T one time and he said why would we do anything different next Tuesday than we did last Tuesday when we made a billion dollars so the so how do how do we get people to want to do something and what I wonder is as a immediate matter could the generative model the application in your slide right here under the Beijing approach the projected generative model to the Past could that be included as a calibration in effect but on the existing projection of claims history to the Future that analysis is there a way to make the thing that we're aspiring to migrate towards a part of an existing part of the analysis so that it's seen as being additional but not requiring migration to the new analysis entirely is there a way to bolt this on so that people can start getting used to it seeing its value in the context of the analysis that's already being done or or are they so fundamentally different that the new approach doesn't lend itself to being um offered as an add-on in existing approaches what are your thoughts on that yeah uh very good question but if you look at these two slides just over here yes and over here both approaches use more or less the same information of course you might say okay in here maybe have a different model than over here but it could also be the same model so you can I think it's necessary that in the transition phase you apply this New Concept in parallel to the existing uh methodology so the people can also get the feeling what is the difference and then it will take some time I don't know how many years until you see the results because you will not see the results immediately it will take some time until you see that yes down the road we have really much less adverse development in the claims and this you need maybe you need about 5 years of uh experience until you see the results really materialize and this is probably also the time it takes for the people to familiarize with the methodology I think if you want to implement something like this you need to do it step by step maybe you don't do it at the beginning for all lines of business you maybe confine it to a few lines of business and then over time you can expand to some other and of course people need to get used to it and once so you run the old framework in parallel and then you can compare and it's time you get the sense for it and once you feel um comfortable then you can basically turn off the old part thanks again for for all I'll just G give a short uh note this slide really captures the difference here going from extrapolating descriptive statistics on observables into making a generative World model that's based upon or conditioned or updated from observables but also has latent States it's like we don't remember the time series of visual input and then extrapolate we update based upon the observations and then use that to do the now casting and the and the forecasting so so that has all kinds of beneficial properties uh the the reduced represent uh and the ability to explore direct counterfactuals generate synthetic data there's just a lot of things that are purely time series correlation based approach doesn't really get at the question I wanted to throw out to all of you was as a non reinsurer coming from a biology perspective what are biological analogies to the reinsurers or the underwriters role like could it be be considered to an epistemic foraging portfolio like research projects or could it relate to like individuals and they're waiting in a group like Risk profiles of different foraging strategies within a group or how how does it happen because that idea of being uh too or too little cautious in a changing context with the hard and the soft market and there's a TR there's a possibility of of being over or under risk averse in either setting but you don't know the true risk setting maybe let me try to First keep Pro Just sh a short thought if I look at biology then I think biology you find it over here and I don't have difficulties to come up with an analogy in biology which which correspond to this approach here but maybe someone else has a better idea but maybe just one with something I did here it's kind of intentional I put here let see one part of the information into red one into green and one into blue so you could say this corresponds to the uh um cones you have in your eyes so basically you also have three receptors for different information which of course it's just now it's not really one to one comparison but this is maybe what I tried to do so you have the receptor you combine this this thing you come up with the internal um view of the external world and then you act on it so this is the analogy as far as I can see it but maybe someone else has a better uh example in mind I mean at the at the broadest level sorry Roland did you have something no no just go ahead David I was to say at the broadest level I put it in the chat I mean it feels like so I used to work in securitizations so you take credit card portfolios or car um uh Finance portfolios and put them into packages and sell them off and I used to occur to me that what you're doing is trying to find a bigger sucker uh for the risk right you package it up you make it look good and that was the 2008 Financial break was packages of mortgages they were crappy mortgages cu the banks were enthusiastic about selling off the mortgages so they made the mortgages so they were appropriate for sale in these packages but they weren't necessarily good risk so the risk just got iterated up and when I'm wondering you you think about a biology what do what are the name of the organisms that eat dead decaying matter like Leaf litter and stuff like that um whatever they I mean that the death it's everything's recycled and risk is recycled continuously that's what biology is so you have this big neant Tropic urge that neg Tropic beings have whether they're physical or not and they're all dealing and everyone loses ultimately their own individual battle on neg entropy all organisms die but then they're recycled by other organisms and so they're coites you know that eat poop you know things like that I mean there's all sorts of recycling that happens and so so it feels like from a reinsurance perspective you have abstractions of risk you what you're with with all Insurance you're creating instruments to create a a an organism a Sim bio Sim infogenesis organism that can cause you to drisk in ways you can't do Alan period so it's like a UK carot right you're adding people could just not do reinsurance and not do insurance and self-insure and then have to pay the full price of their house burning down or whatever but we have these abstractions where we kind of develop these eukariotic things and say hey let's do this together it's a good idea because then I just pay a premium I get a house back and then the reinsurers say hey let's do this together we got all this different flood kind of risk or whatever and we can do an evaluation at a metadata level it feels a lot like exactly what happens in biology without memos and voting and all that and so the power of what you're talking about revealing Stefan feels like this is how stuff works if we don't get in our own way politically and with false models of from a history of Economics Etc and if we really allow for the um information to flow among the entities at risk doesn't mean that every organism survives forever it's not about perpetual motion or Perpetual you know Perpetual life but rather there's a balance that happens naturally so so that feels like as an an immediate matter there's an ability to sell this to the quanton Wall Street and say hey your model is slightly out of whack with the biophysical physics reality the mathematical physical reality and so hey you're an arbitrager if you want to make money on the difference between reality and your model here it is you don't have to do it do do what you're doing so it feels like it could be pretty attractive in the right kind of audience for people who have to deal with risk professionally that they can't turn away once you say something to them if it has value and shows value they won't be able to say oh no we're just going to do it our old way and their clients won't tolerate that anyway sorry I got a little excited there it's great great SC yeah but it's you go you go in the absolutely correct direction I mean it's I've I've had to to calibrate a a lot of risk models in my life and I mean when you look at the definition of risk it's what happens above the expected so normally you subtract the expected and normally you have to answer questions like what is the worst that can happen within a 100 years and uh hardly any one of us has ever lived 100 years to actually tell right uh and so um actually what you learn very quickly when you're a risk professional is that you have to break down the question to something that fits the experience Horizon of a living being and uh so typically when I when I was calibrating risk models uh I saw okay this guy has been in his profession for 10 years so um I I was asking questions around what what was the worst thing that happened in 10 years and then given a model you try to extrapolate so what's the worst that can happen in a 100 years right and in my opinion this approach here is is just doing it much more systematically you know uh so we always had a bit of a tendency to do this but it it was more heuristics and and this kind of gives it a more a more scientific uh way of of of looking at the problem that's that's awesome I'll just make one quick sorry um I mean you mentioned Scott mentioned the the CL Financial Market you know when I joined the re insurance industry this was many years ago the first thing which happened was the collapse of the so-called LMX market and this was the London excessive loss Market um and what they did there is they just handed the risks from one to the next and it went around the circle many many times and then there were claims and the claims were also circled around so the same claim you had already sudden you get it by the back door again you put it on top of it and this happened until someone run out of cover and then it w there were some big losses then happening some interal events some other things I think there was a piper Alpha and there was a wind storms in Europe there were hurricanes on and suddenly this whole thing collapsed yeah so it was working before because the the circling the things around took very long as long as it was with paper but then they started to introduce uh new technology there was time of computers the first computers so the whole thing went much faster and the whole thing collapsed and so the industry had to learn already in the past and I don't know what has to happen for the industry to maybe go through a new uh leap and to start to implement new uh um approaches because as long as there is nothing massive impacting as you also have mentioned before then maybe people are just saying okay why should I change anything everything is okay and if I if I can the um so I put in the chat that's a direct relationship of what you're talking about there to climate change and let me tell you what's I think is going on here it's I think it's pretty pretty wild so we talk about abstractions we have abstractions upon abstractions you got Futures markets and derivatives and they're trading upon trading upon trading and there's all intangible now there's a connection ultimately to some physicality some commodity or some phenomenon but they're abstractions that are layered on abstractions and that's fine but the challenge is we're trying to drisk and create neg entropy we're dissipating information differentials in those abstract areas and that requires energy AI requires coal burning and so the energy required to maintain the energy generation in physical space coal oil Etc um required and the agriculture required to feed the people to did all that stuff that's required in physical space to keep taking these exponentially increasing levels of abstraction in intangible risk space because it's the financial markets are one set of instantiations but reinsurance is an abstraction on top of an abstraction the insurance didn't exist in all forms forever and it keeps getting more abstracted forward contracts a form of insurance when you're hedging you're doing it for insurance speculation in markets like that create liquidity but also are make it noisy in terms of what the risk profile is right so it it feels like the when as we're drisking in abstract space information space we have we it requires energy that we have to burn coal for in physical space and so that it's it's an Insidious problem that we continue to try to do that but I have to go grit something at the door but I wanted to make that point about that relationship between climate change and the information space thanks one uh key Point Roland that you you mentioned was fit fitting The Experience Horizon to the living beings in some ways that's the cognitive approach which is fit it to this the cognitive slice like the sense making and decisions selection in the moment and then there's the consequences for the kinds of AI or or synthetic intelligence models that are made for example downloading all of the internet's plain text Data that's more experienced than one human life learning and then using correlative aspects of that large data set to then carry forth those correlations forward that's like the llm framework and then the question is how and where alongside or differently does a small generative model or some other kind of nested or structured generative Model come into play that is able to do better or differently than just recapitulating correlations Plus+ plus and and and how that actually plays out and then I think there was a great comments about how does it really get a do opted or how does it be shown to make a difference and it can be used in retrospective meta analysis possibly with the kinds of data that you've shown today so having open source and clear methods for doing the retrospective and saying not saying that that doing anything differently would have been better but here's how this pattern of outcomes and decision making differed from this so give a metaanalytic framework for longer past time series do back casting kind of like in stock trading and then open up that space of well people are familiar with stock trading and investing based upon the mean value and this moves it into also thinking about the variance profiles I find this extremely interesting as an observation so when I used to work also at swissre together with Stefan and I used to an initiative in casualty and uh you see in property uh things are somewhat objective because you have earthquakes and you have windstorms and they have kind of um a physical nature and uh you you can study how the earth works and and you understand it uh something like liability is much more man-made and uh so essentially uh it's all about things that are not supposed to happen so you always have like an infinite configuration space and you cannot model it uh and already at the time it was like 20 years ago uh but I said we we need to have like monitors uh to systematically scan the media what's being published what what's the awareness uh because with a certain likelihood the lawyers are going to pick up what's in the media and they are going to do lawsuits about it which then result in liability claims right and there I see exact the applicability of llms and all this stuff because what what they can do extremely well is is put things in a in a correlation with each other right and and so I think especially for modeling casualty business I see I see a huge potential in what you are saying so just you know putting llms on top and and finding out uh any any form of uh dependence between what's being published and and lawsuits Etc so Stefan where are you going to go with this work or what are your directions okay uh good question but you know as I I think a briefly mentioned um this see my have to maybe just make a short comment about my career so I have a degree in physics uh but then after my PhD I then went into reinsurance industry and I left the reinsurance industry about 10 years ago and then I was doing completely different things but this topic here was kind of something which was brewing in the back of my mind so by exploring other fields and the uh maybe I have to say this is also thanks to my wife which she exposes me to all kind of topics suddenly I got familiar with different approach is how to solve problems and then suddenly this uh um I saw a way how to solve this issue here and my intention was simply to bring it to paper to write it down to publish it and get it out so basically saying okay then for me the this is done but now Roland came into the picture and he is now coming up with some ideas that maybe we can do something with it so I don't know exactly what's going to happen I as I also mentioned I approached some colleagues from former colleagues but there was no big interest and I've also seen in when I published the paper the peer reviewers they say yes it's interesting but for them it was not something they say yes we need to move in this direction it was simply asking what is then the benefit compared to the old approaches so so far I don't see really the I haven't encountered anyone which is open to it and I didn't have any plans because I have other ideas which I want to presume I'm retired and I have other projects but depending what is happening if people approach me say yes how can we somehow implement this framework then I'm open to be involved there but I have no specific plans to do anything with it anymore maybe some reinsurance listen to this so I think uh uh obviously it's it's not very nice to do it uh to do a dry exercise with fake data uh so we should have real data and we should have a real interest behind it uh but maybe if some some reinsurers are listening to this podcast um uh please contact us and and uh let's see that we can collaborate in this that we can do something in this yeah there's there's a lot of things to say there's this laid out calculation and the kinds of Basi analyses that are described with this kind of Baseline structure and then as you alluded to that's even before getting into multi-agent cognitive economics thinking more about the decision- making the sense making of different entities this is just taking in a very specific approach to to structuring the data and the input and and it's an interesting direction to see what what tools and how General does the toolkit have to be like when you say external data comes in that could range from nothing to all the stock data in the world so how do you even exactly go about that to make it useful without just saying well it could be tiny or it could be all the data analysis in the world and then again how does that connect to doing similarly or better or different than the kinds of models already in play and then the decisions coming out of those I mean it's funny because oh go ahead Stan oh sorry I think you make your bring an interesting thought into the picture because what I see you can see I focus on a very tiny part of the whole decision process but of course you can expand now this um Market blank the pro make say okay at the end we have within an reur company there is many different buckets which we could try to understand from this uh from the perspective of this new framework and see how is the um the corporate decision processes what they happening how do different place um part parts of the company interact how can the company interact with there's other information coming from the outside and you have other stakeholders I mean I just looked here just as the um Insurance clients but there of course there is the you have the financial part you have the maybe some exchanging risk maybe you have the uh shareholders and so on so there is many different facets and you can also see them yourself as part of a bigger economic framework and see what is your place in there so at the end of the day I think if you want to have kind of digital Earth you could try to expand this kind of thinking into some much bigger uh framework and and try to understand but I don't know if anyone has the capability to it because it sounds pretty complex but maybe with the help of artificial intelligence we might get there yeah this is another thought because every time we have these Tech revolutions like now with the AI Revolution uh it also shifts industry players right so if we if we look at the invention of the smartphone before phone companies were called like uh Ericson or Nokia and afterwards they were called Apple and Google and so I think there are always these shifts when when Tech revolutions come and I think this this AI Revolution together with such modeling uh it could actually then suddenly become attractive for big Tech to start doing reinsurance because what you need is you need uh Capital they have Capital uh and and you need you need skills and uh they are actually quite good at you know acquiring data and analyzing data so maybe it's exactly this type of model that makes it accessible for big Tech now to to enter the domain reinsurance on digital information interesting area I mean insurance it's very simple it's B saying we need someone who absorbs it's the last resort for risk so you need to be able to assess this risk and in the case if something happens you need to be able to pay and I mean this is the traditional approach but this could be easily replaced as Roland said by other players which Step into the market and since it's not largely regulated this should entry point should be relatively easy if you have the technology and if you have also the financial strengths behind and the of course the skills this is the important part then you can basically replace all the rur industry and it's not complete fiction if you look at Alibaba this big Chinese um Amazon uh so Alibaba started selling insurance so it's it's not completely off because they they have a platform so I could picture uh you know Amazon or or Facebook uh to start selling insurance it's it's it's not completely science fiction okay very interesting uh Roland and Scott if you have any last comments then Stefan you can have the last word okay all good Scott okay what if your what's your last word Stefan okay well I didn't think about having a last word but I mean for me it was very interesting also having this discussion it was for me Al a new experience to be in a podcast and to present in this way I mean of course I was presenting many uh traditionally and it's now I'm entering it my later days into a new uh platform which was quite uh an interesting experience so I enjoyed it having this conversation and then let's see what's happening out of it great all right thanks again all till next time okay thank you e 
